
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.3">
    
    
      
        <title>prompt-to-prompt - 项小羽的学习与科研</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.7a952b86.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#prompt-to-prompt" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="项小羽的学习与科研" class="md-header__button md-logo" aria-label="项小羽的学习与科研" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            项小羽的学习与科研
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              prompt-to-prompt
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      欢迎来到项小羽的学习与科研
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E5%85%B6%E4%BB%96/books/" class="md-tabs__link">
        其他
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E5%AD%A6%E4%B9%A0/%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
        学习
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E5%B7%A5%E5%85%B7/" class="md-tabs__link">
        工具
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../%E4%BB%8B%E7%BB%8D/" class="md-tabs__link md-tabs__link--active">
        科研
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="项小羽的学习与科研" class="md-nav__button md-logo" aria-label="项小羽的学习与科研" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    项小羽的学习与科研
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        欢迎来到项小羽的学习与科研
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          其他
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="其他" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          其他
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%85%B6%E4%BB%96/books/" class="md-nav__link">
        推荐几本书?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%85%B6%E4%BB%96/music/" class="md-nav__link">
        好歌推荐
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%85%B6%E4%BB%96/website/" class="md-nav__link">
        在这里分享一些有用的网址
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%AD%A6%E4%B9%A0/%E7%AE%80%E4%BB%8B/" class="md-nav__link">
        一个简单的简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          超算
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="超算" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          超算
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ASC22/" class="md-nav__link">
        ASC22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/IPCC22/" class="md-nav__link">
        IPCC2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ISC23/" class="md-nav__link">
        ISC23
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          工具
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="工具" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          工具
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%B7%A5%E5%85%B7/" class="md-nav__link">
        简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%B7%A5%E5%85%B7/teach%20you%20mac/" class="md-nav__link">
        项小羽试图教你用mac
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%B7%A5%E5%85%B7/vscode/" class="md-nav__link">
        Vscode配置
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" class="md-nav__link">
        如何跨越长城
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          科研
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="科研" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          科研
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E4%BB%8B%E7%BB%8D/" class="md-nav__link">
        一个介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E5%A6%82%E4%BD%95/" class="md-nav__link">
        如何
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" class="md-nav__link">
        环境配置
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E8%AE%BA%E6%96%87%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" class="md-nav__link">
        论文公式推导
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_5" type="checkbox" id="__nav_5_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_5">
          Diffusion库
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Diffusion库" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_5">
          <span class="md-nav__icon md-icon"></span>
          Diffusion库
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../diffusion%E5%BA%93/" class="md-nav__link">
        diffusion库
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_6" type="checkbox" id="__nav_5_6" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5_6">
          图文大模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="图文大模型" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_6">
          <span class="md-nav__icon md-icon"></span>
          图文大模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01working_log/" class="md-nav__link">
        工作日志
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../diffedit/" class="md-nav__link">
        Diffedit
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../dream_booth/" class="md-nav__link">
        DreamBooth
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../hand_inject/" class="md-nav__link">
        attention map hand inject
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          prompt-to-prompt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        prompt-to-prompt
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    前言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    标记说明
  </a>
  
    <nav class="md-nav" aria-label="标记说明">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cross-attention-mapprompt_edit_tokens_end-prompt_edit_tokens_start" class="md-nav__link">
    Cross Attention map的替换从prompt_edit_tokens_end开始, 到prompt_edit_tokens_start结束
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-mapprompt_edit_spatial_end-prompt_edit_spatial_start" class="md-nav__link">
    Self Attention map的替换从prompt_edit_spatial_end开始, prompt_edit_spatial_start结束
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    复现和熟悉过程
  </a>
  
    <nav class="md-nav" aria-label="复现和熟悉过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    粗读代码
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    需要关注的问题
  </a>
  
    <nav class="md-nav" aria-label="需要关注的问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    attention 的层数, 分布
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequence-matcher" class="md-nav__link">
    sequence matcher到底在做什么?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    各个参数控制原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sliced_attention" class="md-nav__link">
    sliced_attention在做什么?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    算法是不是有问题?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-map" class="md-nav__link">
    attention map 到底如何起作用?
  </a>
  
    <nav class="md-nav" aria-label="attention map 到底如何起作用?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    前提提要
  </a>
  
    <nav class="md-nav" aria-label="前提提要">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#seed-case1" class="md-nav__link">
    seed case1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seed-case2" class="md-nav__link">
    seed case2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossattention-selfattention" class="md-nav__link">
    crossattention是否有用? selfattention 是否有用?
  </a>
  
    <nav class="md-nav" aria-label="crossattention是否有用? selfattention 是否有用?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cat-tiger" class="md-nav__link">
    cat-tiger
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-hamster" class="md-nav__link">
    dog-hamster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-hamster-detail" class="md-nav__link">
    dog-hamster-detail
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hamster-dog" class="md-nav__link">
    hamster-dog
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hamster-dog-detail" class="md-nav__link">
    hamster-dog-detail
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling" class="md-nav__link">
    dog-smiling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-detail" class="md-nav__link">
    dog-smiling-detail
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-end" class="md-nav__link">
    dog-smiling-end
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cake" class="md-nav__link">
    cake 实验
  </a>
  
    <nav class="md-nav" aria-label="cake 实验">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cake_1" class="md-nav__link">
    cake
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemon-cheese" class="md-nav__link">
    lemon-cheese
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemon-pistachio" class="md-nav__link">
    lemon-pistachio
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#value-map" class="md-nav__link">
    value, map不同组合
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-conditioninject" class="md-nav__link">
    image condition的不同组合(并行inject)
  </a>
  
    <nav class="md-nav" aria-label="image condition的不同组合(并行inject)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hamster-dog-self" class="md-nav__link">
    hamster-dog-self
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#margin" class="md-nav__link">
    margin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-self" class="md-nav__link">
    dog-smiling-self
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-self_1" class="md-nav__link">
    dog-smiling-self
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention_1" class="md-nav__link">
    打印所有的生成过程以及attention
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inject" class="md-nav__link">
    手动inject
  </a>
  
    <nav class="md-nav" aria-label="手动inject">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    方法1
  </a>
  
    <nav class="md-nav" aria-label="方法1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inject-scale-10" class="md-nav__link">
    inject scale = 10
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inject-scale4" class="md-nav__link">
    inject scale=4
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inject-scale3" class="md-nav__link">
    inject scale=3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    值得探索的方向
  </a>
  
    <nav class="md-nav" aria-label="值得探索的方向">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inject_1" class="md-nav__link">
    手动inject结论
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    结论
  </a>
  
    <nav class="md-nav" aria-label="结论">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    问题:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    注意到的问题
  </a>
  
    <nav class="md-nav" aria-label="注意到的问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    其实生成控制没有很理想
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../textual_inversion/" class="md-nav__link">
        Textual inversion
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_7" type="checkbox" id="__nav_5_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_7">
          珠算
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="珠算" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_7">
          <span class="md-nav__icon md-icon"></span>
          珠算
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E7%8F%A0%E7%AE%97/" class="md-nav__link">
        简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E7%8F%A0%E7%AE%97/zhusuan_worklog/" class="md-nav__link">
        工作日志
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    前言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    标记说明
  </a>
  
    <nav class="md-nav" aria-label="标记说明">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cross-attention-mapprompt_edit_tokens_end-prompt_edit_tokens_start" class="md-nav__link">
    Cross Attention map的替换从prompt_edit_tokens_end开始, 到prompt_edit_tokens_start结束
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-mapprompt_edit_spatial_end-prompt_edit_spatial_start" class="md-nav__link">
    Self Attention map的替换从prompt_edit_spatial_end开始, prompt_edit_spatial_start结束
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    复现和熟悉过程
  </a>
  
    <nav class="md-nav" aria-label="复现和熟悉过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    粗读代码
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    需要关注的问题
  </a>
  
    <nav class="md-nav" aria-label="需要关注的问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    attention 的层数, 分布
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequence-matcher" class="md-nav__link">
    sequence matcher到底在做什么?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    各个参数控制原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sliced_attention" class="md-nav__link">
    sliced_attention在做什么?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    算法是不是有问题?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-map" class="md-nav__link">
    attention map 到底如何起作用?
  </a>
  
    <nav class="md-nav" aria-label="attention map 到底如何起作用?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    前提提要
  </a>
  
    <nav class="md-nav" aria-label="前提提要">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#seed-case1" class="md-nav__link">
    seed case1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seed-case2" class="md-nav__link">
    seed case2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossattention-selfattention" class="md-nav__link">
    crossattention是否有用? selfattention 是否有用?
  </a>
  
    <nav class="md-nav" aria-label="crossattention是否有用? selfattention 是否有用?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cat-tiger" class="md-nav__link">
    cat-tiger
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-hamster" class="md-nav__link">
    dog-hamster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-hamster-detail" class="md-nav__link">
    dog-hamster-detail
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hamster-dog" class="md-nav__link">
    hamster-dog
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hamster-dog-detail" class="md-nav__link">
    hamster-dog-detail
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling" class="md-nav__link">
    dog-smiling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-detail" class="md-nav__link">
    dog-smiling-detail
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-end" class="md-nav__link">
    dog-smiling-end
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cake" class="md-nav__link">
    cake 实验
  </a>
  
    <nav class="md-nav" aria-label="cake 实验">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cake_1" class="md-nav__link">
    cake
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemon-cheese" class="md-nav__link">
    lemon-cheese
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lemon-pistachio" class="md-nav__link">
    lemon-pistachio
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#value-map" class="md-nav__link">
    value, map不同组合
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-conditioninject" class="md-nav__link">
    image condition的不同组合(并行inject)
  </a>
  
    <nav class="md-nav" aria-label="image condition的不同组合(并行inject)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hamster-dog-self" class="md-nav__link">
    hamster-dog-self
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#margin" class="md-nav__link">
    margin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-self" class="md-nav__link">
    dog-smiling-self
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dog-smiling-self_1" class="md-nav__link">
    dog-smiling-self
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention_1" class="md-nav__link">
    打印所有的生成过程以及attention
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inject" class="md-nav__link">
    手动inject
  </a>
  
    <nav class="md-nav" aria-label="手动inject">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    方法1
  </a>
  
    <nav class="md-nav" aria-label="方法1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inject-scale-10" class="md-nav__link">
    inject scale = 10
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inject-scale4" class="md-nav__link">
    inject scale=4
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inject-scale3" class="md-nav__link">
    inject scale=3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    值得探索的方向
  </a>
  
    <nav class="md-nav" aria-label="值得探索的方向">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inject_1" class="md-nav__link">
    手动inject结论
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    结论
  </a>
  
    <nav class="md-nav" aria-label="结论">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    问题:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    注意到的问题
  </a>
  
    <nav class="md-nav" aria-label="注意到的问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    其实生成控制没有很理想
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="prompt-to-prompt">prompt-to-prompt</h1>
<ul>
<li>[toc]</li>
</ul>
<h2 id="_1">前言</h2>
<p>这一部分主要是聚焦研究在diffusion model生成的过程中Attention map是如何起作用的, 可以用什么样的方法更好的利用attentionmap来控制模型的生成。这个部分最主要的启发工作来源于google的“Prompt-to-Prompt Image Editing with Cross Attention Control”, 也是我们希望复现和学习的文章。</p>
<p><img alt="image-20221101183150181" src="../prompt-to-prompt.assets/image-20221101183150181.png" /></p>
<h2 id="_2">标记说明</h2>
<h4 id="cross-attention-mapprompt_edit_tokens_end-prompt_edit_tokens_start">Cross Attention map的替换从prompt_edit_tokens_end开始, 到prompt_edit_tokens_start结束</h4>
<h4 id="self-attention-mapprompt_edit_spatial_end-prompt_edit_spatial_start">Self Attention map的替换从prompt_edit_spatial_end开始, prompt_edit_spatial_start结束</h4>
<p><code>ss</code>是<code>self Attention inject start</code>的简称, 也即<code>prompt_edit_spatial_start</code>, 而默认的<code>prompt_edit_tokens_end=1.0</code>, 生成过程是1.0 -&gt; 0, 所以map inject 从一开始就开始, 到<code>prompt_edit_spatial_start</code>结束, 例如生成过程50步, <code>prompt_edit_spatial_start=0.8</code>, 这意味着前<code>50*(1-0.8)=10</code>步是有map inject, 后续则没有。如果<code>prompt_edit_spatial_start=prompt_edit_spatial_end=0</code>, 则全称无inject, 生成src原图, 即图片矩阵最右下角的图片。</p>
<p><code>cs</code>是<code>cross Attention inject start</code>的简称, 也即<code>prompt_edit_tokens_start</code>, 作用机制同理。</p>
<p><code>ce</code>是<code>corss Attention inject end</code>的简称, 也即<code>prompt_edit_tokens_end</code>。</p>
<p><code>se</code>是<code>self Attention inject end</code>的简称, 也即<code>prompt_edit_spatial_end</code>。</p>
<h2 id="_3">复现和熟悉过程</h2>
<p>之前在github上看到了有人对google的这篇文章进行了复现, 在这里进行尝试, 复现他的结果, 然后研究一下他是怎么做, 以及我们可以怎么做吧。直接上代码。</p>
<div class="highlight"><pre><span></span><code>git clone git@github.com:bloc97/CrossAttentionControl.git
pip install torch transformers <span class="nv">diffusers</span><span class="o">==</span><span class="m">0</span>.4.1 numpy pillow tqdm jupyter
jupyter-notebook
<span class="c1"># 如果是ssh在服务器上, 则需要通过ssh传递</span>
jupyter-notebook --no-browser --port<span class="o">=</span><span class="m">1234</span> <span class="c1"># on server</span>
ssh -NL localhost:1234:localhost:1234 g5 <span class="c1"># on your pc, then open link in server jupyter, notice port need to be your host port</span>
</code></pre></div>
<p>跑是能跑的, 不断按shift + enter, 除了模型下载慢一点, 其他都可以。</p>
<h3 id="_4">粗读代码</h3>
<p>引入包, load模型</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CLIPModel</span><span class="p">,</span> <span class="n">CLIPTextModel</span><span class="p">,</span> <span class="n">CLIPTokenizer</span>
<span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">AutoencoderKL</span><span class="p">,</span> <span class="n">UNet2DConditionModel</span>
<span class="c1">#NOTE: Last tested working diffusers version is diffusers==0.4.1, https://github.com/huggingface/diffusers/releases/tag/v0.4.1</span>

<span class="c1">#Init CLIP tokenizer and model</span>
<span class="n">model_path_clip</span> <span class="o">=</span> <span class="s2">&quot;openai/clip-vit-large-patch14&quot;</span>
<span class="n">clip_tokenizer</span> <span class="o">=</span> <span class="n">CLIPTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path_clip</span><span class="p">)</span>
<span class="n">clip_model</span> <span class="o">=</span> <span class="n">CLIPModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path_clip</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">clip</span> <span class="o">=</span> <span class="n">clip_model</span><span class="o">.</span><span class="n">text_model</span>

<span class="c1">#Init diffusion model</span>
<span class="n">auth_token</span> <span class="o">=</span> <span class="s2">&quot;这个是hugging face 的access token&quot;</span> <span class="c1">#Replace this with huggingface auth token as a string if model is not already downloaded</span>
<span class="n">model_path_diffusion</span> <span class="o">=</span> <span class="s2">&quot;CompVis/stable-diffusion-v1-4&quot;</span>
<span class="c1"># 看起来stable diffusion用的diffusion model就是这一个, 但是对于模型参数而言, 不知道他到底下载的是参数的那一部分, 而且用了半精度</span>
<span class="c1"># 代码在这里 https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py</span>
<span class="n">unet</span> <span class="o">=</span> <span class="n">UNet2DConditionModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path_diffusion</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="n">auth_token</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s2">&quot;fp16&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="c1"># 这看起来就是stable diffusion用的vae</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">AutoencoderKL</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path_diffusion</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&quot;vae&quot;</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="n">auth_token</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s2">&quot;fp16&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="c1">#Move to GPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="n">unet</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vae</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">clip</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loaded all models&quot;</span><span class="p">)</span>
</code></pre></div>
<p>写的一些有关attention的函数</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">LMSDiscreteScheduler</span> <span class="c1"># https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py</span>
<span class="c1"># 这个类的代码在这里, 但是具体的作用目前还不知道</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">autocast</span> <span class="c1"># 混合精度的东西</span>
<span class="kn">from</span> <span class="nn">difflib</span> <span class="kn">import</span> <span class="n">SequenceMatcher</span> <span class="c1"># 比较两个序列</span>

<span class="k">def</span> <span class="nf">init_attention_weights</span><span class="p">(</span><span class="n">weight_tuples</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  初始化权重, 如果提供了对应的位置的权重, 则替换, 否则全为1</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="n">tokens_length</span> <span class="o">=</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weight_tuples</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokens_length</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># TODO 权重是一个序列长度的一维向量, 被初始化到last_attn_slice_weights, 这是什么东西</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn1&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_weights</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">init_attention_edit</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tokens_edit</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        &quot;&quot;&quot;</span>
    <span class="n">tokens_length</span> <span class="o">=</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">)</span>
    <span class="n">indices_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="c1"># 0, 1, 2...</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="c1"># 0, 0, 0 ...</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 字典index?</span>
    <span class="n">tokens_edit</span> <span class="o">=</span> <span class="n">tokens_edit</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="n">SequenceMatcher</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tokens_edit</span><span class="p">)</span><span class="o">.</span><span class="n">get_opcodes</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">b0</span> <span class="o">&lt;</span> <span class="n">tokens_length</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;equal&quot;</span> <span class="ow">or</span> <span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;replace&quot;</span> <span class="ow">and</span> <span class="n">a1</span><span class="o">-</span><span class="n">a0</span> <span class="o">==</span> <span class="n">b1</span><span class="o">-</span><span class="n">b0</span><span class="p">):</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">b0</span><span class="p">:</span><span class="n">b1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">indices</span><span class="p">[</span><span class="n">b0</span><span class="p">:</span><span class="n">b1</span><span class="p">]</span> <span class="o">=</span> <span class="n">indices_target</span><span class="p">[</span><span class="n">a0</span><span class="p">:</span><span class="n">a1</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn1&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_mask</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_indices</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># TODO 弄清last_attn_slice是什么, sliced_attention, attention两个函数的区别</span>
<span class="k">def</span> <span class="nf">init_attention_func</span><span class="p">():</span>
    <span class="c1">#ORIGINAL SOURCE CODE: https://github.com/huggingface/diffusers/blob/91ddd2a25b848df0fa1262d4f1cd98c7ccb87750/src/diffusers/models/attention.py#L276</span>
    <span class="k">def</span> <span class="nf">new_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="c1"># TODO: use baddbmm for better performance</span>
        <span class="c1"># query 和 key 通过矩阵乘, 再通过softmax得到attention map</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># compute attention output</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_slice</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_attn_slice</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_indices</span><span class="p">)</span> <span class="c1"># 在单词维度进行选择</span>
                <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">new_attn_slice</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attn_slice</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># 这一步是在进行edit之前的执行的一次使用原始prompt进行forward，然后保存attn map</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_last_attn_slice</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># 如果没有injection, 则直接再矩阵乘value, 就得到了下一层的输出</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_slice</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="c1"># reshape hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_batch_dim_to_heads</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>

    <span class="k">def</span> <span class="nf">new_sliced_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>

        <span class="n">batch_size_attention</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size_attention</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">slice_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">slice_size</span><span class="p">):</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">slice_size</span>
            <span class="n">end_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">slice_size</span>
            <span class="n">attn_slice</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span> <span class="n">key</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
            <span class="p">)</span>  <span class="c1"># TODO: use baddbmm for better performance</span>
            <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_slice</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">new_attn_slice</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_indices</span><span class="p">)</span>
                    <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">new_attn_slice</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_mask</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">attn_slice</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_last_attn_slice</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_attn_slice_weights</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_slice</span><span class="p">,</span> <span class="n">value</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">])</span>

            <span class="n">hidden_states</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_slice</span>

        <span class="c1"># reshape hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_batch_dim_to_heads</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">module</span><span class="o">.</span><span class="n">save_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>
 <span class="c1"># 以下是一个描述器功能, 此后调用module._sliced_attention, 等价于调用new_sliced_attention</span>
 <span class="c1"># 但是为什么不能直接把函数地址传进去呢, 也就是module._sliced_attention = new_sliced_attention</span>
 <span class="c1"># 这是因为函数中有参数self,如果按地址传, self也需要显示参数提供, 而使用描述器就可以直接传我们定义的参数了</span>
            <span class="n">module</span><span class="o">.</span><span class="n">_sliced_attention</span> <span class="o">=</span> <span class="n">new_sliced_attention</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>
            <span class="n">module</span><span class="o">.</span><span class="n">_attention</span> <span class="o">=</span> <span class="n">new_attention</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">use_last_tokens_attention</span><span class="p">(</span><span class="n">use</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_slice</span> <span class="o">=</span> <span class="n">use</span>

<span class="k">def</span> <span class="nf">use_last_tokens_attention_weights</span><span class="p">(</span><span class="n">use</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="o">=</span> <span class="n">use</span>

<span class="k">def</span> <span class="nf">use_last_self_attention</span><span class="p">(</span><span class="n">use</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn1&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_slice</span> <span class="o">=</span> <span class="n">use</span>

<span class="k">def</span> <span class="nf">save_last_tokens_attention</span><span class="p">(</span><span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">save_last_attn_slice</span> <span class="o">=</span> <span class="n">save</span>

<span class="k">def</span> <span class="nf">save_last_self_attention</span><span class="p">(</span><span class="n">save</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn1&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">save_last_attn_slice</span> <span class="o">=</span> <span class="n">save</span>
</code></pre></div>
<p>这里我们需要看一下unet中的CrossAttention是如何定义的, 有哪些方法, 以及具体的运算逻辑时怎么样的。</p>
<div class="highlight"><pre><span></span><code><span class="c1"># https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py</span>
<span class="k">class</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A cross attention layer.</span>
<span class="sd">    Parameters:</span>
<span class="sd">        query_dim (:obj:`int`): The number of channels in the query.</span>
<span class="sd">        context_dim (:obj:`int`, *optional*):</span>
<span class="sd">            The number of channels in the context. If not given, defaults to `query_dim`.</span>
<span class="sd">        heads (:obj:`int`,  *optional*, defaults to 8): The number of heads to use for multi-head attention.</span>
<span class="sd">        dim_head (:obj:`int`,  *optional*, defaults to 64): The number of channels in each head.</span>
<span class="sd">        dropout (:obj:`float`, *optional*, defaults to 0.0): The dropout probability to use.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="n">context_dim</span> <span class="o">=</span> <span class="n">context_dim</span> <span class="k">if</span> <span class="n">context_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">query_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim_head</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="c1"># for slice_size &gt; 0 the attention score computation</span>
        <span class="c1"># is split across the batch axis to save memory</span>
        <span class="c1"># You can set slice_size with `set_attention_slice`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">reshape_heads_to_batch_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor</span>

    <span class="k">def</span> <span class="nf">reshape_batch_dim_to_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span> <span class="k">if</span> <span class="n">context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_states</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>

        <span class="n">dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_heads_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_heads_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_heads_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># TODO(PVP) - mask is currently never used. Remember to re-implement when used</span>

        <span class="c1"># attention, what we cannot get enough of</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sliced_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="c1"># 此函数被新写的函数替换</span>
    <span class="k">def</span> <span class="nf">_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="c1"># TODO: use baddbmm for better performance</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># compute attention output</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="c1"># reshape hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_batch_dim_to_heads</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
<span class="c1"># 此函数被新写的函数替换</span>
    <span class="k">def</span> <span class="nf">_sliced_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">batch_size_attention</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size_attention</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">slice_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">slice_size</span><span class="p">):</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">slice_size</span>
            <span class="n">end_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">slice_size</span>
            <span class="n">attn_slice</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span> <span class="n">key</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
            <span class="p">)</span>  <span class="c1"># TODO: use baddbmm for better performance</span>
            <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">attn_slice</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn_slice</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_slice</span><span class="p">,</span> <span class="n">value</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">])</span>

            <span class="n">hidden_states</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_slice</span>

        <span class="c1"># reshape hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape_batch_dim_to_heads</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div>
<p>接下来是看整个stablediffusion的生成过程是如何运作的。</p>
<div class="highlight"><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">stablediffusion</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">prompt_edit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prompt_edit_token_weights</span><span class="o">=</span><span class="p">[],</span> <span class="n">prompt_edit_tokens_start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">prompt_edit_tokens_end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">prompt_edit_spatial_start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">prompt_edit_spatial_end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">guidance_scale</span><span class="o">=</span><span class="mf">7.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">init_image</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_image_strength</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="c1">#Change size to multiple of 64 to prevent size mismatches inside model</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="n">width</span> <span class="o">%</span> <span class="mi">64</span>
    <span class="n">height</span> <span class="o">=</span> <span class="n">height</span> <span class="o">-</span> <span class="n">height</span> <span class="o">%</span> <span class="mi">64</span>

    <span class="c1">#If seed is None, randomly select seed from 0 to 2^32-1</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">seed</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1">#Set inference timesteps to scheduler</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">LMSDiscreteScheduler</span><span class="p">(</span><span class="n">beta_start</span><span class="o">=</span><span class="mf">0.00085</span><span class="p">,</span> <span class="n">beta_end</span><span class="o">=</span><span class="mf">0.012</span><span class="p">,</span> <span class="n">beta_schedule</span><span class="o">=</span><span class="s2">&quot;scaled_linear&quot;</span><span class="p">,</span> <span class="n">num_train_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">set_timesteps</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

    <span class="c1">#Preprocess image if it exists (img2img)</span>
    <span class="k">if</span> <span class="n">init_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1">#Resize and transpose for numpy b h w c -&gt; torch b c h w</span>
        <span class="n">init_image</span> <span class="o">=</span> <span class="n">init_image</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">),</span> <span class="n">resample</span><span class="o">=</span><span class="n">Image</span><span class="o">.</span><span class="n">Resampling</span><span class="o">.</span><span class="n">LANCZOS</span><span class="p">)</span>
        <span class="n">init_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">init_image</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="mf">2.0</span> <span class="o">-</span> <span class="mf">1.0</span>
        <span class="n">init_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">init_image</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="c1">#If there is alpha channel, composite alpha for white, as the diffusion model does not support alpha channel</span>
        <span class="k">if</span> <span class="n">init_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">init_image</span> <span class="o">=</span> <span class="n">init_image</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">init_image</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">init_image</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:])</span>

        <span class="c1">#Move image to GPU</span>
        <span class="n">init_image</span> <span class="o">=</span> <span class="n">init_image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1">#Encode image</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
            <span class="n">init_latent</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">init_image</span><span class="p">)</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.18215</span>

        <span class="n">t_start</span> <span class="o">=</span> <span class="n">steps</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">steps</span> <span class="o">*</span> <span class="n">init_image_strength</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">init_latent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">unet</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">height</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="mi">8</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">t_start</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">#Generate random normal noise</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">init_latent</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1">#latent = noise * scheduler.init_noise_sigma</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">add_noise</span><span class="p">(</span><span class="n">init_latent</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">scheduler</span><span class="o">.</span><span class="n">timesteps</span><span class="p">[</span><span class="n">t_start</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1">#Process clip</span>
    <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="n">tokens_unconditional</span> <span class="o">=</span> <span class="n">clip_tokenizer</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">embedding_unconditional</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">tokens_unconditional</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="n">tokens_conditional</span> <span class="o">=</span> <span class="n">clip_tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">embedding_conditional</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">tokens_conditional</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="c1">#Process prompt editing</span>
        <span class="k">if</span> <span class="n">prompt_edit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokens_conditional_edit</span> <span class="o">=</span> <span class="n">clip_tokenizer</span><span class="p">(</span><span class="n">prompt_edit</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">embedding_conditional_edit</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">tokens_conditional_edit</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">last_hidden_state</span>

            <span class="n">init_attention_edit</span><span class="p">(</span><span class="n">tokens_conditional</span><span class="p">,</span> <span class="n">tokens_conditional_edit</span><span class="p">)</span>

        <span class="n">init_attention_func</span><span class="p">()</span>
        <span class="n">init_attention_weights</span><span class="p">(</span><span class="n">prompt_edit_token_weights</span><span class="p">)</span>

        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">timesteps</span><span class="p">[</span><span class="n">t_start</span><span class="p">:]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)):</span>
            <span class="n">t_index</span> <span class="o">=</span> <span class="n">t_start</span> <span class="o">+</span> <span class="n">i</span>

            <span class="c1">#sigma = scheduler.sigmas[t_index]</span>
            <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">latent</span>
            <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">scale_model_input</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

            <span class="c1">#Predict the unconditional noise residual</span>
            <span class="c1"># 这里是为了使用classifier free guidance</span>
            <span class="n">noise_pred_uncond</span> <span class="o">=</span> <span class="n">unet</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">embedding_unconditional</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span>

            <span class="c1">#Prepare the Cross-Attention layers</span>
            <span class="k">if</span> <span class="n">prompt_edit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">save_last_tokens_attention</span><span class="p">()</span>
                <span class="n">save_last_self_attention</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#Use weights on non-edited prompt when edit is None</span>
                <span class="n">use_last_tokens_attention_weights</span><span class="p">()</span>

            <span class="c1">#Predict the conditional noise residual and save the cross-attention layer activations</span>
            <span class="n">noise_pred_cond</span> <span class="o">=</span> <span class="n">unet</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">embedding_conditional</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span>

            <span class="c1">#Edit the Cross-Attention layer activations</span>
            <span class="k">if</span> <span class="n">prompt_edit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">t_scale</span> <span class="o">=</span> <span class="n">t</span> <span class="o">/</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">num_train_timesteps</span>
                <span class="k">if</span> <span class="n">t_scale</span> <span class="o">&gt;=</span> <span class="n">prompt_edit_tokens_start</span> <span class="ow">and</span> <span class="n">t_scale</span> <span class="o">&lt;=</span> <span class="n">prompt_edit_tokens_end</span><span class="p">:</span>
                    <span class="n">use_last_tokens_attention</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">t_scale</span> <span class="o">&gt;=</span> <span class="n">prompt_edit_spatial_start</span> <span class="ow">and</span> <span class="n">t_scale</span> <span class="o">&lt;=</span> <span class="n">prompt_edit_spatial_end</span><span class="p">:</span>
                    <span class="n">use_last_self_attention</span><span class="p">()</span>

                <span class="c1">#Use weights on edited prompt</span>
                <span class="n">use_last_tokens_attention_weights</span><span class="p">()</span>

                <span class="c1">#Predict the edited conditional noise residual using the cross-attention masks</span>
                <span class="n">noise_pred_cond</span> <span class="o">=</span> <span class="n">unet</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">embedding_conditional_edit</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span>

            <span class="c1">#Perform guidance</span>
            <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">noise_pred_uncond</span> <span class="o">+</span> <span class="n">guidance_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">noise_pred_cond</span> <span class="o">-</span> <span class="n">noise_pred_uncond</span><span class="p">)</span>

            <span class="n">latent</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">t_index</span><span class="p">,</span> <span class="n">latent</span><span class="p">)</span><span class="o">.</span><span class="n">prev_sample</span>

        <span class="c1">#scale and decode the image latents with vae</span>
        <span class="c1"># TODO 0.18215 这个数字是怎么来的呢</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="n">latent</span> <span class="o">/</span> <span class="mf">0.18215</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">latent</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">vae</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span>

    <span class="n">image</span> <span class="o">=</span> <span class="p">(</span><span class="n">image</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">image</span> <span class="o">=</span> <span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</code></pre></div>
<h2 id="_5">需要关注的问题</h2>
<ul>
<li>弄清last_attn_slice是什么, sliced_attention, attention两个函数的区别</li>
<li>弄清楚VAE的原理, 弄清楚attention map中的位置是否体现出原图中的空域信息</li>
<li>弄清楚attention有几层,  每个层的attention层的变化情况, 以及哪些层是重要的</li>
</ul>
<h3 id="attention">attention 的层数, 分布</h3>
<p>话不多说, 直接写代码来解决问题。</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">attn2_layers</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">attn_layers</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">module</span><span class="o">.</span><span class="n">use_last_attn_weights</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">module</span><span class="o">.</span><span class="n">save_last_attn_slice</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">module</span><span class="o">.</span><span class="n">_sliced_attention</span> <span class="o">=</span> <span class="n">new_sliced_attention</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>
            <span class="n">module</span><span class="o">.</span><span class="n">_attention</span> <span class="o">=</span> <span class="n">new_attention</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;attn layers:&quot;</span><span class="p">,</span> <span class="n">attn_layers</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;atten2 layers:&quot;</span><span class="p">,</span> <span class="n">attn2_layers</span><span class="p">)</span>
</code></pre></div>
<p>输出:</p>
<div class="highlight"><pre><span></span><code>attn layers: 32
atten2 layers: 16
根据后续的输出, 可知attn1和attn2是交替出现的
</code></pre></div>
<p>同时查看query, key, value的维度参数, 我们可以注意到这是<code>selfAttention</code> 以及 <code>crossAttention</code>, 交替使用, 这也印证了代码中需要区分attn1以及attn2, 这也是为什么主要inject的是attn2的layer, 而不是attn1, 其次可以通过attention map的维度看出, 模型呈现中间attention map小, 两端的attention map大的特点, 这也就是unet的架构。</p>
<div class="highlight"><pre><span></span><code>attn1  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 4096, 40]) 
value.shape torch.Size([8, 4096, 40])
1 origin attn map: torch.Size([8, 4096, 4096])
new attn map: torch.Size([8, 4096, 4096])

attn2  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 77, 40]) 
value.shape torch.Size([8, 77, 40])
2 origin attn map: torch.Size([8, 4096, 77])
new attn map: torch.Size([8, 4096, 77])

attn1  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 4096, 40]) 
value.shape torch.Size([8, 4096, 40])
3 origin attn map: torch.Size([8, 4096, 4096])
new attn map: torch.Size([8, 4096, 4096])

attn2  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 77, 40]) 
value.shape torch.Size([8, 77, 40])
4 origin attn map: torch.Size([8, 4096, 77])
new attn map: torch.Size([8, 4096, 77])

attn1  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 1024, 80]) 
value.shape torch.Size([8, 1024, 80])
5 origin attn map: torch.Size([8, 1024, 1024])
new attn map: torch.Size([8, 1024, 1024])

attn2  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 77, 80]) 
value.shape torch.Size([8, 77, 80])
6 origin attn map: torch.Size([8, 1024, 77])
new attn map: torch.Size([8, 1024, 77])

attn1  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 1024, 80]) 
value.shape torch.Size([8, 1024, 80])
7 origin attn map: torch.Size([8, 1024, 1024])
new attn map: torch.Size([8, 1024, 1024])

attn2  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 77, 80]) 
value.shape torch.Size([8, 77, 80])
8 origin attn map: torch.Size([8, 1024, 77])
new attn map: torch.Size([8, 1024, 77])

attn1  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 256, 160]) 
value.shape torch.Size([8, 256, 160])
9 origin attn map: torch.Size([8, 256, 256])
new attn map: torch.Size([8, 256, 256])

attn2  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 77, 160]) 
value.shape torch.Size([8, 77, 160])
10 origin attn map: torch.Size([8, 256, 77])
new attn map: torch.Size([8, 256, 77])

attn1  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 256, 160]) 
value.shape torch.Size([8, 256, 160])
11 origin attn map: torch.Size([8, 256, 256])
new attn map: torch.Size([8, 256, 256])

attn2  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 77, 160]) 
value.shape torch.Size([8, 77, 160])
12 origin attn map: torch.Size([8, 256, 77])
new attn map: torch.Size([8, 256, 77])

attn1  query.shape torch.Size([8, 64, 160]) 
key.shape torch.Size([8, 64, 160]) 
value.shape torch.Size([8, 64, 160])
31 origin attn map: torch.Size([8, 64, 64])
new attn map: torch.Size([8, 64, 64])

attn2  query.shape torch.Size([8, 64, 160]) 
key.shape torch.Size([8, 77, 160]) 
value.shape torch.Size([8, 77, 160])
32 origin attn map: torch.Size([8, 64, 77])
new attn map: torch.Size([8, 64, 77])

attn1  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 256, 160]) 
value.shape torch.Size([8, 256, 160])
13 origin attn map: torch.Size([8, 256, 256])
new attn map: torch.Size([8, 256, 256])

attn2  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 77, 160]) 
value.shape torch.Size([8, 77, 160])
14 origin attn map: torch.Size([8, 256, 77])
new attn map: torch.Size([8, 256, 77])

attn1  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 256, 160]) 
value.shape torch.Size([8, 256, 160])
15 origin attn map: torch.Size([8, 256, 256])
new attn map: torch.Size([8, 256, 256])

attn2  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 77, 160]) 
value.shape torch.Size([8, 77, 160])
16 origin attn map: torch.Size([8, 256, 77])
new attn map: torch.Size([8, 256, 77])

attn1  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 256, 160]) 
value.shape torch.Size([8, 256, 160])
17 origin attn map: torch.Size([8, 256, 256])
new attn map: torch.Size([8, 256, 256])

attn2  query.shape torch.Size([8, 256, 160]) 
key.shape torch.Size([8, 77, 160]) 
value.shape torch.Size([8, 77, 160])
18 origin attn map: torch.Size([8, 256, 77])
new attn map: torch.Size([8, 256, 77])

attn1  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 1024, 80]) 
value.shape torch.Size([8, 1024, 80])
19 origin attn map: torch.Size([8, 1024, 1024])
new attn map: torch.Size([8, 1024, 1024])

attn2  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 77, 80]) 
value.shape torch.Size([8, 77, 80])
20 origin attn map: torch.Size([8, 1024, 77])
new attn map: torch.Size([8, 1024, 77])

attn1  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 1024, 80]) 
value.shape torch.Size([8, 1024, 80])
21 origin attn map: torch.Size([8, 1024, 1024])
new attn map: torch.Size([8, 1024, 1024])

attn2  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 77, 80]) 
value.shape torch.Size([8, 77, 80])
22 origin attn map: torch.Size([8, 1024, 77])
new attn map: torch.Size([8, 1024, 77])

attn1  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 1024, 80]) 
value.shape torch.Size([8, 1024, 80])
23 origin attn map: torch.Size([8, 1024, 1024])
new attn map: torch.Size([8, 1024, 1024])

attn2  query.shape torch.Size([8, 1024, 80]) 
key.shape torch.Size([8, 77, 80]) 
value.shape torch.Size([8, 77, 80])
24 origin attn map: torch.Size([8, 1024, 77])
new attn map: torch.Size([8, 1024, 77])

attn1  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 4096, 40]) 
value.shape torch.Size([8, 4096, 40])
25 origin attn map: torch.Size([8, 4096, 4096])
new attn map: torch.Size([8, 4096, 4096])

attn2  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 77, 40]) 
value.shape torch.Size([8, 77, 40])
26 origin attn map: torch.Size([8, 4096, 77])
new attn map: torch.Size([8, 4096, 77])
2
attn1  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 4096, 40]) 
value.shape torch.Size([8, 4096, 40])
27 origin attn map: torch.Size([8, 4096, 4096])
new attn map: torch.Size([8, 4096, 4096])

attn2  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 77, 40]) 
value.shape torch.Size([8, 77, 40])
28 origin attn map: torch.Size([8, 4096, 77])
new attn map: torch.Size([8, 4096, 77])

attn1  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 4096, 40]) 
value.shape torch.Size([8, 4096, 40])
29 origin attn map: torch.Size([8, 4096, 4096])
new attn map: torch.Size([8, 4096, 4096])

attn2  query.shape torch.Size([8, 4096, 40]) 
key.shape torch.Size([8, 77, 40]) 
value.shape torch.Size([8, 77, 40])
30 origin attn map: torch.Size([8, 4096, 77])
new attn map: torch.Size([8, 4096, 77])
</code></pre></div>
<h3 id="sequence-matcher">sequence matcher到底在做什么?</h3>
<p>我们先简单print一下中间变量, 看一下结果:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_attention_edit</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tokens_edit</span><span class="p">):</span>
    <span class="n">tokens_length</span> <span class="o">=</span> <span class="n">clip_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">)</span>
    <span class="n">indices_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tokens_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">tokens_edit</span> <span class="o">=</span> <span class="n">tokens_edit</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">Debug</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;init mask:&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">[:</span><span class="n">Debug_token_len</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="n">Debug_token_len</span><span class="p">],</span> <span class="n">tokens_edit</span><span class="p">[:</span><span class="n">Debug_token_len</span><span class="p">],</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="n">SequenceMatcher</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tokens_edit</span><span class="p">)</span><span class="o">.</span><span class="n">get_opcodes</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">Debug</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;name:&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">a0:&quot;</span><span class="p">,</span><span class="n">a0</span><span class="p">,</span> <span class="s2">&quot;a1:&quot;</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">b0:&quot;</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="s2">&quot;b1:&quot;</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">b0</span> <span class="o">&lt;</span> <span class="n">tokens_length</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;equal&quot;</span> <span class="ow">or</span> <span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;replace&quot;</span> <span class="ow">and</span> <span class="n">a1</span><span class="o">-</span><span class="n">a0</span> <span class="o">==</span> <span class="n">b1</span><span class="o">-</span><span class="n">b0</span><span class="p">):</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">b0</span><span class="p">:</span><span class="n">b1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">indices</span><span class="p">[</span><span class="n">b0</span><span class="p">:</span><span class="n">b1</span><span class="p">]</span> <span class="o">=</span> <span class="n">indices_target</span><span class="p">[</span><span class="n">a0</span><span class="p">:</span><span class="n">a1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">Debug</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;final mask:&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">[:</span><span class="n">Debug_token_len</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;final indices:&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="n">Debug_token_len</span><span class="p">])</span>


    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn2&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span> <span class="c1"># 对于crossAttention而言, 需要mask以及indices</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s2">&quot;CrossAttention&quot;</span> <span class="ow">and</span> <span class="s2">&quot;attn1&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_mask</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">module</span><span class="o">.</span><span class="n">last_attn_slice_indices</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>
<p>看一下output:</p>
<div class="highlight"><pre><span></span><code>stablediffusion<span class="o">(</span><span class="s2">&quot;a cat sitting on a car&quot;</span>, <span class="s2">&quot;a smiling dog sitting on a car&quot;</span>,  <span class="nv">prompt_edit_spatial_start</span><span class="o">=</span><span class="m">0</span>.7, <span class="nv">seed</span><span class="o">=</span><span class="m">248396402679</span><span class="o">)</span>
-------
init mask: tensor<span class="o">([</span><span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">0</span>.<span class="o">])</span>
<span class="o">[</span><span class="m">49406</span>   <span class="m">320</span>  <span class="o">{</span><span class="m">2368</span><span class="o">}</span>  <span class="m">4919</span>   <span class="m">525</span>   <span class="m">320</span>  <span class="m">1615</span> <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span>
 <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span><span class="o">]</span>
<span class="o">[</span><span class="m">49406</span>   <span class="m">320</span>  <span class="o">{</span><span class="m">9200</span>  <span class="m">1929</span><span class="o">}</span>  <span class="m">4919</span>   <span class="m">525</span>   <span class="m">320</span>  <span class="m">1615</span> <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span>
 <span class="m">49407</span> <span class="m">49407</span> <span class="m">49407</span><span class="o">]</span>
name: equal 
a0: <span class="m">0</span> a1: <span class="m">2</span> 
b0: <span class="m">0</span> b1: <span class="m">2</span>
name: replace 
a0: <span class="m">2</span> a1: <span class="m">3</span> 
b0: <span class="m">2</span> b1: <span class="m">4</span>
name: equal 
a0: <span class="m">3</span> a1: <span class="m">76</span> 
b0: <span class="m">4</span> b1: <span class="m">77</span>
name: delete 
a0: <span class="m">76</span> a1: <span class="m">77</span> 
b0: <span class="m">77</span> b1: <span class="m">77</span>
final mask: tensor<span class="o">([</span><span class="m">1</span>., <span class="m">1</span>., <span class="m">0</span>., <span class="m">0</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>., <span class="m">1</span>.<span class="o">])</span>
final indices: tensor<span class="o">([</span> <span class="m">0</span>,  <span class="m">1</span>,  <span class="m">0</span>,  <span class="m">0</span>,  <span class="m">3</span>,  <span class="m">4</span>,  <span class="m">5</span>,  <span class="m">6</span>,  <span class="m">7</span>,  <span class="m">8</span>,  <span class="m">9</span>, <span class="m">10</span>, <span class="m">11</span>, <span class="m">12</span>, <span class="m">13</span><span class="o">])</span>
<span class="c1"># mask 为1的部分, 使用origin的attention map, mask为0的部分使用edit的attention map</span>
<span class="c1"># 至于来自origin的attention map来自那个word_index, 以及要放到哪个word_index, 则由 indices进行选择。</span>
</code></pre></div>
<p>mask和indices有什么用呢, 主要用在这里:</p>
<p>在我们修改句子或者替换attention map时, 我们期望相同的单词计算出的attention map是对应的, 也就是说如果严格按照单词顺序来对应attention map, 那么"a smiling dog sitting on a car"中的sitting对应的attention map会被替换为a对应的attention map, 但是对应的value没有改变, 这就导致了attention map的错位, 导致最后生成的语义信息不好。</p>
<p>如果不适用indices, 而直接采用按序替换所有attention map的方式, 同样的条件得到如下的图片。</p>
<p><img alt="WeChat816a8104fc1490fcf78081f0ae64a22c" src="../prompt-to-prompt.assets/WeChat816a8104fc1490fcf78081f0ae64a22c.png" /></p>
<h3 id="_6">各个参数控制原理</h3>
<div class="highlight"><pre><span></span><code><span class="n">stable</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># 原始生成过程</span>
<span class="n">prompt_edit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 后续修改的语句</span>
<span class="n">prompt_edit_token_weights</span><span class="o">=</span><span class="p">[],</span>  <span class="c1"># 一个token位置以及权重组成的tuple的列表, 在使用attention map时, 会进行一个reweight, 默认都为1</span>
<span class="n">prompt_edit_tokens_start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="c1"># 在处于 edit_tokens_start 和end 之间的迭代会将crossattention map 进行替换</span>
<span class="n">prompt_edit_tokens_end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
<span class="n">prompt_edit_spatial_start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="c1"># 在处于edit_spatial_start 和end之间的迭代会进行selfattention map的替换</span>
<span class="n">prompt_edit_spatial_end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
<span class="n">guidance_scale</span><span class="o">=</span><span class="mf">7.5</span><span class="p">,</span>  <span class="c1"># CFG的乘数</span>
<span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="c1"># 迭代轮数</span>
<span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># seed, readme中说seed相同才可以进行修改?</span>
<span class="n">width</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
<span class="n">height</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
<span class="n">init_image</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># 初始化的图片</span>
<span class="n">init_image_strength</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># 初始化的强度</span>

<span class="c1"># 这里需要注意的一个地方在于, 生成控制的顺序是从1到0的小数点, 控制的开始和结束貌似相反了</span>
<span class="k">if</span> <span class="n">prompt_edit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">t_scale</span> <span class="o">=</span> <span class="n">t</span> <span class="o">/</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">num_train_timesteps</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">t_scale</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">t_scale</span> <span class="o">&gt;=</span> <span class="n">prompt_edit_tokens_start</span> <span class="ow">and</span> <span class="n">t_scale</span> <span class="o">&lt;=</span> <span class="n">prompt_edit_tokens_end</span><span class="p">:</span>
    <span class="n">use_last_tokens_attention</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">t_scale</span> <span class="o">&gt;=</span> <span class="n">prompt_edit_spatial_start</span> <span class="ow">and</span> <span class="n">t_scale</span> <span class="o">&lt;=</span> <span class="n">prompt_edit_spatial_end</span><span class="p">:</span>
     <span class="n">use_last_self_attention</span><span class="p">()</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">这里得到的输出是:</span>
<span class="sd">tensor(0.9990, dtype=torch.float64)</span>
<span class="sd">tensor(0.9786, dtype=torch.float64)</span>
<span class="sd">tensor(0.9582, dtype=torch.float64)</span>
<span class="sd">tensor(0.9378, dtype=torch.float64)</span>
<span class="sd">tensor(0.9174, dtype=torch.float64)</span>
<span class="sd">tensor(0.8971, dtype=torch.float64)</span>
<span class="sd">tensor(0.8767, dtype=torch.float64)</span>
<span class="sd">tensor(0.8563, dtype=torch.float64)</span>
<span class="sd">tensor(0.8359, dtype=torch.float64)</span>
<span class="sd">tensor(0.8155, dtype=torch.float64)</span>
<span class="sd">tensor(0.7951, dtype=torch.float64)</span>
<span class="sd">tensor(0.7747, dtype=torch.float64)</span>
<span class="sd">tensor(0.7543, dtype=torch.float64)</span>
<span class="sd">tensor(0.7340, dtype=torch.float64)</span>
<span class="sd">tensor(0.7136, dtype=torch.float64)</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>
<h3 id="sliced_attention">sliced_attention在做什么?</h3>
<p>貌似真的没调用....</p>
<h3 id="_7">算法是不是有问题?</h3>
<p><img alt="image-20221024121624185" src="../prompt-to-prompt.assets/image-20221024121624185.png" /> </p>
<p>注意到, 这个不带星的z过程中, 从始至终都存在一个完成的生成链, 也就是最后的z0, 就是原始生成的图片, 而z*是edit后的图片。而我们目前跑的代码的实现是, zt 和 zt*在取两个Mt的过程中是共享的, 也就是说, 这里的第六行变为$$z_{t-1}, M_{t} \leftarrow DM(z_{t}^{*}, P, t, s)$$, 而$$z_{t-1}$$是被完整抛弃的。</p>
<p>为了这一点, 我们必须再改代码来验证, 这和condition的改变是有关系的, 具体的部分见“image condition的不同组合。</p>
<h2 id="attention-map">attention map 到底如何起作用?</h2>
<h3 id="_8">前提提要</h3>
<p>我们注意到, seed也可能是决定图片布局的一个因素。我们举个例子:</p>
<h6 id="seed-case1">seed case1</h6>
<p><code>seed=248396402679, steps=50</code></p>
<table>
<thead>
<tr>
<th>a cat sitting on a car</th>
<th>a smiling dog sitting on a car</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="image-20221022112218153" src="../prompt-to-prompt.assets/image-20221022112218153.png" /></td>
<td><img alt="image-20221022112240664" src="../prompt-to-prompt.assets/image-20221022112240664.png" /></td>
</tr>
<tr>
<td>a dog sitting on a car</td>
<td>a hamster sitting on a car</td>
</tr>
<tr>
<td><img alt="image-20221022112302069" src="../prompt-to-prompt.assets/image-20221022112302069.png" /></td>
<td><img alt="image-20221022112320448" src="../prompt-to-prompt.assets/image-20221022112320448.png" /></td>
</tr>
<tr>
<td>a tiger sitting on a car</td>
<td>a lion sitting on a car</td>
</tr>
<tr>
<td><img alt="image-20221022112341613" src="../prompt-to-prompt.assets/image-20221022112341613.png" /></td>
<td><img alt="image-20221022112457166" src="../prompt-to-prompt.assets/image-20221022112457166.png" /></td>
</tr>
</tbody>
</table>
<p>我们期望寻找一个seed, 使得不同的prompt得到的图片布局有比价明显的不一致性, 这样才能更加充分体现我们使用attention map进行编辑的用处。</p>
<h6 id="seed-case2">seed case2</h6>
<p><code>seed = 24839640267, steps=50</code></p>
<table>
<thead>
<tr>
<th>a cat sitting on a car</th>
<th>a smiling dog sitting on a car</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="image-20221022112841882" src="../prompt-to-prompt.assets/image-20221022112841882.png" /></td>
<td><img alt="image-20221022112922883" src="../prompt-to-prompt.assets/image-20221022112922883.png" /></td>
</tr>
<tr>
<td>a dog sitting on a car</td>
<td>a hamster sitting on a car</td>
</tr>
<tr>
<td><img alt="image-20221022112945353" src="../prompt-to-prompt.assets/image-20221022112945353.png" /></td>
<td><img alt="image-20221022113013842" src="../prompt-to-prompt.assets/image-20221022113013842.png" /></td>
</tr>
<tr>
<td>a tiger sitting on a car</td>
<td>a lion sitting on a car</td>
</tr>
<tr>
<td><img alt="image-20221022113141509" src="../prompt-to-prompt.assets/image-20221022113141509.png" /></td>
<td><img alt="image-20221022113038301" src="../prompt-to-prompt.assets/image-20221022113038301.png" /></td>
</tr>
</tbody>
</table>
<h4 id="crossattention-selfattention">crossattention是否有用? selfattention 是否有用?</h4>
<h6 id="cat-tiger">cat-tiger</h6>
<p><code>origin:cat, new:tiger</code>  , <code>left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0</code>  , 越上面CrossAttention 持续的越久, 越左边, selfAttention持续的越久</p>
<table>
<thead>
<tr>
<th><img alt="image-20221022112841882" src="../prompt-to-prompt.assets/image-20221022112841882.png" /></th>
<th><img alt="image-20221022113141509" src="../prompt-to-prompt.assets/image-20221022113141509.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>图中可以看出， <strong>attention map对于空域控制确实是有作用的</strong>，而将value替换为老虎的value同时也导致背后车变为车，这也有可能和attention map的扩散有关，即老虎的attention map权重大的空域并不全程在老虎身上。</p>
<p><img alt="cat-tiger" src="../prompt-to-prompt.assets/cat-tiger.jpg" /></p>
<h6 id="dog-hamster">dog-hamster</h6>
<p><code>origin:dog, new:hamster</code>  , <code>left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0</code> </p>
<table>
<thead>
<tr>
<th><img alt="image-20221022112945353" src="../prompt-to-prompt.assets/image-20221022112945353.png" /></th>
<th><img alt="image-20221022113013842" src="../prompt-to-prompt.assets/image-20221022113013842.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>左上角, 空域控制过强, 导致无法生成老鼠的形态, 注意到最后一行, 此时crossAttention并无inject, 仅仅selfAttention inject, 此时空域限制就不是很强。有比较好的效果, 但是却和文章中的selfAttention不够搭边了, 最后一步突然跳到原图, 也是有些匪夷所思, 需要再细致分析。</p>
<p><img alt="dog-hamster" src="../prompt-to-prompt.assets/dog-hamster.jpg" /></p>
<h6 id="dog-hamster-detail">dog-hamster-detail</h6>
<p><code>left to right ss=0.6-1.0, up to down cs=0.6-1.0 ce=se=1</code></p>
<p><img alt="dog-hamster-detail" src="../prompt-to-prompt.assets/dog-hamster-detail.jpg" /></p>
<h6 id="hamster-dog">hamster-dog</h6>
<p><code>origin:hamster, new:dog</code>  , <code>left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0</code> </p>
<p>这里还是体现出了空域控制很强的效果, 导致狗的毛发成色都和老鼠比较相近。注意到最右边的列, 此时只有crossAttention inject, 没有selfAttention inject, 大幅度的inject范围改变都没有对构图和风格有明显变化, 这也值得思考。最后一行的行为也非常奇怪。</p>
<p><img alt="hamster-dog" src="../prompt-to-prompt.assets/hamster-dog.jpg" /></p>
<h6 id="hamster-dog-detail">hamster-dog-detail</h6>
<p>0.6-1.0, 注意到<strong>初始的inject对构图的变化程度还是比较明显</strong>的, 例如出现了戴眼镜狗的图片。</p>
<p><img alt="hamster-dog-detail" src="../prompt-to-prompt.assets/hamster-dog-detail.jpg" /></p>
<h6 id="dog-smiling">dog-smiling</h6>
<p><code>origin:dog, new:smiling dog</code>  , <code>left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0</code> </p>
<table>
<thead>
<tr>
<th><img alt="image-20221022112945353" src="../prompt-to-prompt.assets/image-20221022112945353.png" /></th>
<th><img alt="image-20221022112922883" src="../prompt-to-prompt.assets/image-20221022112922883.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="dog-smiling dog" src="../prompt-to-prompt.assets/dog-smiling%20dog.jpg" /></p>
<h6 id="dog-smiling-detail">dog-smiling-detail</h6>
<p><code>left to right: ss0.6 to ss1.0, up to down: cs0.6 to cs1.0</code> </p>
<p><img alt="dog-smiling dog-detail" src="../prompt-to-prompt.assets/dog-smiling%20dog-detail.jpg" /></p>
<h6 id="dog-smiling-end">dog-smiling-end</h6>
<p>这里说明了, 前期是定结构位置的关键期, 前期一定, 后续再改比较麻烦, 当然, 这也是因为这里代码实现的Attention map是依赖于前一个latent的input的, 而不是独立的。</p>
<p><code>left to right: ce0.6 to ce1.0, up to down: se0.6 to se1.0, ss=cs=0.3</code></p>
<p><img alt="dog-smiling dog-end" src="../prompt-to-prompt.assets/dog-smiling%20dog-end.jpg" /> </p>
<h4 id="cake">cake 实验</h4>
<p><img alt="lemon cake" src="../prompt-to-prompt.assets/lemon%20cake.png" /></p>
<h6 id="cake_1">cake</h6>
<p><code>ss0.7, cs0.7, se=ce=1</code></p>
<table>
<thead>
<tr>
<th>apple</th>
<th>cheese</th>
<th>chocolate</th>
<th>jello</th>
<th>lego</th>
<th>matcha</th>
<th>pistachio</th>
<th>pumpkin</th>
</tr>
</thead>
<tbody>
<tr>
<td>苹果</td>
<td>芝士</td>
<td>巧克力</td>
<td>果冻</td>
<td>乐高</td>
<td>抹茶</td>
<td>开心果</td>
<td>南瓜</td>
</tr>
</tbody>
</table>
<p><img alt="origin-cakes" src="../prompt-to-prompt.assets/origin-cakes-6517914.jpg" /></p>
<p><img alt="inject-cakes" src="../prompt-to-prompt.assets/inject-cakes-6525536.jpg" /></p>
<h6 id="lemon-cheese">lemon-cheese</h6>
<p><code>ss0.0-1.0  cs0.0-1.0 lemon cheese</code></p>
<p><img alt="lemon cake-cheese cake" src="../prompt-to-prompt.assets/lemon%20cake-cheese%20cake.jpg" /></p>
<h6 id="lemon-pistachio">lemon-pistachio</h6>
<p><code>ss0.7-1.0, cs0.7-1.0 lemon pistachio</code></p>
<p><img alt="lemon cake-pistachio cake-detail" src="../prompt-to-prompt.assets/lemon%20cake-pistachio%20cake-detail.jpg" /></p>
<h4 id="value-map">value, map不同组合</h4>
<p>以上的矩阵就体现出了value, map的不同组合, 有一定的趋势可以证明map控制位置, value控制内容, 但这还需要image condition的验证。</p>
<h4 id="image-conditioninject">image condition的不同组合(并行inject)</h4>
<p>这里其实就是指并行inject, 同时也解释了算法是否有问题的问题。</p>
<h6 id="hamster-dog-self">hamster-dog-self</h6>
<table>
<thead>
<tr>
<th><img alt="image-20221022112945353" src="../prompt-to-prompt.assets/image-20221022112945353.png" /></th>
<th><img alt="image-20221022113013842" src="../prompt-to-prompt.assets/image-20221022113013842.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><code>left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0, se=ce=1</code></p>
<p><img alt="hamster-dog-self" src="../prompt-to-prompt.assets/hamster-dog-self.jpg" /></p>
<h6 id="margin">margin</h6>
<p><img alt="hamster-dog-margin-self" src="../prompt-to-prompt.assets/hamster-dog-margin-self.jpg" /></p>
<h6 id="dog-smiling-self">dog-smiling-self</h6>
<p><img alt="dog-smiling-self" src="../prompt-to-prompt.assets/dog-smiling-self.jpg" /></p>
<h6 id="dog-smiling-self_1">dog-smiling-self</h6>
<p><img alt="dog-smiling-detail-self" src="../prompt-to-prompt.assets/dog-smiling-detail-self.jpg" /></p>
<h4 id="attention_1">打印所有的生成过程以及attention</h4>
<h2 id="inject">手动inject</h2>
<h3 id="1">方法1</h3>
<p>实验方法,  选定图片的一块方形区域, 然后在Attention map这个区域中使得该部分的权重增加, 增加方式还有待尝试。</p>
<ol>
<li>只inject 64X64的attention map</li>
<li>绝对inject，mast相加后不做softmax，直接乘value</li>
</ol>
<h5 id="inject-scale-10">inject scale = 10</h5>
<p>这里首先阐明了我们injcet的位置的大致区域， 为暗红色部分。</p>
<p><img alt="image-20221102133158148" src="../prompt-to-prompt.assets/image-20221102133158148.png" /></p>
<h5 id="inject-scale4">inject scale=4</h5>
<p><img alt="image-20221102133139722" src="../prompt-to-prompt.assets/image-20221102133139722.png" /></p>
<h5 id="inject-scale3">inject scale=3</h5>
<p><img alt="image-20221102133756441" src="../prompt-to-prompt.assets/image-20221102133756441.png" /></p>
<h3 id="_9">值得探索的方向</h3>
<ol>
<li>self Attention</li>
<li>模型结构</li>
<li>dreambooth</li>
<li>动词</li>
<li>inpainting()</li>
<li>再过一次归一化(尝试各种方法)</li>
<li>调稳定一些</li>
</ol>
<h4 id="inject_1">手动inject结论</h4>
<ol>
<li>目前只inject 64X64, inject 32X32 或许效果会更好(已经验证)</li>
<li>inject 的面积未必能够限制生成动物的面积</li>
<li>有一定的控制作用</li>
<li>做softmax归一化似乎没有效果, 可能是softmax的维度</li>
</ol>
<h2 id="_10">结论</h2>
<ol>
<li>map决定空域, value决定内容基本成立</li>
<li>以上二者均受到image condition影响</li>
<li>并行inject 更合理</li>
</ol>
<p>t</p>
<p>z4 z3 z2 z1 z0</p>
<p>猫  ...</p>
<p>t*</p>
<p>z4* z3*  z2*  z1*</p>
<p>狗 ...</p>
<p>z4  z3  z3end  z2   z2end</p>
<p>​    z3*            z2*</p>
<h4 id="_11">问题:</h4>
<ol>
<li>dog-smiling 出现了人</li>
<li>出现像素化</li>
<li>控制的不是很理想</li>
</ol>
<h2 id="_12">注意到的问题</h2>
<h3 id="_13">其实生成控制没有很理想</h3>
<p>我们注意到代码中给的样例看起来不错, 但是稍微添加一些修改, 就会出现一些问题, 当然这任然需要再回头看一下论文并进行修改。</p>
<p>例如, 原来的实现中, 使用的原条件生成<code>“a cat sitting on a car”,    seed=248396402679</code>, 得到如下图片:</p>
<p><img alt="image-20221019113709789" src="../prompt-to-prompt.assets/image-20221019113709789.png" /></p>
<p>attention inject的方式, 参数为<code>"a cat sitting on a car", "a smiling dog sitting on a car", prompt_edit_spatial_start=0.7, seed=248396402679</code> 则如下图:</p>
<p><img alt="image-20221019113909102" src="../prompt-to-prompt.assets/image-20221019113909102.png" /></p>
<p>如果将参数改变为<code>"a cat sitting on a car", "a dog sitting on a car", prompt_edit_spatial_start=0.7,seed=248396402679,steps=50</code>, 则得到如下图片, 比较恐怖:</p>
<p><img alt="image-20221019114052787" src="../prompt-to-prompt.assets/image-20221019114052787.png" /></p>
<p>value, map不同组合, 抛弃selfattention , new_sliced_attention , seq compare</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../hand_inject/" class="md-footer__link md-footer__link--prev" aria-label="Previous: attention map hand inject" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              attention map hand inject
            </div>
          </div>
        </a>
      
      
        
        <a href="../textual_inversion/" class="md-footer__link md-footer__link--next" aria-label="Next: Textual inversion" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Textual inversion
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs"], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.37e9125f.min.js"></script>
      
    
  </body>
</html>