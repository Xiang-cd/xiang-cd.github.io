{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u6b22\u8fce\u6765\u5230\u9879\u5c0f\u7fbd\u7684\u5b66\u4e60\u4e0e\u79d1\u7814 \u8fd9\u91cc\u4e3b\u8981\u8bb0\u5f55\u9879\u5c0f\u7fbd\u7684\u5b66\u4e60\u548c\u79d1\u7814\u7ecf\u5386, \u79d1\u7814\u53ef\u80fd\u4ee5\u9879\u76ee\u4e3a\u4e3b, \u5b66\u4e60\u4ee5\u9886\u57df\u7684\u4f8b\u5b50\u4e3a\u4e3b, \u8fd8\u9644\u5e26\u4e86\u5176\u4ed6\u7684\u4e00\u4e9b\u5de5\u5177\u7684\u4f7f\u7528\u548c\u522b\u7684\u5185\u5bb9\u3002\u5982\u679c\u5185\u5bb9\u5199\u7684\u8fd8\u53ef\u4ee5\u7684\u8bdd\u4e5f\u53ef\u4ee5push\u6211\u53bb\u53d1B\u7ad9\u89c6\u9891(doge)\u3002 \u5feb! push\u6211 2022\u5e7410\u670814\u65e5 \u786e\u5b9e, \u6211\u53d1\u89c9\u5176\u5b9e\u5982\u679c\u5c06\u81ea\u5df1\u4e1c\u897f\u5199\u51fa\u6765, \u53d1\u73b0\u81ea\u5df1\u5176\u5b9e\u6ca1\u6709\u591a\u5c11\u4e1c\u897f, \u8fd8\u662f\u9700\u8981\u4e0d\u65ad\u7684\u7ec8\u8eab\u7684\u5b66\u4e60\u3002","title":"\u6b22\u8fce\u6765\u5230\u9879\u5c0f\u7fbd\u7684\u5b66\u4e60\u4e0e\u79d1\u7814"},{"location":"#_1","text":"\u8fd9\u91cc\u4e3b\u8981\u8bb0\u5f55\u9879\u5c0f\u7fbd\u7684\u5b66\u4e60\u548c\u79d1\u7814\u7ecf\u5386, \u79d1\u7814\u53ef\u80fd\u4ee5\u9879\u76ee\u4e3a\u4e3b, \u5b66\u4e60\u4ee5\u9886\u57df\u7684\u4f8b\u5b50\u4e3a\u4e3b, \u8fd8\u9644\u5e26\u4e86\u5176\u4ed6\u7684\u4e00\u4e9b\u5de5\u5177\u7684\u4f7f\u7528\u548c\u522b\u7684\u5185\u5bb9\u3002\u5982\u679c\u5185\u5bb9\u5199\u7684\u8fd8\u53ef\u4ee5\u7684\u8bdd\u4e5f\u53ef\u4ee5push\u6211\u53bb\u53d1B\u7ad9\u89c6\u9891(doge)\u3002 \u5feb! push\u6211","title":"\u6b22\u8fce\u6765\u5230\u9879\u5c0f\u7fbd\u7684\u5b66\u4e60\u4e0e\u79d1\u7814"},{"location":"#20221014","text":"\u786e\u5b9e, \u6211\u53d1\u89c9\u5176\u5b9e\u5982\u679c\u5c06\u81ea\u5df1\u4e1c\u897f\u5199\u51fa\u6765, \u53d1\u73b0\u81ea\u5df1\u5176\u5b9e\u6ca1\u6709\u591a\u5c11\u4e1c\u897f, \u8fd8\u662f\u9700\u8981\u4e0d\u65ad\u7684\u7ec8\u8eab\u7684\u5b66\u4e60\u3002","title":"2022\u5e7410\u670814\u65e5"},{"location":"%E5%85%B6%E4%BB%96/books/","text":"\u63a8\u8350\u51e0\u672c\u4e66? \u611f\u89c9\u81ea\u5df1\u8bfb\u7684\u4e66\u8fd8\u4e0d\u591f\u591a, \u5e0c\u671b\u53ef\u4ee5\u8fb9\u5199\u8fb9\u63a8\u8350, \u8fb9\u7763\u4fc3\u81ea\u5df1\u8bfb\u4e66\u5427\u3002 \u516b\u6b21\u5371\u673a","title":"\u63a8\u8350\u51e0\u672c\u4e66?"},{"location":"%E5%85%B6%E4%BB%96/books/#_1","text":"\u611f\u89c9\u81ea\u5df1\u8bfb\u7684\u4e66\u8fd8\u4e0d\u591f\u591a, \u5e0c\u671b\u53ef\u4ee5\u8fb9\u5199\u8fb9\u63a8\u8350, \u8fb9\u7763\u4fc3\u81ea\u5df1\u8bfb\u4e66\u5427\u3002","title":"\u63a8\u8350\u51e0\u672c\u4e66?"},{"location":"%E5%85%B6%E4%BB%96/books/#_2","text":"","title":"\u516b\u6b21\u5371\u673a"},{"location":"%E5%85%B6%E4%BB%96/music/","text":"\u597d\u6b4c\u63a8\u8350 \u5728\u8fd9\u91cc\u63a8\u8350\u4e00\u4e9b\u81ea\u5df1\u559c\u6b22\u542c\u7684\u6b4c, \u9884\u671f\u63a8\u8350\u7684\u6b4c\u90fd\u662f\u7981\u5f97\u8d77\u65f6\u95f4\u6253\u78e8\u7684\u6b4c, \u4e0d\u662f\u521a\u5f00\u59cb\u542c\u89c9\u5f97\u4e0d\u9519\u6216\u8005\u6d17\u8111, \u542c\u4e00\u6574\u5b50\u5c31\u4e0d\u4f1a\u518d\u542c\u4e86\u3002\u6211\u89c9\u5f97\u6709\u4e9b\u7ecf\u5178\u7684\u6b4c\u975e\u5e38\u8010\u542c, \u800c\u4e14\u4e0d\u540c\u7684\u65f6\u671f\u542c\u4e5f\u6709\u4e0d\u540c\u7684\u611f\u89c9\u3002 \u4e0d\u8fc7\u672c\u4eba\u4e0d\u662f\u975e\u5e38\u6f6e, \u9648\u5955\u8fc5, \u5468\u6770\u4f26, \u738b\u83f2\u53ef\u80fd\u5360\u4e86\u6211\u4e00\u6bb5\u65f6\u671f\u7684\u5927\u90e8\u5206\u4e3b\u6d41\u6b4c\u66f2, \u63a8\u8350\u7684\u4e0d\u5168\u9762\u5e0c\u671b\u4e0d\u8981\u89c1\u602a\u3002 \u518d\u89c1\u4e8c\u4e01\u76ee \u6768\u5343\u5b05 \u4ece\u66f2\u8c03\u770b, \u8fd9\u9996\u6b4c\u5f00\u5934\u7684\u94a2\u7434\u66f2\u975e\u5e38\u60a0\u626c\u5927\u6c14, \u53ef\u4ee5\u8ba9\u4eba\u7acb\u523b\u8fdb\u5165\u6b4c\u66f2\u7684\u60c5\u666f\u4e2d\u3002\u6b4c\u624b\u6f14\u5531\u611f\u60c5\u975e\u5e38\u7ec6\u817b, \u628a\u90a3\u79cd\u6de1\u6de1\u7684\u5fe7\u4f24\u8868\u8fbe\u7684\u5f88\u5230\u4f4d, \u5176\u5b9e\u6211\u5f00\u59cb\u542c\u89c9\u5f97\u751a\u81f3\u8fd8\u89c9\u5f97\u662f\u6bd4\u8f83\u8f7b\u5feb\u7684\u6b4c\u66f2, \u770b\u4e86\u6b4c\u8bcd\u4e4b\u540e\u6b4c\u66f2\u8981\u8868\u8fbe\u7684\u611f\u60c5\u7acb\u523b\u6e05\u6670\u8868\u73b0\u5728\u8111\u6d77\u4e2d\u3002\u8fd9\u9996\u6b4c\u767e\u542c\u4e0d\u538c, \u8d8a\u542c\u8d8a\u7ec6\u817b, \u975e\u5e38\u63a8\u8350\u3002 \u8fd9\u9996\u6b4c\u7684\u6b4c\u8bcd\u4e5f\u662f\u4e00\u7edd, \u6b4c\u8bcd\u7684\u80cc\u666f\u662f\u4f5c\u8005\u5728\u5f02\u56fd\u4e8c\u4e01\u76ee\u7b49\u53cb\u4eba\u8d74\u7ea6, \u4f46\u662f\u53cb\u4eba\u8fdf\u8fdf\u4e0d\u6765, \u6240\u4ee5\u624d\u5199\u4e0b\u4e86\u8fd9\u8bcd\u3002\u5176\u4e2d\u201c\u539f\u6765\u8fc7\u7684\u5f88\u5feb\u4e50, \u53ea\u6211\u4e00\u4eba\u672a\u53d1\u89c9\u201d\u6211\u8ba4\u4e3a\u6574\u9996\u6b4c\u6700\u4e3a\u7edd\u5999\u7684\u5730\u65b9, \u5176\u5b9e\u4e0d\u5feb\u4e50\u7684\u5730\u65b9\u662f\u66f2\u4e2d\u5bf9\u6709\u4eba\u7684\u601d\u5ff5, \u601d\u5ff5\u548c\u671f\u671b\u592a\u6df1, \u4eba\u9677\u8fdb\u53bb\u4e86, \u5feb\u4e50\u672a\u80fd\u611f\u53d7, \u6709\u65f6\u751a\u81f3\u60b2\u4f24\u90fd\u4e0d\u80fd\u611f\u53d7\u3002","title":"\u597d\u6b4c\u63a8\u8350"},{"location":"%E5%85%B6%E4%BB%96/music/#_1","text":"\u5728\u8fd9\u91cc\u63a8\u8350\u4e00\u4e9b\u81ea\u5df1\u559c\u6b22\u542c\u7684\u6b4c, \u9884\u671f\u63a8\u8350\u7684\u6b4c\u90fd\u662f\u7981\u5f97\u8d77\u65f6\u95f4\u6253\u78e8\u7684\u6b4c, \u4e0d\u662f\u521a\u5f00\u59cb\u542c\u89c9\u5f97\u4e0d\u9519\u6216\u8005\u6d17\u8111, \u542c\u4e00\u6574\u5b50\u5c31\u4e0d\u4f1a\u518d\u542c\u4e86\u3002\u6211\u89c9\u5f97\u6709\u4e9b\u7ecf\u5178\u7684\u6b4c\u975e\u5e38\u8010\u542c, \u800c\u4e14\u4e0d\u540c\u7684\u65f6\u671f\u542c\u4e5f\u6709\u4e0d\u540c\u7684\u611f\u89c9\u3002 \u4e0d\u8fc7\u672c\u4eba\u4e0d\u662f\u975e\u5e38\u6f6e, \u9648\u5955\u8fc5, \u5468\u6770\u4f26, \u738b\u83f2\u53ef\u80fd\u5360\u4e86\u6211\u4e00\u6bb5\u65f6\u671f\u7684\u5927\u90e8\u5206\u4e3b\u6d41\u6b4c\u66f2, \u63a8\u8350\u7684\u4e0d\u5168\u9762\u5e0c\u671b\u4e0d\u8981\u89c1\u602a\u3002","title":"\u597d\u6b4c\u63a8\u8350"},{"location":"%E5%85%B6%E4%BB%96/music/#_2","text":"\u4ece\u66f2\u8c03\u770b, \u8fd9\u9996\u6b4c\u5f00\u5934\u7684\u94a2\u7434\u66f2\u975e\u5e38\u60a0\u626c\u5927\u6c14, \u53ef\u4ee5\u8ba9\u4eba\u7acb\u523b\u8fdb\u5165\u6b4c\u66f2\u7684\u60c5\u666f\u4e2d\u3002\u6b4c\u624b\u6f14\u5531\u611f\u60c5\u975e\u5e38\u7ec6\u817b, \u628a\u90a3\u79cd\u6de1\u6de1\u7684\u5fe7\u4f24\u8868\u8fbe\u7684\u5f88\u5230\u4f4d, \u5176\u5b9e\u6211\u5f00\u59cb\u542c\u89c9\u5f97\u751a\u81f3\u8fd8\u89c9\u5f97\u662f\u6bd4\u8f83\u8f7b\u5feb\u7684\u6b4c\u66f2, \u770b\u4e86\u6b4c\u8bcd\u4e4b\u540e\u6b4c\u66f2\u8981\u8868\u8fbe\u7684\u611f\u60c5\u7acb\u523b\u6e05\u6670\u8868\u73b0\u5728\u8111\u6d77\u4e2d\u3002\u8fd9\u9996\u6b4c\u767e\u542c\u4e0d\u538c, \u8d8a\u542c\u8d8a\u7ec6\u817b, \u975e\u5e38\u63a8\u8350\u3002 \u8fd9\u9996\u6b4c\u7684\u6b4c\u8bcd\u4e5f\u662f\u4e00\u7edd, \u6b4c\u8bcd\u7684\u80cc\u666f\u662f\u4f5c\u8005\u5728\u5f02\u56fd\u4e8c\u4e01\u76ee\u7b49\u53cb\u4eba\u8d74\u7ea6, \u4f46\u662f\u53cb\u4eba\u8fdf\u8fdf\u4e0d\u6765, \u6240\u4ee5\u624d\u5199\u4e0b\u4e86\u8fd9\u8bcd\u3002\u5176\u4e2d\u201c\u539f\u6765\u8fc7\u7684\u5f88\u5feb\u4e50, \u53ea\u6211\u4e00\u4eba\u672a\u53d1\u89c9\u201d\u6211\u8ba4\u4e3a\u6574\u9996\u6b4c\u6700\u4e3a\u7edd\u5999\u7684\u5730\u65b9, \u5176\u5b9e\u4e0d\u5feb\u4e50\u7684\u5730\u65b9\u662f\u66f2\u4e2d\u5bf9\u6709\u4eba\u7684\u601d\u5ff5, \u601d\u5ff5\u548c\u671f\u671b\u592a\u6df1, \u4eba\u9677\u8fdb\u53bb\u4e86, \u5feb\u4e50\u672a\u80fd\u611f\u53d7, \u6709\u65f6\u751a\u81f3\u60b2\u4f24\u90fd\u4e0d\u80fd\u611f\u53d7\u3002","title":"\u518d\u89c1\u4e8c\u4e01\u76ee \u6768\u5343\u5b05"},{"location":"%E5%85%B6%E4%BB%96/website/","text":"\u5728\u8fd9\u91cc\u5206\u4eab\u4e00\u4e9b\u6709\u7528\u7684\u7f51\u5740 [toc] \u4e00\u4e2a\u7528\u4e8e\u753b\u7535\u8def\u65f6\u5e8f\u56fe\u7684\u7f51\u5740 https://wavedrom.com/editor.html \u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u7684\u7f51\u5740 https://regexr.com/ \u753b\u4e00\u4e9b\u7eaf\u5b57\u7b26\u7684\u56fe https://www.asciiart.eu/faq https://manytools.org/hacker-tools/ascii-banner \u6216\u8005\u76f4\u63a5\u641c\u7d22 ascii banner \u4e00\u4e2a\u67e5\u8be2AI\u7b97\u6cd5\u8868\u73b0\u548c\u6570\u636e\u96c6\u8868\u73b0\u7684\u7f51\u5740 https://paperswithcode.com/ \u4e00\u4e2a\u4e70VPS\u7684\u7f51\u5740 https://nthu.cc/#/plan \u6e05\u534e\u5927\u5b66\u4f18\u79c0\u6bd5\u4e1a\u8bba\u6587 http://reserves.lib.tsinghua.edu.cn/zhuantibooks?topic=7.%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E7%BB%BC%E5%90%88%E8%AE%BA%E6%96%87%E8%AE%AD%E7%BB%83%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87%EF%BC%882021%EF%BC%89 \u4e00\u4e9b\u535a\u5ba2 https://pluskid.org/ https://kexue.fm/ mac \u7206\u7834\u8f6f\u4ef6 https://appstorrent.ru/","title":"\u5728\u8fd9\u91cc\u5206\u4eab\u4e00\u4e9b\u6709\u7528\u7684\u7f51\u5740"},{"location":"%E5%85%B6%E4%BB%96/website/#_1","text":"[toc]","title":"\u5728\u8fd9\u91cc\u5206\u4eab\u4e00\u4e9b\u6709\u7528\u7684\u7f51\u5740"},{"location":"%E5%85%B6%E4%BB%96/website/#_2","text":"https://wavedrom.com/editor.html","title":"\u4e00\u4e2a\u7528\u4e8e\u753b\u7535\u8def\u65f6\u5e8f\u56fe\u7684\u7f51\u5740"},{"location":"%E5%85%B6%E4%BB%96/website/#_3","text":"https://regexr.com/","title":"\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u7684\u7f51\u5740"},{"location":"%E5%85%B6%E4%BB%96/website/#_4","text":"https://www.asciiart.eu/faq https://manytools.org/hacker-tools/ascii-banner \u6216\u8005\u76f4\u63a5\u641c\u7d22 ascii banner","title":"\u753b\u4e00\u4e9b\u7eaf\u5b57\u7b26\u7684\u56fe"},{"location":"%E5%85%B6%E4%BB%96/website/#ai","text":"https://paperswithcode.com/","title":"\u4e00\u4e2a\u67e5\u8be2AI\u7b97\u6cd5\u8868\u73b0\u548c\u6570\u636e\u96c6\u8868\u73b0\u7684\u7f51\u5740"},{"location":"%E5%85%B6%E4%BB%96/website/#vps","text":"https://nthu.cc/#/plan","title":"\u4e00\u4e2a\u4e70VPS\u7684\u7f51\u5740"},{"location":"%E5%85%B6%E4%BB%96/website/#_5","text":"http://reserves.lib.tsinghua.edu.cn/zhuantibooks?topic=7.%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E7%BB%BC%E5%90%88%E8%AE%BA%E6%96%87%E8%AE%AD%E7%BB%83%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87%EF%BC%882021%EF%BC%89","title":"\u6e05\u534e\u5927\u5b66\u4f18\u79c0\u6bd5\u4e1a\u8bba\u6587"},{"location":"%E5%85%B6%E4%BB%96/website/#_6","text":"https://pluskid.org/ https://kexue.fm/","title":"\u4e00\u4e9b\u535a\u5ba2"},{"location":"%E5%85%B6%E4%BB%96/website/#mac","text":"https://appstorrent.ru/","title":"mac \u7206\u7834\u8f6f\u4ef6"},{"location":"%E5%AD%A6%E4%B9%A0/%E7%AE%80%E4%BB%8B/","text":"\u4e00\u4e2a\u7b80\u5355\u7684\u7b80\u4ecb","title":"\u4e00\u4e2a\u7b80\u5355\u7684\u7b80\u4ecb"},{"location":"%E5%AD%A6%E4%B9%A0/%E7%AE%80%E4%BB%8B/#_1","text":"","title":"\u4e00\u4e2a\u7b80\u5355\u7684\u7b80\u4ecb"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ASC22/","text":"ASC22 \u8fd9\u662f\u6211\u5728\u8d85\u7b97\u961f\u53c2\u52a0\u7684ASC2022, \u4e3b\u8981\u53c2\u4e0e\u7684\u662fDeePMD\u3002","title":"ASC22"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ASC22/#asc22","text":"\u8fd9\u662f\u6211\u5728\u8d85\u7b97\u961f\u53c2\u52a0\u7684ASC2022, \u4e3b\u8981\u53c2\u4e0e\u7684\u662fDeePMD\u3002","title":"ASC22"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/IPCC22/","text":"IPCC2022 IPCC\u53c8\u53ebACM\u4e2d\u56fd\u56fd\u9645\u5e76\u884c\u8ba1\u7b97\u6311\u6218\u8d5b, \u7f51\u5740 \u3002 \u4ee5\u76ee\u524d\u7684\u53c2\u8d5b\u7ecf\u9a8c\u6765\u770b, \u662f\u9762\u5411\u5c0f\u4ee3\u7801\u89c4\u6a21\u7684 \u521d\u8d5b \u521d\u8d5b\u7684\u8d5b\u9898\u662f\u4e00\u4e2a\u5bfb\u627e\u652f\u6491\u70b9\u7684\u7b97\u6cd5, \u5177\u4f53\u7684\u4ecb\u7ecd\u5728 \u8fd9\u91cc \u8d5b\u65b9\u63d0\u4f9b\u7684\u8d44\u6e90\u662f\u6700\u591a\u4e24\u673a, \u6bcf\u4e2a\u673a\u566864\u6838\u3002 \u8d5b\u65b9\u63d0\u4f9b\u4e86\u6700\u521d\u7248\u672c\u7684\u7248\u672c, \u6211\u4eec\u57fa\u4e8e\u8fd9\u4e2a\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316, \u4ee3\u7801\u6211\u8d34\u51fa\u6765, \u5e76\u5728\u6ce8\u91ca\u4e2d\u8fdb\u884c\u89e3\u91ca\u3002 \u603b\u4f53\u7684\u89c4\u5219\u662f\u9664\u4e86\u6587\u4ef6\u8bfb\u5199\u90e8\u5206\u4e0d\u8ba1\u65f6, \u5176\u4ed6\u90e8\u5206\u90fd\u9700\u8981\u8ba1\u65f6, \u5305\u62ec MPI_int() , \u5e76\u4e14\u4e0d\u80fd\u9488\u5bf9\u7b97\u4f8b\u4f18\u5316, \u4e5f\u5c31\u662f\u4f60\u4e0d\u80fd\u901a\u8fc7\u5224\u65ad\u4e0d\u540c\u7684\u7b97\u4f8b\u89c4\u6a21\u6765\u6539\u53d8\u5e76\u884c\u6216\u8005\u7b97\u6cd5\u7684\u7b56\u7565\u3002","title":"IPCC2022"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/IPCC22/#ipcc2022","text":"IPCC\u53c8\u53ebACM\u4e2d\u56fd\u56fd\u9645\u5e76\u884c\u8ba1\u7b97\u6311\u6218\u8d5b, \u7f51\u5740 \u3002 \u4ee5\u76ee\u524d\u7684\u53c2\u8d5b\u7ecf\u9a8c\u6765\u770b, \u662f\u9762\u5411\u5c0f\u4ee3\u7801\u89c4\u6a21\u7684","title":"IPCC2022"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/IPCC22/#_1","text":"\u521d\u8d5b\u7684\u8d5b\u9898\u662f\u4e00\u4e2a\u5bfb\u627e\u652f\u6491\u70b9\u7684\u7b97\u6cd5, \u5177\u4f53\u7684\u4ecb\u7ecd\u5728 \u8fd9\u91cc \u8d5b\u65b9\u63d0\u4f9b\u7684\u8d44\u6e90\u662f\u6700\u591a\u4e24\u673a, \u6bcf\u4e2a\u673a\u566864\u6838\u3002 \u8d5b\u65b9\u63d0\u4f9b\u4e86\u6700\u521d\u7248\u672c\u7684\u7248\u672c, \u6211\u4eec\u57fa\u4e8e\u8fd9\u4e2a\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316, \u4ee3\u7801\u6211\u8d34\u51fa\u6765, \u5e76\u5728\u6ce8\u91ca\u4e2d\u8fdb\u884c\u89e3\u91ca\u3002 \u603b\u4f53\u7684\u89c4\u5219\u662f\u9664\u4e86\u6587\u4ef6\u8bfb\u5199\u90e8\u5206\u4e0d\u8ba1\u65f6, \u5176\u4ed6\u90e8\u5206\u90fd\u9700\u8981\u8ba1\u65f6, \u5305\u62ec MPI_int() , \u5e76\u4e14\u4e0d\u80fd\u9488\u5bf9\u7b97\u4f8b\u4f18\u5316, \u4e5f\u5c31\u662f\u4f60\u4e0d\u80fd\u901a\u8fc7\u5224\u65ad\u4e0d\u540c\u7684\u7b97\u4f8b\u89c4\u6a21\u6765\u6539\u53d8\u5e76\u884c\u6216\u8005\u7b97\u6cd5\u7684\u7b56\u7565\u3002","title":"\u521d\u8d5b"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ISC23/","text":"ISC23 \u8fd9\u5927\u6982\u662f\u6211\u7b2c\u4e00\u6b21\u4f5c\u4e3a\u6b63\u5f0f\u961f\u5458, \u548c\u8d85\u7b97\u961f\u4e00\u8d77\u53c2\u52a0ISC\u7684\u6bd4\u8d5b, \u5e0c\u671b\u81ea\u5df1\u80fd\u591f\u8db3\u591f\u52aa\u529b, \u83b7\u5f97\u8db3\u591f\u597d\u7684\u6210\u7ee9\u3002 \u524d\u671f\u51c6\u5907 \u63d0\u4f9b\u62a5\u540d\u4fe1\u606f \u7b2c\u4e00\u9879 Reasons for Participation \u6837\u4f8b \u201cI believe the HPC contest is one of the best ways for an undergraduate to learn more about supercomputer and real-world HPC applications. It\u2019s also a great chance to take what we have learnt in class to practice and improve our ability through teamwork.\u201d` \u7b2c\u4e8c\u9879 Diverse Skills and Experience \u6837\u4f8b Yutian Wang has experience in developing Infiniband and RDMA applications, and has developed a DevOps-Oriented Containers-as-a-service Platform. He is also taking a second degree in Digital Media Art. \u7b2c\u4e09\u9879 Prior Competition Experiences \u7b2c\u4e00\u9879 Reasons for Participation I deeply realize the importance of high-performance computing in life and production. I sincerely hope that I can get in touch with actual high-performance computing applications and improve my programming and practicing capabilities with my teamates through competitions. \u7b2c\u4e8c\u9879 Diverse Skills and Experience Chendong Xiang has experience in developing QEMU, adding new hardware feature(user interrupt) in X86 architechure to get better interprocess communication performance. He also have attend ACM-China International Parallel Computing Challenge(IPCC), optimize Pivot selection algorithm and feGRASS algorithm.","title":"ISC23"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ISC23/#isc23","text":"\u8fd9\u5927\u6982\u662f\u6211\u7b2c\u4e00\u6b21\u4f5c\u4e3a\u6b63\u5f0f\u961f\u5458, \u548c\u8d85\u7b97\u961f\u4e00\u8d77\u53c2\u52a0ISC\u7684\u6bd4\u8d5b, \u5e0c\u671b\u81ea\u5df1\u80fd\u591f\u8db3\u591f\u52aa\u529b, \u83b7\u5f97\u8db3\u591f\u597d\u7684\u6210\u7ee9\u3002","title":"ISC23"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ISC23/#_1","text":"","title":"\u524d\u671f\u51c6\u5907"},{"location":"%E5%AD%A6%E4%B9%A0/%E8%B6%85%E7%AE%97/ISC23/#_2","text":"\u7b2c\u4e00\u9879 Reasons for Participation \u6837\u4f8b \u201cI believe the HPC contest is one of the best ways for an undergraduate to learn more about supercomputer and real-world HPC applications. It\u2019s also a great chance to take what we have learnt in class to practice and improve our ability through teamwork.\u201d` \u7b2c\u4e8c\u9879 Diverse Skills and Experience \u6837\u4f8b Yutian Wang has experience in developing Infiniband and RDMA applications, and has developed a DevOps-Oriented Containers-as-a-service Platform. He is also taking a second degree in Digital Media Art. \u7b2c\u4e09\u9879 Prior Competition Experiences \u7b2c\u4e00\u9879 Reasons for Participation I deeply realize the importance of high-performance computing in life and production. I sincerely hope that I can get in touch with actual high-performance computing applications and improve my programming and practicing capabilities with my teamates through competitions. \u7b2c\u4e8c\u9879 Diverse Skills and Experience Chendong Xiang has experience in developing QEMU, adding new hardware feature(user interrupt) in X86 architechure to get better interprocess communication performance. He also have attend ACM-China International Parallel Computing Challenge(IPCC), optimize Pivot selection algorithm and feGRASS algorithm.","title":"\u63d0\u4f9b\u62a5\u540d\u4fe1\u606f"},{"location":"%E5%B7%A5%E5%85%B7/","text":"\u7b80\u4ecb \u8fd9\u91cc\u4e3b\u8981\u4f1a\u4ecb\u7ecd\u9879\u5c0f\u7fbd\u4f7f\u7528\u7684\u4e00\u4e9b\u5de5\u5177\u4ee5\u53ca\u7528\u6cd5, \u4e3b\u8981\u5305\u62ec\u5982\u4e0b\u51e0\u4e2a\u65b9\u9762: \u5de5\u5177\u540d\u79f0 \u5185\u5bb9\u7b80\u4ecb Mac win\u8f6cmac\u7528\u4e0d\u719f? \u7b80\u5355\u4ecb\u7ecdMac\u539f\u751f\u7cfb\u7edf\u7684\u4f7f\u7528 Alfred mac \u6548\u7387\u795e\u5668, \u529f\u80fd\u5f3a\u5927, mac\u5fc5\u5907 notion \u7b14\u8bb0\u795e\u5668, \u81ea\u7531\u5b9a\u4e49, \u529f\u80fd\u592a\u591a\u8fd8\u5728\u63a2\u7d22 marginnote \u8bfb\u6587\u732e\u795e\u5668, \u81ea\u52a8\u7ffb\u8bd1, OCR, \u8111\u56fe\u6784\u5efa, \u601d\u8def\u6e05\u6670, \u544a\u522b\u624b\u5199\u7b14\u8bb0 Hammerspoon mac\u81ea\u52a8\u5316\u811a\u672c, \u975e\u5e38\u5f3a\u5927, \u6211\u4e5f\u53ea\u7528\u4e86\u4e00\u70b9\u8fb9\u89d2 Karabiner-Elements \u952e\u76d8\u6539\u952e\u795e\u5668, \u81ea\u5b9a\u4e49\u4f60\u7684\u5feb\u6377\u952e Vscode \u5176\u5b9e\u6211\u4e5f\u4e0d\u662fvscode\u7684\u6df1\u5ea6\u7528\u6237, \u4f46\u6211\u53ef\u4ee5\u5206\u4eab\u51e0\u4e2a\u6709\u7528\u7684\u63d2\u4ef6","title":"\u7b80\u4ecb"},{"location":"%E5%B7%A5%E5%85%B7/#_1","text":"\u8fd9\u91cc\u4e3b\u8981\u4f1a\u4ecb\u7ecd\u9879\u5c0f\u7fbd\u4f7f\u7528\u7684\u4e00\u4e9b\u5de5\u5177\u4ee5\u53ca\u7528\u6cd5, \u4e3b\u8981\u5305\u62ec\u5982\u4e0b\u51e0\u4e2a\u65b9\u9762: \u5de5\u5177\u540d\u79f0 \u5185\u5bb9\u7b80\u4ecb Mac win\u8f6cmac\u7528\u4e0d\u719f? \u7b80\u5355\u4ecb\u7ecdMac\u539f\u751f\u7cfb\u7edf\u7684\u4f7f\u7528 Alfred mac \u6548\u7387\u795e\u5668, \u529f\u80fd\u5f3a\u5927, mac\u5fc5\u5907 notion \u7b14\u8bb0\u795e\u5668, \u81ea\u7531\u5b9a\u4e49, \u529f\u80fd\u592a\u591a\u8fd8\u5728\u63a2\u7d22 marginnote \u8bfb\u6587\u732e\u795e\u5668, \u81ea\u52a8\u7ffb\u8bd1, OCR, \u8111\u56fe\u6784\u5efa, \u601d\u8def\u6e05\u6670, \u544a\u522b\u624b\u5199\u7b14\u8bb0 Hammerspoon mac\u81ea\u52a8\u5316\u811a\u672c, \u975e\u5e38\u5f3a\u5927, \u6211\u4e5f\u53ea\u7528\u4e86\u4e00\u70b9\u8fb9\u89d2 Karabiner-Elements \u952e\u76d8\u6539\u952e\u795e\u5668, \u81ea\u5b9a\u4e49\u4f60\u7684\u5feb\u6377\u952e Vscode \u5176\u5b9e\u6211\u4e5f\u4e0d\u662fvscode\u7684\u6df1\u5ea6\u7528\u6237, \u4f46\u6211\u53ef\u4ee5\u5206\u4eab\u51e0\u4e2a\u6709\u7528\u7684\u63d2\u4ef6","title":"\u7b80\u4ecb"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/","text":"\u9879\u5c0f\u7fbd\u8bd5\u56fe\u6559\u4f60\u7528mac \u5feb\u6377\u952e finder preview \u6d3b\u52a8\u76d1\u89c6\u5668 terminal \u8f93\u5165\u6cd5, \u6587\u672c\u66ff\u6362 \u5c0f\u8f6f\u4ef6\u63a8\u8350\u5217\u8868 \u4e00\u4e9b\u975e\u5e38\u5bb9\u6613\u4f7f\u7528\u7684\u8f6f\u4ef6, \u4ec5\u505a\u63a8\u8350, \u4e0d\u505a\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"\u9879\u5c0f\u7fbd\u8bd5\u56fe\u6559\u4f60\u7528mac"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#mac","text":"","title":"\u9879\u5c0f\u7fbd\u8bd5\u56fe\u6559\u4f60\u7528mac"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#_1","text":"","title":"\u5feb\u6377\u952e"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#finder","text":"","title":"finder"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#preview","text":"","title":"preview"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#_2","text":"","title":"\u6d3b\u52a8\u76d1\u89c6\u5668"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#terminal","text":"","title":"terminal"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#_3","text":"","title":"\u8f93\u5165\u6cd5, \u6587\u672c\u66ff\u6362"},{"location":"%E5%B7%A5%E5%85%B7/teach%20you%20mac/#_4","text":"\u4e00\u4e9b\u975e\u5e38\u5bb9\u6613\u4f7f\u7528\u7684\u8f6f\u4ef6, \u4ec5\u505a\u63a8\u8350, \u4e0d\u505a\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"\u5c0f\u8f6f\u4ef6\u63a8\u8350\u5217\u8868"},{"location":"%E5%B7%A5%E5%85%B7/vscode/","text":"Vscode\u914d\u7f6e C++ python Rust remote \u5176\u4ed6\u63d2\u4ef6 vim Verilog markdown ToDo Tree","title":"Vscode\u914d\u7f6e"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#vscode","text":"","title":"Vscode\u914d\u7f6e"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#c","text":"","title":"C++"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#python","text":"","title":"python"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#rust","text":"","title":"Rust"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#remote","text":"","title":"remote"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#_1","text":"","title":"\u5176\u4ed6\u63d2\u4ef6"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#vim","text":"","title":"vim"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#verilog","text":"","title":"Verilog"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#markdown","text":"","title":"markdown"},{"location":"%E5%B7%A5%E5%85%B7/vscode/#todo-tree","text":"","title":"ToDo Tree"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/","text":"\u5982\u4f55\u8de8\u8d8a\u957f\u57ce \u5728\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4ee5\u4e0b\u51e0\u4e2a\u4e1c\u897f: 1. VPS 2. \u81ea\u5df1\u7684\u57df\u540d 3. V2ray\u5ba2\u6237\u7aef 4. \u61c2\u5f97\u5982\u4f55\u914d\u7f6e \u68c0\u67e5\u5404\u4e2a\u6587\u4ef6\u7684\u60c5\u51b5: /etc/nginx/conf.d/default.conf \u6587\u4ef6\u4e0d\u5b58\u5728 /etc/nginx/conf/nginx.conf conf\u6587\u4ef6\u5939\u4e0d\u5b58\u5728 \u4fee\u6539\u8def\u5f84\u4e3a/etc/nginx/nginx.conf /etc/nginx/conf.d/default.conf default.conf\u4e0d\u5b58\u5728 /etc/nginx/ssl $conffile root /var/www/ws \u662f\u4ec0\u4e48\u6587\u4ef6, \u5176\u4f5c\u7528\u662f\u4ec0\u4e48 index.html \u662f\u540c\u76ee\u5f55\u4e0b\u7684\u4e00\u4e2a\u6587\u4ef6\u5417? \u6709\u4ec0\u4e48\u7528\u5462 server_name ws.pwe.cat; ws.pwe.cat\u8fd9\u4e2a\u57df\u540d\u4e2d\u7684ws\u6709\u4ec0\u4e48\u7279\u6b8a\u4f5c\u7528\u5417, \u6d4f\u89c8\u5668\u8bbf\u95ee\u5e76\u65e0\u4f5c\u7528 sudo ln -s /etc/nginx/sites-available/ws /etc/nginx/sites-enabled/ \u8fd9\u4e00\u6b65\u7684\u8f6f\u8fde\u63a5\u6709\u4ec0\u4e48\u4f5c\u7528\u5462 \u5982\u679c\u5f53\u524dvmess\u5df2\u7ecf\u65e0\u6cd5\u5de5\u4f5c, \u4f7f\u7528nginx \u53cd\u4ee3\u80fd\u5426work? (gfw\u5c01\u9488\u5bf9\u7684\u662f\u5ba2\u6237\u7aefip\u8fd8\u662fVPSip]) TCP + TLS + Web \u80cc\u666f \u76ee\u524d Vmess + WebSocket + TLS \uff08\u4ee5\u4e0b\u7b80\u79f0 wss\uff09\u65b9\u5f0f\uff0c\u56e0\u5176\u7279\u5f81\u5982\u540c HTTPS \u6d41\u91cf\uff0c\u53ef\u4ee5\u9690\u85cf V2Ray \u8def\u5f84\uff0c\u4e3b\u52a8\u4fa6\u6d4b\u4f1a\u5f97\u5230\u6b63\u5e38 HTTP \u7f51\u7ad9\u54cd\u5e94\uff0c\u5177\u6709\u826f\u597d\u7684\u4f2a\u88c5\u80fd\u529b\uff0c\u76ee\u524d\u88ab\u5e7f\u6cdb\u7528\u4e8e\u53cd\u5ba1\u67e5\u3002 \u4f46\u662f\u5982\u6b64\u5f3a\u5927\u7684\u4f2a\u88c5\u80fd\u529b\uff0c\u9700\u8981\u4ed8\u51fa\u4e25\u91cd\u7684\u6027\u80fd\u4ee3\u4ef7\uff1aTLS 1.3 \u63e1\u624b\u9700\u8981\u6d88\u8017 1-rtt\uff0cWS \u63e1\u624b\u4e5f\u9700\u8981\u6d88\u8017 1-rtt\uff0c\u589e\u5927\u4e86\u63e1\u624b\u5ef6\u8fdf\u3002V2Ray \u589e\u52a0\u4e86 mux \u4ee5\u51cf\u5c11\u63e1\u624b\u7684\u53d1\u751f\uff0c\u7136\u800c\u5b9e\u9645\u4f7f\u7528\u4e2d mux \u4f53\u9a8c\u5e76\u4e0d\u597d\uff0c\u5f88\u591a\u7528\u6237\u9009\u62e9\u5173\u95ed\u3002 \u6700\u8fd1\u5174\u8d77\u4e86\u4e00\u4e2a\u65b0\u7684\u53cd\u5ba1\u67e5\u5de5\u5177\u2014\u2014 Trojan \uff0c\u8fd9\u4e2a\u5de5\u5177\u5c06\u4e00\u4e2a\u7c7b\u4f3c Socks \u7684\u534f\u8bae\u76f4\u63a5\u901a\u8fc7 TLS \u4f20\u8f93\uff0c\u5e76\u5c06\u8ba4\u8bc1\u5931\u8d25\u7684\u6d41\u91cf\u4ea4\u7531 Web \u670d\u52a1\u5668\u5904\u7406\u3002\u964d\u4f4e WS \u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e0e wss \u65b9\u5f0f\u4e00\u6837\u7684\u4f2a\u88c5\u80fd\u529b\u3002\u4f46\u662f\u8be5\u5de5\u5177\u8f83\u4e3a\u5e74\u8f7b\uff0c\u6ca1\u6709\u8def\u7531\u529f\u80fd\uff0c\u5404\u5e73\u53f0\u56fe\u5f62\u5316\u5ba2\u6237\u7aef\u4e5f\u4e0d\u5b8c\u5584\u3002 \u56e0\u6b64\uff0c\u672c\u4eba\u5c1d\u8bd5\u7528 V2Ray \u5b9e\u73b0\u7c7b\u4f3c\u529f\u80fd\uff0c\u5373 Vmess + TCP + TLS \u5e76\u7f51\u7ad9\u4f2a\u88c5\uff0c\u7701\u4e0b WS \u7684\u63e1\u624b\u5ef6\u8fdf\u3002 \u539f\u7406 HaProxy \u76d1\u542c 443 \u7aef\u53e3\uff0c\u5904\u7406 TLS \u4e4b\u540e\uff0c\u5c06 HTTP \u6d41\u91cf\u4ea4\u7531 Web \u670d\u52a1\u5668\u5904\u7406\uff0c\u975e HTTP \u6d41\u91cf\u4ea4\u7531 V2Ray \u6309 Vmess \u5904\u7406\u3002 \u7ea6\u5b9a \u8fd9\u91cc\u63cf\u8ff0\u4e00\u4e9b\u5b8f, \u8fd9\u4e9b\u53d8\u91cf\u9700\u8981\u5728\u914d\u7f6e\u65f6\u8fdb\u884c\u66ff\u6362: {uuid} : \u6807\u8bc6\u8eab\u4efd, \u5ba2\u6237\u7aef\u548c\u670d\u52a1\u7aef\u90fd\u9700\u8981\u914d\u7f6e\u540c\u4e00\u4e2auuid, match\u4e4b\u540e\u624d\u53ef\u901a\u884c\u3002\u53ef\u4ee5\u901a\u8fc7 cat /proc/sys/kernel/random/uuid \u547d\u4ee4\u751f\u6210\u3002 {VPSIP} : VPS\u7684\u516c\u7f51IP\u5730\u5740, \u4e5f\u5c31\u662f\u8d1f\u8d23\u4ee3\u7406\u4f60\u7f51\u7edc\u8bf7\u6c42\u7684\u673a\u5668\u7684ip\u5730\u5740, \u8fd9\u53f0\u673a\u5668\u80fd\u591f\u76f4\u63a5\u8bbf\u95ee\u5916\u7f51\u3002 {xxx.com} : \u81ea\u5df1\u7533\u8bf7\u7684\u57df\u540d, \u8fd9\u4e2a\u57df\u540d\u5df2\u7ecf\u901a\u8fc7DNS\u89e3\u6790\u5230 {VPSIP} , \u53ef\u4ee5\u901a\u8fc7 ping {xxx.com} \u6765\u8fdb\u884c\u9a8c\u8bc1\u3002 \u5b9e\u73b0 \u672c\u6b21\u65b9\u6848\u4f7f\u7528 HaProxy\uff0cCaddy/Nginx\uff08Web \u670d\u52a1\u5668\u7684\u4f7f\u7528\u4e0d\u662f\u672c\u6559\u7a0b\u7684\u91cd\u70b9\uff0c\u53ef\u4ee5\u7528 httpd \u7b49\u66ff\u4ee3\uff09\uff0cV2Ray\uff0c\u670d\u52a1\u5668\u7cfb\u7edf\u4e3a Debian 10\u3002 \u5b89\u88c5 HaProxy apt install haproxy \u4e3a\u4e86\u8f83\u597d\u7684\u652f\u6301 TLS1.3\uff0cHaProxy \u7248\u672c\u5e94\u5927\u4e8e 1.8.15\uff0cOpenSSl \u7248\u672c\u5e94\u5927\u4e8e 1.1.1\uff0c\u5982\u679c\u60a8\u4f7f\u7528\u7684\u53d1\u884c\u7248\u4ed3\u5e93\u81ea\u5e26\u7684\u7248\u672c\u8f83\u4f4e\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u81ea\u884c\u7f16\u8bd1\u5b89\u88c5\u3002 \u5b89\u88c5 Web \u670d\u52a1\u5668\uff0cCaddy \u53c2\u8003 \u8fd9\u4e2a\u6559\u7a0b \uff0cNginx \u4f7f\u7528\u547d\u4ee4 apt install nginx \u5b89\u88c5\u3002 \u5b89\u88c5 V2Ray\uff0c\u53ef\u4ee5\u4f7f\u7528\u5b98\u65b9\u811a\u672c \u5b98\u65b9\u811a\u672c \u4fee\u6539 V2Ray \u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5 Vmess + TCP \u65b9\u5f0f\u76d1\u542c 40001 \u7aef\u53e3\u3002 { \"inbounds\": [ { \"protocol\": \"vmess\", \"listen\": \"127.0.0.1\", \"port\": 40001, \"settings\": { \"clients\": [ { \"id\": \"f2435e5c-9ad9-4367-836a-8341117d0a5f\" } ] }, \"streamSettings\": { \"network\": \"tcp\" } } ], \"outbounds\": [ { \"protocol\": \"freedom\" } ] } \u4fee\u6539 Web \u670d\u52a1\u5668\u914d\u7f6e\u6587\u4ef6\uff0c\u90e8\u7f72 HTTP \u670d\u52a1\u4e8e 8080 \u7aef\u53e3\u3002 Caddy \u76f4\u63a5\u66ff\u6362 http://example.com:8080 { root /var/www/html } Nginx \u5728 http{} \u91cc\u9762\u6dfb\u52a0 server { listen 8080; server_name example.com; root /var/www/html; } \u6ce8\uff1a/var/www/html \u662f\u9759\u6001\u7f51\u7ad9\u76ee\u5f55 \u5b9e\u9645\u670d\u52a1\u8bf7\u6839\u636e\u9700\u8981\u90e8\u7f72\uff0c\u4e5f\u53ef\u4ee5\u7528 httpd \u4e4b\u7c7b\u7684\u66ff\u4ee3 \u4f3c\u4e4e\u5f88\u591a Trojan \u6559\u7a0b\u76f4\u63a5\u76d1\u542c 80 \u7aef\u53e3\uff0c\u5176\u5b9e\u5f88\u591a HTTPS \u7f51\u7ad9 80 \u7aef\u53e3\u901a\u5e38\u662f\u91cd\u5b9a\u5411\u5230 HTTPS \u4fee\u6539 HaProxy \u914d\u7f6e\u6587\u4ef6\u3002 global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private # \u4ec5\u4f7f\u7528\u652f\u6301 FS \u548c AEAD \u7684\u52a0\u5bc6\u5957\u4ef6 ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256 # \u7981\u7528 TLS 1.2 \u4e4b\u524d\u7684 TLS ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 tune.ssl.default-dh-param 2048 defaults log global # \u6211\u4eec\u9700\u8981\u4f7f\u7528 tcp \u6a21\u5f0f mode tcp option dontlognull timeout connect 5s # \u7a7a\u95f2\u8fde\u63a5\u7b49\u5f85\u65f6\u95f4\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0e V2Ray \u9ed8\u8ba4 connIdle \u4e00\u81f4\u7684 300s timeout client 300s timeout server 300s frontend tls-in # \u76d1\u542c 443 tls\uff0ctfo \u6839\u636e\u81ea\u8eab\u60c5\u51b5\u51b3\u5b9a\u662f\u5426\u5f00\u542f\uff0c\u8bc1\u4e66\u653e\u7f6e\u4e8e /etc/ssl/private/example.com.pem bind *:443 tfo ssl crt /etc/ssl/private/example.com.pem tcp-request inspect-delay 5s tcp-request content accept if HTTP # \u5c06 HTTP \u6d41\u91cf\u53d1\u7ed9 web \u540e\u7aef use_backend web if HTTP # \u5c06\u5176\u4ed6\u6d41\u91cf\u53d1\u7ed9 vmess \u540e\u7aef default_backend vmess backend web server server1 127.0.0.1:8080 backend vmess server server1 127.0.0.1:40001 HaProxy \u7684\u8bc1\u4e66\u548c\u5bc6\u94a5\u653e\u4e8e\u540c\u4e00\u4e2a\u6587\u4ef6\uff0c\u4e0e Caddy \u548c Nginx \u4e0d\u540c\uff0c\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4 cat example.com.crt example.com.key > example.com.pem \u5408\u6210\u8bc1\u4e66 \u91cd\u542f\u670d\u52a1 systemctl restart haproxy systemctl restart caddy systemctl restart v2ray \u5ba2\u6237\u7aef\u8fde\u63a5 example.com:443 vmess tls \u5373\u53ef { \"inbounds\" : [ { \"port\" : 1080 , \"listen\" : \"127.0.0.1\" , \"protocol\" : \"socks\" } ], \"outbounds\" : [ { \"protocol\" : \"vmess\" , \"settings\" : { \"vnext\" : [ { \"address\" : \"example.com\" , \"port\" : 443 , \"users\" : [ { \"id\" : \"f2435e5c-9ad9-4367-836a-8341117d0a5f\" , \"security\" : \"none\" } ] } ] }, \"streamSettings\" : { \"network\" : \"tcp\" , \"security\" : \"tls\" } } ] }","title":"\u5982\u4f55\u8de8\u8d8a\u957f\u57ce"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/#_1","text":"\u5728\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4ee5\u4e0b\u51e0\u4e2a\u4e1c\u897f: 1. VPS 2. \u81ea\u5df1\u7684\u57df\u540d 3. V2ray\u5ba2\u6237\u7aef 4. \u61c2\u5f97\u5982\u4f55\u914d\u7f6e \u68c0\u67e5\u5404\u4e2a\u6587\u4ef6\u7684\u60c5\u51b5: /etc/nginx/conf.d/default.conf \u6587\u4ef6\u4e0d\u5b58\u5728 /etc/nginx/conf/nginx.conf conf\u6587\u4ef6\u5939\u4e0d\u5b58\u5728 \u4fee\u6539\u8def\u5f84\u4e3a/etc/nginx/nginx.conf /etc/nginx/conf.d/default.conf default.conf\u4e0d\u5b58\u5728 /etc/nginx/ssl $conffile root /var/www/ws \u662f\u4ec0\u4e48\u6587\u4ef6, \u5176\u4f5c\u7528\u662f\u4ec0\u4e48 index.html \u662f\u540c\u76ee\u5f55\u4e0b\u7684\u4e00\u4e2a\u6587\u4ef6\u5417? \u6709\u4ec0\u4e48\u7528\u5462 server_name ws.pwe.cat; ws.pwe.cat\u8fd9\u4e2a\u57df\u540d\u4e2d\u7684ws\u6709\u4ec0\u4e48\u7279\u6b8a\u4f5c\u7528\u5417, \u6d4f\u89c8\u5668\u8bbf\u95ee\u5e76\u65e0\u4f5c\u7528 sudo ln -s /etc/nginx/sites-available/ws /etc/nginx/sites-enabled/ \u8fd9\u4e00\u6b65\u7684\u8f6f\u8fde\u63a5\u6709\u4ec0\u4e48\u4f5c\u7528\u5462 \u5982\u679c\u5f53\u524dvmess\u5df2\u7ecf\u65e0\u6cd5\u5de5\u4f5c, \u4f7f\u7528nginx \u53cd\u4ee3\u80fd\u5426work? (gfw\u5c01\u9488\u5bf9\u7684\u662f\u5ba2\u6237\u7aefip\u8fd8\u662fVPSip])","title":"\u5982\u4f55\u8de8\u8d8a\u957f\u57ce"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/#tcp-tls-web","text":"","title":"TCP + TLS + Web"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/#_2","text":"\u76ee\u524d Vmess + WebSocket + TLS \uff08\u4ee5\u4e0b\u7b80\u79f0 wss\uff09\u65b9\u5f0f\uff0c\u56e0\u5176\u7279\u5f81\u5982\u540c HTTPS \u6d41\u91cf\uff0c\u53ef\u4ee5\u9690\u85cf V2Ray \u8def\u5f84\uff0c\u4e3b\u52a8\u4fa6\u6d4b\u4f1a\u5f97\u5230\u6b63\u5e38 HTTP \u7f51\u7ad9\u54cd\u5e94\uff0c\u5177\u6709\u826f\u597d\u7684\u4f2a\u88c5\u80fd\u529b\uff0c\u76ee\u524d\u88ab\u5e7f\u6cdb\u7528\u4e8e\u53cd\u5ba1\u67e5\u3002 \u4f46\u662f\u5982\u6b64\u5f3a\u5927\u7684\u4f2a\u88c5\u80fd\u529b\uff0c\u9700\u8981\u4ed8\u51fa\u4e25\u91cd\u7684\u6027\u80fd\u4ee3\u4ef7\uff1aTLS 1.3 \u63e1\u624b\u9700\u8981\u6d88\u8017 1-rtt\uff0cWS \u63e1\u624b\u4e5f\u9700\u8981\u6d88\u8017 1-rtt\uff0c\u589e\u5927\u4e86\u63e1\u624b\u5ef6\u8fdf\u3002V2Ray \u589e\u52a0\u4e86 mux \u4ee5\u51cf\u5c11\u63e1\u624b\u7684\u53d1\u751f\uff0c\u7136\u800c\u5b9e\u9645\u4f7f\u7528\u4e2d mux \u4f53\u9a8c\u5e76\u4e0d\u597d\uff0c\u5f88\u591a\u7528\u6237\u9009\u62e9\u5173\u95ed\u3002 \u6700\u8fd1\u5174\u8d77\u4e86\u4e00\u4e2a\u65b0\u7684\u53cd\u5ba1\u67e5\u5de5\u5177\u2014\u2014 Trojan \uff0c\u8fd9\u4e2a\u5de5\u5177\u5c06\u4e00\u4e2a\u7c7b\u4f3c Socks \u7684\u534f\u8bae\u76f4\u63a5\u901a\u8fc7 TLS \u4f20\u8f93\uff0c\u5e76\u5c06\u8ba4\u8bc1\u5931\u8d25\u7684\u6d41\u91cf\u4ea4\u7531 Web \u670d\u52a1\u5668\u5904\u7406\u3002\u964d\u4f4e WS \u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e0e wss \u65b9\u5f0f\u4e00\u6837\u7684\u4f2a\u88c5\u80fd\u529b\u3002\u4f46\u662f\u8be5\u5de5\u5177\u8f83\u4e3a\u5e74\u8f7b\uff0c\u6ca1\u6709\u8def\u7531\u529f\u80fd\uff0c\u5404\u5e73\u53f0\u56fe\u5f62\u5316\u5ba2\u6237\u7aef\u4e5f\u4e0d\u5b8c\u5584\u3002 \u56e0\u6b64\uff0c\u672c\u4eba\u5c1d\u8bd5\u7528 V2Ray \u5b9e\u73b0\u7c7b\u4f3c\u529f\u80fd\uff0c\u5373 Vmess + TCP + TLS \u5e76\u7f51\u7ad9\u4f2a\u88c5\uff0c\u7701\u4e0b WS \u7684\u63e1\u624b\u5ef6\u8fdf\u3002","title":"\u80cc\u666f"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/#_3","text":"HaProxy \u76d1\u542c 443 \u7aef\u53e3\uff0c\u5904\u7406 TLS \u4e4b\u540e\uff0c\u5c06 HTTP \u6d41\u91cf\u4ea4\u7531 Web \u670d\u52a1\u5668\u5904\u7406\uff0c\u975e HTTP \u6d41\u91cf\u4ea4\u7531 V2Ray \u6309 Vmess \u5904\u7406\u3002","title":"\u539f\u7406"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/#_4","text":"\u8fd9\u91cc\u63cf\u8ff0\u4e00\u4e9b\u5b8f, \u8fd9\u4e9b\u53d8\u91cf\u9700\u8981\u5728\u914d\u7f6e\u65f6\u8fdb\u884c\u66ff\u6362: {uuid} : \u6807\u8bc6\u8eab\u4efd, \u5ba2\u6237\u7aef\u548c\u670d\u52a1\u7aef\u90fd\u9700\u8981\u914d\u7f6e\u540c\u4e00\u4e2auuid, match\u4e4b\u540e\u624d\u53ef\u901a\u884c\u3002\u53ef\u4ee5\u901a\u8fc7 cat /proc/sys/kernel/random/uuid \u547d\u4ee4\u751f\u6210\u3002 {VPSIP} : VPS\u7684\u516c\u7f51IP\u5730\u5740, \u4e5f\u5c31\u662f\u8d1f\u8d23\u4ee3\u7406\u4f60\u7f51\u7edc\u8bf7\u6c42\u7684\u673a\u5668\u7684ip\u5730\u5740, \u8fd9\u53f0\u673a\u5668\u80fd\u591f\u76f4\u63a5\u8bbf\u95ee\u5916\u7f51\u3002 {xxx.com} : \u81ea\u5df1\u7533\u8bf7\u7684\u57df\u540d, \u8fd9\u4e2a\u57df\u540d\u5df2\u7ecf\u901a\u8fc7DNS\u89e3\u6790\u5230 {VPSIP} , \u53ef\u4ee5\u901a\u8fc7 ping {xxx.com} \u6765\u8fdb\u884c\u9a8c\u8bc1\u3002","title":"\u7ea6\u5b9a"},{"location":"%E5%B7%A5%E5%85%B7/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/#_5","text":"\u672c\u6b21\u65b9\u6848\u4f7f\u7528 HaProxy\uff0cCaddy/Nginx\uff08Web \u670d\u52a1\u5668\u7684\u4f7f\u7528\u4e0d\u662f\u672c\u6559\u7a0b\u7684\u91cd\u70b9\uff0c\u53ef\u4ee5\u7528 httpd \u7b49\u66ff\u4ee3\uff09\uff0cV2Ray\uff0c\u670d\u52a1\u5668\u7cfb\u7edf\u4e3a Debian 10\u3002 \u5b89\u88c5 HaProxy apt install haproxy \u4e3a\u4e86\u8f83\u597d\u7684\u652f\u6301 TLS1.3\uff0cHaProxy \u7248\u672c\u5e94\u5927\u4e8e 1.8.15\uff0cOpenSSl \u7248\u672c\u5e94\u5927\u4e8e 1.1.1\uff0c\u5982\u679c\u60a8\u4f7f\u7528\u7684\u53d1\u884c\u7248\u4ed3\u5e93\u81ea\u5e26\u7684\u7248\u672c\u8f83\u4f4e\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u81ea\u884c\u7f16\u8bd1\u5b89\u88c5\u3002 \u5b89\u88c5 Web \u670d\u52a1\u5668\uff0cCaddy \u53c2\u8003 \u8fd9\u4e2a\u6559\u7a0b \uff0cNginx \u4f7f\u7528\u547d\u4ee4 apt install nginx \u5b89\u88c5\u3002 \u5b89\u88c5 V2Ray\uff0c\u53ef\u4ee5\u4f7f\u7528\u5b98\u65b9\u811a\u672c \u5b98\u65b9\u811a\u672c \u4fee\u6539 V2Ray \u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5 Vmess + TCP \u65b9\u5f0f\u76d1\u542c 40001 \u7aef\u53e3\u3002 { \"inbounds\": [ { \"protocol\": \"vmess\", \"listen\": \"127.0.0.1\", \"port\": 40001, \"settings\": { \"clients\": [ { \"id\": \"f2435e5c-9ad9-4367-836a-8341117d0a5f\" } ] }, \"streamSettings\": { \"network\": \"tcp\" } } ], \"outbounds\": [ { \"protocol\": \"freedom\" } ] } \u4fee\u6539 Web \u670d\u52a1\u5668\u914d\u7f6e\u6587\u4ef6\uff0c\u90e8\u7f72 HTTP \u670d\u52a1\u4e8e 8080 \u7aef\u53e3\u3002 Caddy \u76f4\u63a5\u66ff\u6362 http://example.com:8080 { root /var/www/html } Nginx \u5728 http{} \u91cc\u9762\u6dfb\u52a0 server { listen 8080; server_name example.com; root /var/www/html; } \u6ce8\uff1a/var/www/html \u662f\u9759\u6001\u7f51\u7ad9\u76ee\u5f55 \u5b9e\u9645\u670d\u52a1\u8bf7\u6839\u636e\u9700\u8981\u90e8\u7f72\uff0c\u4e5f\u53ef\u4ee5\u7528 httpd \u4e4b\u7c7b\u7684\u66ff\u4ee3 \u4f3c\u4e4e\u5f88\u591a Trojan \u6559\u7a0b\u76f4\u63a5\u76d1\u542c 80 \u7aef\u53e3\uff0c\u5176\u5b9e\u5f88\u591a HTTPS \u7f51\u7ad9 80 \u7aef\u53e3\u901a\u5e38\u662f\u91cd\u5b9a\u5411\u5230 HTTPS \u4fee\u6539 HaProxy \u914d\u7f6e\u6587\u4ef6\u3002 global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private # \u4ec5\u4f7f\u7528\u652f\u6301 FS \u548c AEAD \u7684\u52a0\u5bc6\u5957\u4ef6 ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256 # \u7981\u7528 TLS 1.2 \u4e4b\u524d\u7684 TLS ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 tune.ssl.default-dh-param 2048 defaults log global # \u6211\u4eec\u9700\u8981\u4f7f\u7528 tcp \u6a21\u5f0f mode tcp option dontlognull timeout connect 5s # \u7a7a\u95f2\u8fde\u63a5\u7b49\u5f85\u65f6\u95f4\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0e V2Ray \u9ed8\u8ba4 connIdle \u4e00\u81f4\u7684 300s timeout client 300s timeout server 300s frontend tls-in # \u76d1\u542c 443 tls\uff0ctfo \u6839\u636e\u81ea\u8eab\u60c5\u51b5\u51b3\u5b9a\u662f\u5426\u5f00\u542f\uff0c\u8bc1\u4e66\u653e\u7f6e\u4e8e /etc/ssl/private/example.com.pem bind *:443 tfo ssl crt /etc/ssl/private/example.com.pem tcp-request inspect-delay 5s tcp-request content accept if HTTP # \u5c06 HTTP \u6d41\u91cf\u53d1\u7ed9 web \u540e\u7aef use_backend web if HTTP # \u5c06\u5176\u4ed6\u6d41\u91cf\u53d1\u7ed9 vmess \u540e\u7aef default_backend vmess backend web server server1 127.0.0.1:8080 backend vmess server server1 127.0.0.1:40001 HaProxy \u7684\u8bc1\u4e66\u548c\u5bc6\u94a5\u653e\u4e8e\u540c\u4e00\u4e2a\u6587\u4ef6\uff0c\u4e0e Caddy \u548c Nginx \u4e0d\u540c\uff0c\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4 cat example.com.crt example.com.key > example.com.pem \u5408\u6210\u8bc1\u4e66 \u91cd\u542f\u670d\u52a1 systemctl restart haproxy systemctl restart caddy systemctl restart v2ray \u5ba2\u6237\u7aef\u8fde\u63a5 example.com:443 vmess tls \u5373\u53ef { \"inbounds\" : [ { \"port\" : 1080 , \"listen\" : \"127.0.0.1\" , \"protocol\" : \"socks\" } ], \"outbounds\" : [ { \"protocol\" : \"vmess\" , \"settings\" : { \"vnext\" : [ { \"address\" : \"example.com\" , \"port\" : 443 , \"users\" : [ { \"id\" : \"f2435e5c-9ad9-4367-836a-8341117d0a5f\" , \"security\" : \"none\" } ] } ] }, \"streamSettings\" : { \"network\" : \"tcp\" , \"security\" : \"tls\" } } ] }","title":"\u5b9e\u73b0"},{"location":"%E7%A7%91%E7%A0%94/%E4%BB%8B%E7%BB%8D/","text":"\u4e00\u4e2a\u4ecb\u7ecd \u53ef\u80fd\u4e3b\u8981\u6309\u7167\u9879\u76ee\u6765\u4ecb\u7ecd\u81ea\u5df1\u7684\u5de5\u4f5c\u5427, log\u5199\u7684\u8d8a\u591a\u5de5\u4f5c\u5c31\u505a\u7684\u8d8a\u591a, \u4ee5\u6b64\u6765\u76d1\u7763\u81ea\u5df1\u662f\u4e0d\u662f\u5728\u6478\u9c7c\u3002","title":"\u4e00\u4e2a\u4ecb\u7ecd"},{"location":"%E7%A7%91%E7%A0%94/%E4%BB%8B%E7%BB%8D/#_1","text":"\u53ef\u80fd\u4e3b\u8981\u6309\u7167\u9879\u76ee\u6765\u4ecb\u7ecd\u81ea\u5df1\u7684\u5de5\u4f5c\u5427, log\u5199\u7684\u8d8a\u591a\u5de5\u4f5c\u5c31\u505a\u7684\u8d8a\u591a, \u4ee5\u6b64\u6765\u76d1\u7763\u81ea\u5df1\u662f\u4e0d\u662f\u5728\u6478\u9c7c\u3002","title":"\u4e00\u4e2a\u4ecb\u7ecd"},{"location":"%E7%A7%91%E7%A0%94/%E5%A6%82%E4%BD%95/","text":"\u5982\u4f55 \u5982\u4f55\u5199\u4f5c \u524d\u8a00, \u5176\u5b9e\u6211\u4e5f\u4e0d\u7b97\u662f\u4f1a\u5199\u4f5c\u7684\u4eba, \u6240\u4ee5\u6211\u5c31\u9700\u8981\u5b66\u4e60\u5982\u4f55\u5199\u4f5c\u3002\u5728\u8fd9\u91cc\u6211\u5e0c\u671b\u8bb0\u5f55\u4e00\u4e0b\u81ea\u5df1\u5728\u5b66\u4e60\u5982\u4f55\u5199\u4f5c\u7684\u8fc7\u7a0b\u4e2d\u89c9\u5f97\u6bd4\u8f83\u91cd\u8981\u7684\u70b9\u3002 \u4ecb\u7ecd\u6e05\u695a\u81ea\u5df1\u7684\u5de5\u4f5c\u548c\u522b\u4eba\u7684\u5de5\u4f5c\u7684\u533a\u522b, \u540c\u65f6\u4e5f\u4e0d\u80fd\u9ed8\u8ba4\u522b\u4eba\u5df2\u7ecf\u5341\u5206\u6e05\u695a\u81ea\u5df1\u76f8\u5173\u7684\u5de5\u4f5c\u4e86, \u9002\u5f53\u8be6\u7ec6\u4ecb\u7ecd\u81ea\u5df1\u7684\u76f8\u5173\u5de5\u4f5c\u5bf9\u8bfb\u8005\u6bd4\u8f83\u53cb\u597d, \u540c\u65f6\u4e5f\u80fd\u7406\u6e05\u81ea\u5df1\u548c\u4ed6\u4eba\u5de5\u4f5c\u7684\u5173\u7cfb, \u52a0\u6df1\u81ea\u5df1\u7684\u8ba4\u8bc6\u3002 \u5982\u4f55\u627e\u8bba\u6587? \u53bb\u4f1a\u8bae\u6295\u7a3f\u91cc\u904d\u5386\u8bfb\u6458\u8981 google scochlar, \u88ab\u5f15\u7528 arxvi, \u641c\u5173\u952e\u8bcd\u7136\u540e\u904d\u5386 \u8bba\u6587\u91cc\u9762\u5f15\u7528\u54ea\u4e9b","title":"\u5982\u4f55"},{"location":"%E7%A7%91%E7%A0%94/%E5%A6%82%E4%BD%95/#_1","text":"","title":"\u5982\u4f55"},{"location":"%E7%A7%91%E7%A0%94/%E5%A6%82%E4%BD%95/#_2","text":"\u524d\u8a00, \u5176\u5b9e\u6211\u4e5f\u4e0d\u7b97\u662f\u4f1a\u5199\u4f5c\u7684\u4eba, \u6240\u4ee5\u6211\u5c31\u9700\u8981\u5b66\u4e60\u5982\u4f55\u5199\u4f5c\u3002\u5728\u8fd9\u91cc\u6211\u5e0c\u671b\u8bb0\u5f55\u4e00\u4e0b\u81ea\u5df1\u5728\u5b66\u4e60\u5982\u4f55\u5199\u4f5c\u7684\u8fc7\u7a0b\u4e2d\u89c9\u5f97\u6bd4\u8f83\u91cd\u8981\u7684\u70b9\u3002 \u4ecb\u7ecd\u6e05\u695a\u81ea\u5df1\u7684\u5de5\u4f5c\u548c\u522b\u4eba\u7684\u5de5\u4f5c\u7684\u533a\u522b, \u540c\u65f6\u4e5f\u4e0d\u80fd\u9ed8\u8ba4\u522b\u4eba\u5df2\u7ecf\u5341\u5206\u6e05\u695a\u81ea\u5df1\u76f8\u5173\u7684\u5de5\u4f5c\u4e86, \u9002\u5f53\u8be6\u7ec6\u4ecb\u7ecd\u81ea\u5df1\u7684\u76f8\u5173\u5de5\u4f5c\u5bf9\u8bfb\u8005\u6bd4\u8f83\u53cb\u597d, \u540c\u65f6\u4e5f\u80fd\u7406\u6e05\u81ea\u5df1\u548c\u4ed6\u4eba\u5de5\u4f5c\u7684\u5173\u7cfb, \u52a0\u6df1\u81ea\u5df1\u7684\u8ba4\u8bc6\u3002","title":"\u5982\u4f55\u5199\u4f5c"},{"location":"%E7%A7%91%E7%A0%94/%E5%A6%82%E4%BD%95/#_3","text":"\u53bb\u4f1a\u8bae\u6295\u7a3f\u91cc\u904d\u5386\u8bfb\u6458\u8981 google scochlar, \u88ab\u5f15\u7528 arxvi, \u641c\u5173\u952e\u8bcd\u7136\u540e\u904d\u5386 \u8bba\u6587\u91cc\u9762\u5f15\u7528\u54ea\u4e9b","title":"\u5982\u4f55\u627e\u8bba\u6587?"},{"location":"%E7%A7%91%E7%A0%94/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","text":"\u73af\u5883\u914d\u7f6e \u63a8\u8350\u4f7f\u7528linux \u9a71\u52a8, \u751f\u6001, \u5de5\u5177, \u90fd\u5f88\u597d, \u4e0d\u591a\u8bf4\u3002 \u63a8\u8350\u4f7f\u7528miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \u8fdb\u5165\u540e\u4e00\u76f4\u6309d, \u7136\u540eyes\u5c31\u53ef\u4ee5\u3002\u5f53\u7136\u4f60\u4e5f\u53ef\u4ee5\u6307\u5b9a\u5b89\u88c5\u8def\u5f84, \u4ee5\u53ca\u5176\u4ed6\u4e8b\u5b9c\u3002 \u5b89\u88c5\u6210\u529f\u540e, \u4f60\u9700\u8981\u6267\u884c\u4e00\u4e2a\u547d\u4ee4 conda init \u5b89\u88c5\u9a71\u52a8?","title":"\u73af\u5883\u914d\u7f6e"},{"location":"%E7%A7%91%E7%A0%94/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/#_1","text":"","title":"\u73af\u5883\u914d\u7f6e"},{"location":"%E7%A7%91%E7%A0%94/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/#linux","text":"\u9a71\u52a8, \u751f\u6001, \u5de5\u5177, \u90fd\u5f88\u597d, \u4e0d\u591a\u8bf4\u3002","title":"\u63a8\u8350\u4f7f\u7528linux"},{"location":"%E7%A7%91%E7%A0%94/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/#miniconda","text":"wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \u8fdb\u5165\u540e\u4e00\u76f4\u6309d, \u7136\u540eyes\u5c31\u53ef\u4ee5\u3002\u5f53\u7136\u4f60\u4e5f\u53ef\u4ee5\u6307\u5b9a\u5b89\u88c5\u8def\u5f84, \u4ee5\u53ca\u5176\u4ed6\u4e8b\u5b9c\u3002 \u5b89\u88c5\u6210\u529f\u540e, \u4f60\u9700\u8981\u6267\u884c\u4e00\u4e2a\u547d\u4ee4 conda init","title":"\u63a8\u8350\u4f7f\u7528miniconda"},{"location":"%E7%A7%91%E7%A0%94/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/#_2","text":"","title":"\u5b89\u88c5\u9a71\u52a8?"},{"location":"%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/","text":"\u8bba\u6587\u516c\u5f0f\u63a8\u5bfc $$ \\begin{aligned} &| model(P_{opt}) - input | 2 \\leq C_1 \\ &|model (P_{opt}) - input | 2 \\leq C_2 <C_2 \\ &\\delta P = P - P_{opt} \\ &model_{tune} (P_{opt} + \\delta P) - model_{tune}(P_{opt}) = edited - input &\\end{aligned} $$","title":"\u8bba\u6587\u516c\u5f0f\u63a8\u5bfc"},{"location":"%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/#_1","text":"$$ \\begin{aligned} &| model(P_{opt}) - input | 2 \\leq C_1 \\ &|model (P_{opt}) - input | 2 \\leq C_2 <C_2 \\ &\\delta P = P - P_{opt} \\ &model_{tune} (P_{opt} + \\delta P) - model_{tune}(P_{opt}) = edited - input &\\end{aligned} $$","title":"\u8bba\u6587\u516c\u5f0f\u63a8\u5bfc"},{"location":"%E7%A7%91%E7%A0%94/diffusion%E5%BA%93/","text":"diffusion\u5e93 \u524d\u8a00 \u6700\u8fd1diffusion model\u7684\u706b\u70ed\u8ba9\u6731\u8001\u5e08\u89c9\u5f97\u5e94\u8be5\u5728diffusion\u9886\u57df\u5b9e\u73b0\u81ea\u5df1\u7684\u5e93, \u7136\u540e\u6253\u7b97\u7ec4\u7ec7\u540c\u5b66\u4eec\u6765\u5f00\u53d1\u8fd9\u4e2a\u5e93, \u9996\u5148\u8fd9\u4e2a\u5e93\u7684\u5b9a\u4f4d\u548c\u76ee\u6807\u662f\u8ba8\u8bba\u7684\u91cd\u70b9, \u7b2c\u4e00\u6b21\u5f00\u4f1a\u4e3b\u8981\u8ba8\u8bba\u4e86\u76f8\u5173\u7684\u5185\u5bb9\u3002 \u7b2c\u4e00\u6b21\u5f00\u4f1a \u4e3b\u8981\u5206\u6790\u4e86\u5df2\u6709\u7684\u5e93\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\u3002 \u76ee\u524d\u4e3b\u8981\u7684diffusion\u5e93\u6709diffusers, \u8fd8\u6709k-diffusion, \u5176\u4e2dhugging face\u7684diffusers\u662f\u5f71\u54cd\u529b\u6bd4\u8f83\u5927\u7684, \u540c\u65f6\u8fd8\u6709\u5bf9\u5e94\u7684demo, pipline, \u4f7f\u5f97\u4f7f\u7528\u548c\u5b66\u4e60\u90fd\u6bd4\u8f83\u65b9\u4fbf, \u4ee3\u7801\u548chugging face\u7684\u670d\u52a1\u540c\u6b65\u3002 k-diffusion\u4e3b\u8981\u662f\u4e00\u4e2a\u4eba\u7ef4\u62a4\u7684, stable diffusion\u7684\u5b98\u65b9\u90fd\u4e3b\u8981\u7528\u7684\u662f\u8fd9\u4e2a\u5e93\u3002 \u8def\u6a59\u5b66\u957f\u4e3b\u8981\u62c5\u5fe7\u7684\u662f\u4eba\u624b\u4e0d\u591f, \u96be\u4ee5\u957f\u671f\u7ef4\u62a4\u4e00\u4e2a\u5f00\u6e90\u7684\u793e\u533a, \u6731\u8001\u5e08\u8868\u793a\u53ef\u4ee5\u8c03\u914d\u5de5\u7a0b\u5e08\u548c\u672c\u79d1\u751f\u3002\u5bf9\u4e8e\u5b9a\u4f4d\u548c\u793e\u533a\u7684\u95ee\u9898, \u6731\u8001\u5e08\u8868\u793a\u53ef\u4ee5\u53c2\u8003tianshou\u7684\u6210\u529f, tianshou\u5f53\u65f6\u4e3b\u6253\u7684\u5c31\u662f\u6a21\u5757\u5316\u548c\u53ef\u590d\u7528, \u8def\u6a59\u5b66\u957f\u8868\u793a\u518d\u52a0\u4e0a\u6587\u6863\u548c\u6d4b\u8bd5\u7684\u5de5\u4f5c\u624d\u4f7f\u5f97tianshou\u6bd4\u8f83\u6210\u529f\u3002 \u6700\u540e\u6211\u4eec\u5b9a\u4f4d\u662f, \u7ed9\u521d\u7ea7\u7684\u79d1\u5b66\u7814\u7a76\u8005\u505a\u4e00\u4e2a\u4f7f\u7528\u65b9\u4fbf, \u5bb9\u6613\u5b66, \u5e76\u4e14\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u4e5f\u5bb9\u6613\u590d\u7528\u7684\u5e93, \u540c\u65f6\u8fd8\u8981\u505a\u53ef\u590d\u73b0\u4ee5\u53caevaluation\u3002","title":"diffusion\u5e93"},{"location":"%E7%A7%91%E7%A0%94/diffusion%E5%BA%93/#diffusion","text":"","title":"diffusion\u5e93"},{"location":"%E7%A7%91%E7%A0%94/diffusion%E5%BA%93/#_1","text":"\u6700\u8fd1diffusion model\u7684\u706b\u70ed\u8ba9\u6731\u8001\u5e08\u89c9\u5f97\u5e94\u8be5\u5728diffusion\u9886\u57df\u5b9e\u73b0\u81ea\u5df1\u7684\u5e93, \u7136\u540e\u6253\u7b97\u7ec4\u7ec7\u540c\u5b66\u4eec\u6765\u5f00\u53d1\u8fd9\u4e2a\u5e93, \u9996\u5148\u8fd9\u4e2a\u5e93\u7684\u5b9a\u4f4d\u548c\u76ee\u6807\u662f\u8ba8\u8bba\u7684\u91cd\u70b9, \u7b2c\u4e00\u6b21\u5f00\u4f1a\u4e3b\u8981\u8ba8\u8bba\u4e86\u76f8\u5173\u7684\u5185\u5bb9\u3002","title":"\u524d\u8a00"},{"location":"%E7%A7%91%E7%A0%94/diffusion%E5%BA%93/#_2","text":"\u4e3b\u8981\u5206\u6790\u4e86\u5df2\u6709\u7684\u5e93\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\u3002 \u76ee\u524d\u4e3b\u8981\u7684diffusion\u5e93\u6709diffusers, \u8fd8\u6709k-diffusion, \u5176\u4e2dhugging face\u7684diffusers\u662f\u5f71\u54cd\u529b\u6bd4\u8f83\u5927\u7684, \u540c\u65f6\u8fd8\u6709\u5bf9\u5e94\u7684demo, pipline, \u4f7f\u5f97\u4f7f\u7528\u548c\u5b66\u4e60\u90fd\u6bd4\u8f83\u65b9\u4fbf, \u4ee3\u7801\u548chugging face\u7684\u670d\u52a1\u540c\u6b65\u3002 k-diffusion\u4e3b\u8981\u662f\u4e00\u4e2a\u4eba\u7ef4\u62a4\u7684, stable diffusion\u7684\u5b98\u65b9\u90fd\u4e3b\u8981\u7528\u7684\u662f\u8fd9\u4e2a\u5e93\u3002 \u8def\u6a59\u5b66\u957f\u4e3b\u8981\u62c5\u5fe7\u7684\u662f\u4eba\u624b\u4e0d\u591f, \u96be\u4ee5\u957f\u671f\u7ef4\u62a4\u4e00\u4e2a\u5f00\u6e90\u7684\u793e\u533a, \u6731\u8001\u5e08\u8868\u793a\u53ef\u4ee5\u8c03\u914d\u5de5\u7a0b\u5e08\u548c\u672c\u79d1\u751f\u3002\u5bf9\u4e8e\u5b9a\u4f4d\u548c\u793e\u533a\u7684\u95ee\u9898, \u6731\u8001\u5e08\u8868\u793a\u53ef\u4ee5\u53c2\u8003tianshou\u7684\u6210\u529f, tianshou\u5f53\u65f6\u4e3b\u6253\u7684\u5c31\u662f\u6a21\u5757\u5316\u548c\u53ef\u590d\u7528, \u8def\u6a59\u5b66\u957f\u8868\u793a\u518d\u52a0\u4e0a\u6587\u6863\u548c\u6d4b\u8bd5\u7684\u5de5\u4f5c\u624d\u4f7f\u5f97tianshou\u6bd4\u8f83\u6210\u529f\u3002 \u6700\u540e\u6211\u4eec\u5b9a\u4f4d\u662f, \u7ed9\u521d\u7ea7\u7684\u79d1\u5b66\u7814\u7a76\u8005\u505a\u4e00\u4e2a\u4f7f\u7528\u65b9\u4fbf, \u5bb9\u6613\u5b66, \u5e76\u4e14\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u4e5f\u5bb9\u6613\u590d\u7528\u7684\u5e93, \u540c\u65f6\u8fd8\u8981\u505a\u53ef\u590d\u73b0\u4ee5\u53caevaluation\u3002","title":"\u7b2c\u4e00\u6b21\u5f00\u4f1a"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/","text":"\u7b80\u4ecb \u6700\u8fd1\u56fe\u6587\u5927\u6a21\u578b\u4f8b\u5982dalle2, imagen\u90fd\u505a\u51fa\u4e86\u60ca\u4eba\u7684\u6548\u679c, \u6211\u4eec\u7ec4\u4e5f\u60f3\u505a\u4e00\u4e2a\u5927\u6a21\u578b, \u4ee5\u6b64\u4e3a\u57fa\u7840\u505a\u4e00\u4e9b\u5176\u4ed6\u7684\u5de5\u4f5c, \u5e76\u878d\u5165\u6700\u65b0\u7684\u6280\u672f, \u5728\u5b66\u672f\u548c\u5e94\u7528\u65b9\u9762\u5728\u56fe\u6587\u751f\u6210\u8fd9\u4e2a\u9886\u57df\u5f80\u524d\u63a8\u4e00\u63a8\u3002 \u7531\u4e8e\u5f88\u591a\u7684\u4ee3\u7801\u90fd\u4e0d\u5f00\u6e90, \u6211\u4eec\u7684\u9879\u76ee\u4e3b\u8981\u4f9d\u8d56\u5f00\u6e90\u7684\u9879\u76ee stable diffusion , \u5e76\u9884\u8ba1\u572812\u6708\u524d\u6709\u521d\u6b65\u7684\u590d\u73b0\u4ee5\u53ca\u6a21\u578b\u521b\u65b0\u7684\u7ed3\u679c\u3002 \u5176\u5b9e\u53c2\u4e0e\u4e00\u4e2a\u6bd4\u8f83\u5927\u578b\u7684\u5de5\u4f5c, \u627e\u51c6\u81ea\u5df1\u7684\u5b9a\u4f4d\u8fd8\u662f\u633a\u91cd\u8981\u7684, \u5728\u9879\u76ee\u7684\u521d\u671f, \u6211\u5728\u9879\u76ee\u4e2d\u7684\u5b9a\u4f4d\u8fd8\u4e0d\u591f\u660e\u786e, \u5bfc\u81f4\u81ea\u5df1\u6709\u4e9b\u7126\u8651, \u5230\u5341\u6708\u4efd, \u6211\u53d1\u73b0\u6211\u5bf9\u5982\u4f55\u8bc4\u4ef7\u548c\u589e\u5f3a\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u6709\u4e00\u5b9a\u7684\u5174\u8da3, \u540c\u65f6\u5bf9\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u7531\u4e00\u5b9a\u7684\u5174\u8da3, \u5982\u679c\u5b66\u6709\u4f59\u529b, \u540c\u65f6\u4e5f\u5e0c\u671b\u8fdb\u884c\u6a21\u578b\u7684\u4f18\u5316, \u5f53\u7136, \u5bf9\u6a21\u578b\u7684\u901f\u5ea6\u4f18\u5316\u5e94\u8be5\u662f\u6709\u7a7a\u95f4\u7684, \u4f46\u662f\u6bd4\u8f83\u56f0\u96be, \u6211\u89c9\u5f97\u5728\u672c\u79d1\u9636\u6bb5\u638c\u63e1\u4e00\u4e9b\u6bd4\u8f83\u5de5\u7a0b\u7684\u4e1c\u897f\u5bf9\u65e5\u540e\u5199\u4ee3\u7801\u662f\u6709\u5e2e\u52a9\u7684\u3002\u597d\u4e86\u522b\u9a82\u4e86, \u8d76\u7d27\u5e72\u6d3b\u53bb\u4e86\u3002","title":"\u7b80\u4ecb"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/#_1","text":"\u6700\u8fd1\u56fe\u6587\u5927\u6a21\u578b\u4f8b\u5982dalle2, imagen\u90fd\u505a\u51fa\u4e86\u60ca\u4eba\u7684\u6548\u679c, \u6211\u4eec\u7ec4\u4e5f\u60f3\u505a\u4e00\u4e2a\u5927\u6a21\u578b, \u4ee5\u6b64\u4e3a\u57fa\u7840\u505a\u4e00\u4e9b\u5176\u4ed6\u7684\u5de5\u4f5c, \u5e76\u878d\u5165\u6700\u65b0\u7684\u6280\u672f, \u5728\u5b66\u672f\u548c\u5e94\u7528\u65b9\u9762\u5728\u56fe\u6587\u751f\u6210\u8fd9\u4e2a\u9886\u57df\u5f80\u524d\u63a8\u4e00\u63a8\u3002 \u7531\u4e8e\u5f88\u591a\u7684\u4ee3\u7801\u90fd\u4e0d\u5f00\u6e90, \u6211\u4eec\u7684\u9879\u76ee\u4e3b\u8981\u4f9d\u8d56\u5f00\u6e90\u7684\u9879\u76ee stable diffusion , \u5e76\u9884\u8ba1\u572812\u6708\u524d\u6709\u521d\u6b65\u7684\u590d\u73b0\u4ee5\u53ca\u6a21\u578b\u521b\u65b0\u7684\u7ed3\u679c\u3002 \u5176\u5b9e\u53c2\u4e0e\u4e00\u4e2a\u6bd4\u8f83\u5927\u578b\u7684\u5de5\u4f5c, \u627e\u51c6\u81ea\u5df1\u7684\u5b9a\u4f4d\u8fd8\u662f\u633a\u91cd\u8981\u7684, \u5728\u9879\u76ee\u7684\u521d\u671f, \u6211\u5728\u9879\u76ee\u4e2d\u7684\u5b9a\u4f4d\u8fd8\u4e0d\u591f\u660e\u786e, \u5bfc\u81f4\u81ea\u5df1\u6709\u4e9b\u7126\u8651, \u5230\u5341\u6708\u4efd, \u6211\u53d1\u73b0\u6211\u5bf9\u5982\u4f55\u8bc4\u4ef7\u548c\u589e\u5f3a\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u6709\u4e00\u5b9a\u7684\u5174\u8da3, \u540c\u65f6\u5bf9\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u7531\u4e00\u5b9a\u7684\u5174\u8da3, \u5982\u679c\u5b66\u6709\u4f59\u529b, \u540c\u65f6\u4e5f\u5e0c\u671b\u8fdb\u884c\u6a21\u578b\u7684\u4f18\u5316, \u5f53\u7136, \u5bf9\u6a21\u578b\u7684\u901f\u5ea6\u4f18\u5316\u5e94\u8be5\u662f\u6709\u7a7a\u95f4\u7684, \u4f46\u662f\u6bd4\u8f83\u56f0\u96be, \u6211\u89c9\u5f97\u5728\u672c\u79d1\u9636\u6bb5\u638c\u63e1\u4e00\u4e9b\u6bd4\u8f83\u5de5\u7a0b\u7684\u4e1c\u897f\u5bf9\u65e5\u540e\u5199\u4ee3\u7801\u662f\u6709\u5e2e\u52a9\u7684\u3002\u597d\u4e86\u522b\u9a82\u4e86, \u8d76\u7d27\u5e72\u6d3b\u53bb\u4e86\u3002","title":"\u7b80\u4ecb"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/01working_log/","text":"\u5de5\u4f5c\u65e5\u5fd7 2022\u5e7409\u670821\u65e5 \u4e3b\u8981\u5c31\u662f\u505a\u4e86\u4e00\u4e9b textrual inverse \u7684\u590d\u73b0, \u5176\u5b9e\u5c31\u662f\u628a\u4ee3\u7801clone\u4e0b\u6765\u8dd1\u4e86\u4e00\u4e0b, \u8fd9\u4e2a\u6587\u7ae0\u4e3b\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f\u5728\u5f53\u524d\u7684diffusion model\u751f\u6210\u7684\u56fe\u7247\u591a\u6837\u6027\u7684\u662f\u8db3\u591f\u7684, \u4f46\u662f\u5982\u679c\u6211\u4eec\u60f3\u4e00\u76f4keep\u4e00\u4e2a\u6211\u4eec\u60f3\u8981\u7684\u7269\u4f53\u5728\u56fe\u7247\u4e2d, \u9700\u8981\u600e\u4e48\u505a\u5462\u3002\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u5728text embedding\u7a7a\u95f4\u4e2d\u627e\u5230\u4e00\u4e2a\u7279\u5b9a\u7684embedding \u6765\u4f5c\u4e3a\u8fd9\u4e2a\u7279\u5b9a\u7269\u4f53\u7684\u7b26\u53f7\u4ee3\u8868, \u5bfb\u627e\u7684\u65b9\u6cd5\u662f\u901a\u8fc7\u4f18\u5316\u7684\u65b9\u6cd5, \u53ef\u4ee5\u8ba4\u4e3a\u7528\u68af\u5ea6\u4f18\u5316embedding, \u6a21\u578b\u56fa\u5b9a, loss \u548c train diffusion \u8fc7\u7a0b\u4e00\u81f4\u3002 \u5f53\u524d\u5728\u9879\u76ee\u4e2d\u81ea\u5df1\u7684\u5b9a\u4f4d\u8fd8\u6ca1\u6709\u7279\u522b\u660e\u6670, \u81ea\u5df1\u5bf9\u8fd9\u4e2a\u9886\u57df\u7684idea\u4e5f\u6ca1\u6709\u4ec0\u4e48, \u52a8\u624b\u80fd\u529b\u6709\u5f85\u63d0\u9ad8, \u8fd8\u662f\u5e0c\u671b\u5728push\u81ea\u5df1\u7684\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u8fdb\u6b65\u3002 2022\u5e7409\u670828\u65e5 \u6700\u8fd1\u5728\u5f04zhusuan\u7684\u5de5\u4f5c, \u8fd9\u8fb9\u5c31\u6ca1\u600e\u4e48\u987e\u53ca, \u60ed\u6127\u60ed\u6127\u3002\u4e3b\u8981\u5c31\u662f\u95ee\u4e86\u4e00\u4e0b\u5b66\u957f\u4ee3\u7801\u7684\u603b\u4f53\u6846\u67b6\u7684\u95ee\u9898, \u7136\u540e\u5927\u6982\u7406\u89e3\u4e86\u4ee3\u7801\u6846\u67b6\u7684\u8bbe\u8ba1, \u7136\u540e\u4eca\u5929\u5f00\u4f1a\u8ba8\u8bba\u4e86\u56fe\u50cf\u63d0\u7279\u5f81\u7684\u95ee\u9898, \u4e3a\u4e86\u4fdd\u9669\u8d77\u89c1\u8001\u5e08\u51b3\u5b9a\u53ef\u4ee5\u76f4\u63a5\u901aimg train, \u53ef\u4ee5\u4e0d\u7528\u56fe\u50cf\u7684\u7279\u5f81\u3002\u5176\u5b9e\u56fe\u50cf\u672c\u8eab\u6709\u538b\u7f29, \u6240\u4ee5\u901a\u8fc7\u63d0\u7279\u5f81\u7684\u538b\u7f29\u65b9\u5f0f\u5e76\u4e0d\u662f\u5f88\u6709\u6548, \u4e0d\u8fc7\u6211\u770b\u4e3b\u8981\u662f\u770b\u63d0\u7279\u5f81\u7684\u65b9\u5f0f, VQ\u7cfb\u5217\u6211\u89c9\u5f97\u5927\u5c0f\u4f1a\u5c0f\u4e00\u4e9b, \u4f46\u662f\u603b\u611f\u89c9\u6548\u679c\u53ef\u80fd\u4f1a\u4e0d\u591f\u597d\u3002 \u6253\u7b97\u4e0b\u5468\u505a\u4e2a\u8bb2\u56fe\u6587\u751f\u6210\u7684B\u7ad9\u89c6\u9891, \u7136\u540e\u8bfb\u4e00\u4e0b\u4e4b\u524d\u7684DDPM\u7b49\u7684\u6587\u732e, \u518d\u662f\u505a\u5230\u51e0\u4e2a\u4ee3\u7801\u90fd\u80fd\u8dd1, \u8fd9\u91cc\u7684\u4ee3\u7801\u5305\u62ec: DDPM Improved DDPM Stablediffusion 2022\u5e7410\u670813\u65e5 \u8fc7\u4e86\u4e2a\u56fd\u5e86, \u7136\u540e\u5c31\u6ca1\u600e\u4e48\u505a\u79d1\u7814\u7684\u5de5\u4f5c, \u73b0\u5728\u5728\u9694\u79bb, \u7136\u540e\u6700\u8fd1\u6ce8\u610f\u5230\u4e4b\u524d\u5173\u6ce8\u7684\u4e00\u7bc7\u6587\u7ae0\u6709\u4eba\u7ed9\u590d\u73b0\u51fa\u6765\u4e86, \u6240\u4ee5\u6253\u7b97\u6df1\u5165\u7684\u770b\u4e00\u4e0b\u3002\u8fd9\u7bc7\u6587\u7ae0\u7684\u540d\u5b57\u53eb\u201cPrompt-to-Prompt Image Editing with Cross Attention Control\u201d\u3002 \u5177\u4f53\u7684\u4ee3\u7801\u64cd\u4f5c\u548c\u7814\u7a76\u8fc7\u7a0b\u53e6\u5916\u52a0\u4e00\u4e2asection\u5427\u3002 2022\u5e7410\u670830\u65e5 \u4e4b\u524d\u8ba4\u771f\u8dd1\u4e86\u4e00\u4e0bPrompt-to-Prompt Image Editing with Cross Attention Control\u7684\u5b9e\u9a8c, \u7136\u540e\u53d1\u73b0\u6548\u679c\u4e0d\u662f\u5f88\u597d, \u800c\u4e14\u540e\u6765\u53d1\u73b0\u7b97\u6cd5\u7684\u5b9e\u73b0\u6709\u95ee\u9898, \u7136\u540e\u6309\u7167\u6587\u7ae0\u7684\u5b9e\u73b0\u53c8\u6539\u4e86\u4e00\u6ce2, \u611f\u89c9\u6548\u679c\u53c8\u597d\u4e86\u4e00\u4e9b\u3002\u7136\u540e\u8bfb\u4e86dreambooth\u7684\u6587\u7ae0, \u611f\u89c9\u4e5f\u4e0d\u9519, \u7136\u540e\u4e5f\u53d1\u73b0\u6709\u5fae\u8c03\u7684\u57fa\u4e8establediffusion\u7684\u4ee3\u7801, \u611f\u89c9\u8fd9\u4e2a\u4e5f\u5f97\u8ba4\u771f\u8dd1\u4e00\u4e0b, \u7136\u540e\u57fa\u4e8e\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03, \u53ef\u4ee5\u5f04\u4e00\u7bc7\u6587\u7ae0\u3002","title":"\u5de5\u4f5c\u65e5\u5fd7"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/01working_log/#_1","text":"","title":"\u5de5\u4f5c\u65e5\u5fd7"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/01working_log/#20220921","text":"\u4e3b\u8981\u5c31\u662f\u505a\u4e86\u4e00\u4e9b textrual inverse \u7684\u590d\u73b0, \u5176\u5b9e\u5c31\u662f\u628a\u4ee3\u7801clone\u4e0b\u6765\u8dd1\u4e86\u4e00\u4e0b, \u8fd9\u4e2a\u6587\u7ae0\u4e3b\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f\u5728\u5f53\u524d\u7684diffusion model\u751f\u6210\u7684\u56fe\u7247\u591a\u6837\u6027\u7684\u662f\u8db3\u591f\u7684, \u4f46\u662f\u5982\u679c\u6211\u4eec\u60f3\u4e00\u76f4keep\u4e00\u4e2a\u6211\u4eec\u60f3\u8981\u7684\u7269\u4f53\u5728\u56fe\u7247\u4e2d, \u9700\u8981\u600e\u4e48\u505a\u5462\u3002\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u5728text embedding\u7a7a\u95f4\u4e2d\u627e\u5230\u4e00\u4e2a\u7279\u5b9a\u7684embedding \u6765\u4f5c\u4e3a\u8fd9\u4e2a\u7279\u5b9a\u7269\u4f53\u7684\u7b26\u53f7\u4ee3\u8868, \u5bfb\u627e\u7684\u65b9\u6cd5\u662f\u901a\u8fc7\u4f18\u5316\u7684\u65b9\u6cd5, \u53ef\u4ee5\u8ba4\u4e3a\u7528\u68af\u5ea6\u4f18\u5316embedding, \u6a21\u578b\u56fa\u5b9a, loss \u548c train diffusion \u8fc7\u7a0b\u4e00\u81f4\u3002 \u5f53\u524d\u5728\u9879\u76ee\u4e2d\u81ea\u5df1\u7684\u5b9a\u4f4d\u8fd8\u6ca1\u6709\u7279\u522b\u660e\u6670, \u81ea\u5df1\u5bf9\u8fd9\u4e2a\u9886\u57df\u7684idea\u4e5f\u6ca1\u6709\u4ec0\u4e48, \u52a8\u624b\u80fd\u529b\u6709\u5f85\u63d0\u9ad8, \u8fd8\u662f\u5e0c\u671b\u5728push\u81ea\u5df1\u7684\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u8fdb\u6b65\u3002","title":"2022\u5e7409\u670821\u65e5"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/01working_log/#20220928","text":"\u6700\u8fd1\u5728\u5f04zhusuan\u7684\u5de5\u4f5c, \u8fd9\u8fb9\u5c31\u6ca1\u600e\u4e48\u987e\u53ca, \u60ed\u6127\u60ed\u6127\u3002\u4e3b\u8981\u5c31\u662f\u95ee\u4e86\u4e00\u4e0b\u5b66\u957f\u4ee3\u7801\u7684\u603b\u4f53\u6846\u67b6\u7684\u95ee\u9898, \u7136\u540e\u5927\u6982\u7406\u89e3\u4e86\u4ee3\u7801\u6846\u67b6\u7684\u8bbe\u8ba1, \u7136\u540e\u4eca\u5929\u5f00\u4f1a\u8ba8\u8bba\u4e86\u56fe\u50cf\u63d0\u7279\u5f81\u7684\u95ee\u9898, \u4e3a\u4e86\u4fdd\u9669\u8d77\u89c1\u8001\u5e08\u51b3\u5b9a\u53ef\u4ee5\u76f4\u63a5\u901aimg train, \u53ef\u4ee5\u4e0d\u7528\u56fe\u50cf\u7684\u7279\u5f81\u3002\u5176\u5b9e\u56fe\u50cf\u672c\u8eab\u6709\u538b\u7f29, \u6240\u4ee5\u901a\u8fc7\u63d0\u7279\u5f81\u7684\u538b\u7f29\u65b9\u5f0f\u5e76\u4e0d\u662f\u5f88\u6709\u6548, \u4e0d\u8fc7\u6211\u770b\u4e3b\u8981\u662f\u770b\u63d0\u7279\u5f81\u7684\u65b9\u5f0f, VQ\u7cfb\u5217\u6211\u89c9\u5f97\u5927\u5c0f\u4f1a\u5c0f\u4e00\u4e9b, \u4f46\u662f\u603b\u611f\u89c9\u6548\u679c\u53ef\u80fd\u4f1a\u4e0d\u591f\u597d\u3002 \u6253\u7b97\u4e0b\u5468\u505a\u4e2a\u8bb2\u56fe\u6587\u751f\u6210\u7684B\u7ad9\u89c6\u9891, \u7136\u540e\u8bfb\u4e00\u4e0b\u4e4b\u524d\u7684DDPM\u7b49\u7684\u6587\u732e, \u518d\u662f\u505a\u5230\u51e0\u4e2a\u4ee3\u7801\u90fd\u80fd\u8dd1, \u8fd9\u91cc\u7684\u4ee3\u7801\u5305\u62ec: DDPM Improved DDPM Stablediffusion","title":"2022\u5e7409\u670828\u65e5"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/01working_log/#20221013","text":"\u8fc7\u4e86\u4e2a\u56fd\u5e86, \u7136\u540e\u5c31\u6ca1\u600e\u4e48\u505a\u79d1\u7814\u7684\u5de5\u4f5c, \u73b0\u5728\u5728\u9694\u79bb, \u7136\u540e\u6700\u8fd1\u6ce8\u610f\u5230\u4e4b\u524d\u5173\u6ce8\u7684\u4e00\u7bc7\u6587\u7ae0\u6709\u4eba\u7ed9\u590d\u73b0\u51fa\u6765\u4e86, \u6240\u4ee5\u6253\u7b97\u6df1\u5165\u7684\u770b\u4e00\u4e0b\u3002\u8fd9\u7bc7\u6587\u7ae0\u7684\u540d\u5b57\u53eb\u201cPrompt-to-Prompt Image Editing with Cross Attention Control\u201d\u3002 \u5177\u4f53\u7684\u4ee3\u7801\u64cd\u4f5c\u548c\u7814\u7a76\u8fc7\u7a0b\u53e6\u5916\u52a0\u4e00\u4e2asection\u5427\u3002","title":"2022\u5e7410\u670813\u65e5"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/01working_log/#20221030","text":"\u4e4b\u524d\u8ba4\u771f\u8dd1\u4e86\u4e00\u4e0bPrompt-to-Prompt Image Editing with Cross Attention Control\u7684\u5b9e\u9a8c, \u7136\u540e\u53d1\u73b0\u6548\u679c\u4e0d\u662f\u5f88\u597d, \u800c\u4e14\u540e\u6765\u53d1\u73b0\u7b97\u6cd5\u7684\u5b9e\u73b0\u6709\u95ee\u9898, \u7136\u540e\u6309\u7167\u6587\u7ae0\u7684\u5b9e\u73b0\u53c8\u6539\u4e86\u4e00\u6ce2, \u611f\u89c9\u6548\u679c\u53c8\u597d\u4e86\u4e00\u4e9b\u3002\u7136\u540e\u8bfb\u4e86dreambooth\u7684\u6587\u7ae0, \u611f\u89c9\u4e5f\u4e0d\u9519, \u7136\u540e\u4e5f\u53d1\u73b0\u6709\u5fae\u8c03\u7684\u57fa\u4e8establediffusion\u7684\u4ee3\u7801, \u611f\u89c9\u8fd9\u4e2a\u4e5f\u5f97\u8ba4\u771f\u8dd1\u4e00\u4e0b, \u7136\u540e\u57fa\u4e8e\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03, \u53ef\u4ee5\u5f04\u4e00\u7bc7\u6587\u7ae0\u3002","title":"2022\u5e7410\u670830\u65e5"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/diffedit/","text":"Diffedit \u524d\u8a00 \u8fd9\u6b21\u6211\u4eec\u8981\u590d\u73b0\u7684\u662fdiffedit\u8fd9\u7bc7\u6587\u7ae0, \u7531\u4e8e\u8fd9\u662funder review\u7684\u6587\u7ae0, \u6240\u4ee5\u6ca1\u6709\u6e90\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003, \u6211\u4eec\u8981\u81ea\u5df1\u590d\u73b0, \u8fd9\u662f\u81ea\u5df1\u7b2c\u4e00\u6b21\u4e25\u683c\u610f\u4e49\u4e0a\u7684\u590d\u73b0\u6587\u7ae0\u3002 \u6211\u89c9\u5f97\u5f53\u524d\u590d\u73b0\u8fd9\u4e2a\u6587\u7ae0\u6709\u4ee5\u4e0b\u51e0\u4e2a\u96be\u70b9: \u9700\u8981\u7528\u5230ddim\u7684\u52a0\u566a\u8fc7\u7a0b, \u8fd9\u4e00\u90e8\u5206\u7684\u7406\u8bba\u8fd8\u6ca1\u5f88\u719f, \u4ee3\u7801\u5b9e\u9a8c\u4e5f\u4ece\u672a\u63a5\u89e6 \u5982\u4f55\u4fdd\u5b58\u4e2d\u95f4\u53d8\u91cf\u5427, \u53ef\u80fd\u8981\u642c\u8fd0\u5230\u4e3b\u5b58\u4e0a, \u4f46\u91c7\u6837\u4f1a\u53d8\u6162","title":"Diffedit"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/diffedit/#diffedit","text":"","title":"Diffedit"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/diffedit/#_1","text":"\u8fd9\u6b21\u6211\u4eec\u8981\u590d\u73b0\u7684\u662fdiffedit\u8fd9\u7bc7\u6587\u7ae0, \u7531\u4e8e\u8fd9\u662funder review\u7684\u6587\u7ae0, \u6240\u4ee5\u6ca1\u6709\u6e90\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003, \u6211\u4eec\u8981\u81ea\u5df1\u590d\u73b0, \u8fd9\u662f\u81ea\u5df1\u7b2c\u4e00\u6b21\u4e25\u683c\u610f\u4e49\u4e0a\u7684\u590d\u73b0\u6587\u7ae0\u3002 \u6211\u89c9\u5f97\u5f53\u524d\u590d\u73b0\u8fd9\u4e2a\u6587\u7ae0\u6709\u4ee5\u4e0b\u51e0\u4e2a\u96be\u70b9: \u9700\u8981\u7528\u5230ddim\u7684\u52a0\u566a\u8fc7\u7a0b, \u8fd9\u4e00\u90e8\u5206\u7684\u7406\u8bba\u8fd8\u6ca1\u5f88\u719f, \u4ee3\u7801\u5b9e\u9a8c\u4e5f\u4ece\u672a\u63a5\u89e6 \u5982\u4f55\u4fdd\u5b58\u4e2d\u95f4\u53d8\u91cf\u5427, \u53ef\u80fd\u8981\u642c\u8fd0\u5230\u4e3b\u5b58\u4e0a, \u4f46\u91c7\u6837\u4f1a\u53d8\u6162","title":"\u524d\u8a00"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/","text":"DreamBooth [toc] \u524d\u8a00 \u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u805a\u7126\u5982\u4f55\u63a7\u5236\u4e00\u4e2a\u56fa\u5b9a\u7269\u4f53\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0, \u5e76\u4e14\u7ec6\u8282\u4e5f\u5f88\u5230\u4f4d, \u8fd9\u91cc\u7684\u590d\u73b0\u5de5\u4f5c\u57fa\u4e8egoogle\u7684\u6587\u7ae0\u201cDreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\u201d , \u4ee3\u7801\u5219\u57fa\u4e8ehttps://github.com/XavierXiao/Dreambooth-Stable-Diffusion, \u8fd9\u662f\u57fa\u4e8estable-diffusion\u7684\u5b9e\u73b0\u3002 \u6211\u4eec\u5728\u8fd9\u91cc\u671f\u671b\u505a\u5230\u4ee5\u4e0b\u51e0\u70b9: \u590d\u73b0\u4ee3\u7801\u548c\u6587\u7ae0, \u770b\u770b\u6548\u679c \u5bf9\u6bd4finetune\u524d\u540e\u7684\u6a21\u578b, \u54ea\u4e9b\u53c2\u6570\u6539\u53d8\u6bd4\u8f83\u5927? \u6b63\u5219\u5316\u5bf9\u5176\u4ed6\u7c7b\u578b\u7684\u751f\u6210\u6709\u591a\u5927\u5f71\u54cd? \u5c1d\u8bd5fix\u4f4f\u4e00\u90e8\u5206\u53c2\u6570\u7684\u6548\u679c \u601d\u8003\u5982\u4f55\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002 \u590d\u73b0\u548c\u719f\u6089\u8fc7\u7a0b \u8bdd\u4e0d\u591a\u8bf4, \u76f4\u63a5\u4e0a\u547d\u4ee4\u3002 git clone git@github.com:Xiang-cd/Dreambooth-Stable-Diffusion.git conda env create -f environment.yaml conda activate ldm \u63a5\u4e0b\u6765\u9700\u8981\u4e0b\u8f7d\u6a21\u578b, \u8fd9\u91cc\u9700\u8981\u5982\u4e0b\u547d\u4ee4, \u5e76\u4e14\u9700\u8981huggingface\u7684\u8d26\u6237, \u8fd0\u884c\u540e\u8f93\u5165\u5bc6\u7801\u5373\u53ef, \u968f\u540e\u4f1a\u5f00\u59cb\u4e0b\u8f7d, \u8bf7\u8bb0\u4f4f\u4f60\u4e0b\u8f7d\u7684\u8def\u5f84: wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt --user = Xiang-cd --ask-password https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt \u6839\u636e\u6587\u7ae0, \u4e3a\u4e86\u6b63\u5219\u5316\u6a21\u578b, \u9996\u5148\u4f1a\u8ba9\u6a21\u578b\u751f\u6210\u5f88\u591a\u7684\u540c\u7c7b\u56fe\u7247, \u9700\u8981\u6307\u5b9a\u79cd\u7c7b, \u4f8b\u5982\u4f60\u5e0c\u671b\u4f60\u5bb6\u7684\u72d7(\u4e00\u53ea\u7279\u5b9a\u7684\u72d7), \u51c6\u786e\u7684\u88ab\u751f\u6210, \u4f46\u540c\u65f6\u53c8\u5e0c\u671b\u540c\u65f6\u4fdd\u6301\u751f\u6210\u5176\u4ed6\u79cd\u7c7b\u7684\u72d7\u7684\u80fd\u529b, \u6240\u4ee5\u4f60\u9700\u8981\u51c6\u5907\u5f88\u591a\u5176\u4ed6\u72d7\u7684\u7167\u7247, \u5728finetune\u7684\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8bbe\u7f6e\u76f8\u5e94loss, \u4fdd\u6301\u6a21\u578b\u751f\u6210\u5176\u4ed6\u72d7\u7684\u80fd\u529b, \u6240\u4ee5, \u6211\u4eec\u8c03\u52a8\u751f\u6210\u5668, \u8fdb\u884c\u751f\u6210\u3002 \u7b2c\u4e00\u6b65\u6211\u4eec\u4f7f\u7528\u72d7\u4f5c\u4e3a\u6b63\u5219\u5316\u7684\u56fe\u7247\u3002 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 100 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ --prompt \"a photo of a dog\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/xiaoyang/reg_dog \u9b54\u65b9\u5176\u5b9e\u5728stable diffusion\u7684\u751f\u6210\u4e2d\u5176\u5b9e\u6ca1\u6709\u5f88\u597d\u7684\u6548\u679c, \u6211\u4eec\u60f3\u770b\u770b, \u5982\u679c\u5229\u7528finetune\u7684\u65b9\u6cd5, \u80fd\u4e0d\u80fd\u81f3\u5c11\u751f\u6210\u4e00\u4e2a\u50cf\u6837\u7684\u9b54\u65b9\u3002 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 100 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ --prompt \"a photo of a magic cube\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/cube/reg_cube \u7b2c\u4e8c\u6b65, \u540c\u65f6\u6211\u4eec\u53ef\u4ee5\u8fdb\u884cfinetune, \u56e0\u4e3a\u6587\u4ef6\u7cfb\u7edf\u78c1\u76d8\u7684\u539f\u56e0, \u628a\u8f93\u51fa\u7684\u6a21\u578b\u53c2\u6570\u7684\u4f4d\u7f6e\u6539\u4e86\u4e00\u4e0b, \u5177\u4f53\u7684\u811a\u672c\u542b\u4e49\u770b\u6e90\u4ee3\u7801\u4ed3\u5e93\u7684readme\u5c31\u53ef\u4ee5: python main.py \\ --base configs/stable-diffusion/v1-finetune_unfrozen.yaml \\ -t \\ --actual_resume ~/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ -n xiaoyang \\ --gpus 0 , \\ --data_root ~/Dreambooth-Stable-Diffusion/data/xiaoyang/example \\ --reg_data_root ~/Dreambooth-Stable-Diffusion/data/xiaoyang/reg_dog/samples \\ --class_word dog \\ --logdir ~/DreamBoothlog python main.py \\ --base configs/stable-diffusion/v1-finetune_unfrozen.yaml \\ -t \\ --actual_resume ~/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ -n cube \\ --gpus 0 , \\ --data_root ~/Dreambooth-Stable-Diffusion/data/cube/example \\ --reg_data_root ~/Dreambooth-Stable-Diffusion/data/cube/reg_cube/samples \\ --class_word \"magic cube\" \u7b2c\u4e09\u6b65, \u751f\u6210 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 \\ --n_samples 4 \\ --n_iter 1 \\ --scale 10 .0 \\ --ddim_steps 100 \\ --ckpt ~/Dreambooth-Stable-Diffusion/logs/example2022-10-31T16-56-41_xiaoyang/checkpoints/last.ckpt \\ --prompt \"photo of a sks dog\" python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 \\ --n_samples 4 \\ --n_iter 1 \\ --scale 10 .0 \\ --ddim_steps 100 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/cube.ckpt \\ --prompt \"photo of a sks magic cube\" \u7c97\u8bfb\u4ee3\u7801 \u540c\u7406, \u6211\u4eec\u6765\u770b\u770b\u4ee3\u7801\u7684\u5b9e\u73b0\u662f\u600e\u4e48\u6837\u6765\u8fdb\u884c\u8bad\u7ec3\u7684\u5427! # load \u4e00\u4e9b\u5305 import argparse , os , sys , datetime , glob , importlib , csv import numpy as np import time import torch import torchvision import pytorch_lightning as pl from packaging import version from omegaconf import OmegaConf from torch.utils.data import random_split , DataLoader , Dataset , Subset from functools import partial from PIL import Image from pytorch_lightning import seed_everything from pytorch_lightning.trainer import Trainer from pytorch_lightning.callbacks import ModelCheckpoint , Callback , LearningRateMonitor from pytorch_lightning.utilities.distributed import rank_zero_only from pytorch_lightning.utilities import rank_zero_info from ldm.data.base import Txt2ImgIterableBaseDataset from ldm.util import instantiate_from_config # \u4f9d\u636econfig\u6587\u4ef6, \u4ececkpt\u8def\u5f84\u4e2d\u52a0\u8f7d\u6a21\u578b def load_model_from_config ( config , ckpt , verbose = False ): print ( f \"Loading model from { ckpt } \" ) pl_sd = torch . load ( ckpt , map_location = \"cpu\" ) sd = pl_sd [ \"state_dict\" ] config . model . params . ckpt_path = ckpt model = instantiate_from_config ( config . model ) m , u = model . load_state_dict ( sd , strict = False ) if len ( m ) > 0 and verbose : print ( \"missing keys:\" ) print ( m ) if len ( u ) > 0 and verbose : print ( \"unexpected keys:\" ) print ( u ) model . cuda () return model def get_parser ( ** parser_kwargs ): def str2bool ( v ): if isinstance ( v , bool ): return v if v . lower () in ( \"yes\" , \"true\" , \"t\" , \"y\" , \"1\" ): return True elif v . lower () in ( \"no\" , \"false\" , \"f\" , \"n\" , \"0\" ): return False else : raise argparse . ArgumentTypeError ( \"Boolean value expected.\" ) parser = argparse . ArgumentParser ( ** parser_kwargs ) parser . add_argument ( \"-n\" , \"--name\" , type = str , const = True , default = \"\" , nargs = \"?\" , help = \"postfix for logdir\" , ) parser . add_argument ( \"-r\" , \"--resume\" , type = str , const = True , default = \"\" , nargs = \"?\" , help = \"resume from logdir or checkpoint in logdir\" , ) parser . add_argument ( \"-b\" , \"--base\" , nargs = \"*\" , metavar = \"base_config.yaml\" , help = \"paths to base configs. Loaded from left-to-right. \" \"Parameters can be overwritten or added with command-line options of the form `--key value`.\" , default = list (), ) parser . add_argument ( \"-t\" , \"--train\" , type = str2bool , const = True , default = False , nargs = \"?\" , help = \"train\" , ) parser . add_argument ( \"--no-test\" , type = str2bool , const = True , default = False , nargs = \"?\" , help = \"disable test\" , ) parser . add_argument ( \"-p\" , \"--project\" , help = \"name of new or path to existing project\" ) parser . add_argument ( \"-d\" , \"--debug\" , type = str2bool , nargs = \"?\" , const = True , default = False , help = \"enable post-mortem debugging\" , ) parser . add_argument ( \"-s\" , \"--seed\" , type = int , default = 23 , help = \"seed for seed_everything\" , ) parser . add_argument ( \"-f\" , \"--postfix\" , type = str , default = \"\" , help = \"post-postfix for default name\" , ) parser . add_argument ( \"-l\" , \"--logdir\" , type = str , default = \"logs\" , help = \"directory for logging dat shit\" , ) parser . add_argument ( \"--scale_lr\" , type = str2bool , nargs = \"?\" , const = False , default = False , help = \"scale base-lr by ngpu * batch_size * n_accumulate\" , ) parser . add_argument ( \"--datadir_in_name\" , type = str2bool , nargs = \"?\" , const = True , default = True , help = \"Prepend the final directory in the data_root to the output directory name\" ) parser . add_argument ( \"--actual_resume\" , type = str , required = True , help = \"Path to model to actually resume from\" ) parser . add_argument ( \"--data_root\" , type = str , required = True , help = \"Path to directory with training images\" ) parser . add_argument ( \"--reg_data_root\" , type = str , required = True , help = \"Path to directory with regularization images\" ) parser . add_argument ( \"--embedding_manager_ckpt\" , type = str , default = \"\" , help = \"Initialize embedding manager from a checkpoint\" ) parser . add_argument ( \"--class_word\" , type = str , default = \"dog\" , help = \"Placeholder token which will be used to denote the concept in future prompts\" ) parser . add_argument ( \"--init_word\" , type = str , help = \"Word to use as source for initial token embedding\" ) return parser def nondefault_trainer_args ( opt ): parser = argparse . ArgumentParser () parser = Trainer . add_argparse_args ( parser ) args = parser . parse_args ([]) return sorted ( k for k in vars ( args ) if getattr ( opt , k ) != getattr ( args , k )) class WrappedDataset ( Dataset ): \"\"\"Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset\"\"\" def __init__ ( self , dataset ): self . data = dataset def __len__ ( self ): return len ( self . data ) def __getitem__ ( self , idx ): return self . data [ idx ] def worker_init_fn ( _ ): worker_info = torch . utils . data . get_worker_info () dataset = worker_info . dataset worker_id = worker_info . id if isinstance ( dataset , Txt2ImgIterableBaseDataset ): split_size = dataset . num_records // worker_info . num_workers # reset num_records to the true number to retain reliable length information dataset . sample_ids = dataset . valid_ids [ worker_id * split_size :( worker_id + 1 ) * split_size ] current_id = np . random . choice ( len ( np . random . get_state ()[ 1 ]), 1 ) return np . random . seed ( np . random . get_state ()[ 1 ][ current_id ] + worker_id ) else : return np . random . seed ( np . random . get_state ()[ 1 ][ 0 ] + worker_id ) class ConcatDataset ( Dataset ): def __init__ ( self , * datasets ): self . datasets = datasets def __getitem__ ( self , idx ): return tuple ( d [ idx ] for d in self . datasets ) def __len__ ( self ): return min ( len ( d ) for d in self . datasets ) class DataModuleFromConfig ( pl . LightningDataModule ): def __init__ ( self , batch_size , train = None , reg = None , validation = None , test = None , predict = None , wrap = False , num_workers = None , shuffle_test_loader = False , use_worker_init_fn = False , shuffle_val_dataloader = False ): super () . __init__ () self . batch_size = batch_size self . dataset_configs = dict () self . num_workers = num_workers if num_workers is not None else batch_size * 2 self . use_worker_init_fn = use_worker_init_fn if train is not None : self . dataset_configs [ \"train\" ] = train if reg is not None : self . dataset_configs [ \"reg\" ] = reg self . train_dataloader = self . _train_dataloader if validation is not None : self . dataset_configs [ \"validation\" ] = validation self . val_dataloader = partial ( self . _val_dataloader , shuffle = shuffle_val_dataloader ) if test is not None : self . dataset_configs [ \"test\" ] = test self . test_dataloader = partial ( self . _test_dataloader , shuffle = shuffle_test_loader ) if predict is not None : self . dataset_configs [ \"predict\" ] = predict self . predict_dataloader = self . _predict_dataloader self . wrap = wrap def prepare_data ( self ): for data_cfg in self . dataset_configs . values (): instantiate_from_config ( data_cfg ) def setup ( self , stage = None ): self . datasets = dict ( ( k , instantiate_from_config ( self . dataset_configs [ k ])) for k in self . dataset_configs ) if self . wrap : for k in self . datasets : self . datasets [ k ] = WrappedDataset ( self . datasets [ k ]) def _train_dataloader ( self ): is_iterable_dataset = isinstance ( self . datasets [ 'train' ], Txt2ImgIterableBaseDataset ) if is_iterable_dataset or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None train_set = self . datasets [ \"train\" ] reg_set = self . datasets [ \"reg\" ] concat_dataset = ConcatDataset ( train_set , reg_set ) return DataLoader ( concat_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False if is_iterable_dataset else True , worker_init_fn = init_fn ) def _val_dataloader ( self , shuffle = False ): if isinstance ( self . datasets [ 'validation' ], Txt2ImgIterableBaseDataset ) or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None return DataLoader ( self . datasets [ \"validation\" ], batch_size = self . batch_size , num_workers = self . num_workers , worker_init_fn = init_fn , shuffle = shuffle ) def _test_dataloader ( self , shuffle = False ): is_iterable_dataset = isinstance ( self . datasets [ 'train' ], Txt2ImgIterableBaseDataset ) if is_iterable_dataset or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None # do not shuffle dataloader for iterable dataset shuffle = shuffle and ( not is_iterable_dataset ) return DataLoader ( self . datasets [ \"test\" ], batch_size = self . batch_size , num_workers = self . num_workers , worker_init_fn = init_fn , shuffle = shuffle ) def _predict_dataloader ( self , shuffle = False ): if isinstance ( self . datasets [ 'predict' ], Txt2ImgIterableBaseDataset ) or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None return DataLoader ( self . datasets [ \"predict\" ], batch_size = self . batch_size , num_workers = self . num_workers , worker_init_fn = init_fn ) class SetupCallback ( Callback ): def __init__ ( self , resume , now , logdir , ckptdir , cfgdir , config , lightning_config ): super () . __init__ () self . resume = resume self . now = now self . logdir = logdir self . ckptdir = ckptdir self . cfgdir = cfgdir self . config = config self . lightning_config = lightning_config def on_keyboard_interrupt ( self , trainer , pl_module ): if trainer . global_rank == 0 : print ( \"Summoning checkpoint.\" ) ckpt_path = os . path . join ( self . ckptdir , \"last.ckpt\" ) trainer . save_checkpoint ( ckpt_path ) def on_pretrain_routine_start ( self , trainer , pl_module ): if trainer . global_rank == 0 : # Create logdirs and save configs os . makedirs ( self . logdir , exist_ok = True ) os . makedirs ( self . ckptdir , exist_ok = True ) os . makedirs ( self . cfgdir , exist_ok = True ) if \"callbacks\" in self . lightning_config : if 'metrics_over_trainsteps_checkpoint' in self . lightning_config [ 'callbacks' ]: os . makedirs ( os . path . join ( self . ckptdir , 'trainstep_checkpoints' ), exist_ok = True ) print ( \"Project config\" ) print ( OmegaConf . to_yaml ( self . config )) OmegaConf . save ( self . config , os . path . join ( self . cfgdir , \" {} -project.yaml\" . format ( self . now ))) print ( \"Lightning config\" ) print ( OmegaConf . to_yaml ( self . lightning_config )) OmegaConf . save ( OmegaConf . create ({ \"lightning\" : self . lightning_config }), os . path . join ( self . cfgdir , \" {} -lightning.yaml\" . format ( self . now ))) else : # ModelCheckpoint callback created log directory --- remove it if not self . resume and os . path . exists ( self . logdir ): dst , name = os . path . split ( self . logdir ) dst = os . path . join ( dst , \"child_runs\" , name ) os . makedirs ( os . path . split ( dst )[ 0 ], exist_ok = True ) try : os . rename ( self . logdir , dst ) except FileNotFoundError : pass class ImageLogger ( Callback ): def __init__ ( self , batch_frequency , max_images , clamp = True , increase_log_steps = True , rescale = True , disabled = False , log_on_batch_idx = False , log_first_step = False , log_images_kwargs = None ): super () . __init__ () self . rescale = rescale self . batch_freq = batch_frequency self . max_images = max_images self . logger_log_images = { pl . loggers . TestTubeLogger : self . _testtube , } self . log_steps = [ 2 ** n for n in range ( int ( np . log2 ( self . batch_freq )) + 1 )] if not increase_log_steps : self . log_steps = [ self . batch_freq ] self . clamp = clamp self . disabled = disabled self . log_on_batch_idx = log_on_batch_idx self . log_images_kwargs = log_images_kwargs if log_images_kwargs else {} self . log_first_step = log_first_step @rank_zero_only def _testtube ( self , pl_module , images , batch_idx , split ): for k in images : grid = torchvision . utils . make_grid ( images [ k ]) grid = ( grid + 1.0 ) / 2.0 # -1,1 -> 0,1; c,h,w tag = f \" { split } / { k } \" pl_module . logger . experiment . add_image ( tag , grid , global_step = pl_module . global_step ) @rank_zero_only def log_local ( self , save_dir , split , images , global_step , current_epoch , batch_idx ): root = os . path . join ( save_dir , \"images\" , split ) for k in images : grid = torchvision . utils . make_grid ( images [ k ], nrow = 4 ) if self . rescale : grid = ( grid + 1.0 ) / 2.0 # -1,1 -> 0,1; c,h,w grid = grid . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) . squeeze ( - 1 ) grid = grid . numpy () grid = ( grid * 255 ) . astype ( np . uint8 ) filename = \" {} _gs- {:06} _e- {:06} _b- {:06} .jpg\" . format ( k , global_step , current_epoch , batch_idx ) path = os . path . join ( root , filename ) os . makedirs ( os . path . split ( path )[ 0 ], exist_ok = True ) Image . fromarray ( grid ) . save ( path ) def log_img ( self , pl_module , batch , batch_idx , split = \"train\" ): check_idx = batch_idx if self . log_on_batch_idx else pl_module . global_step if ( self . check_frequency ( check_idx ) and # batch_idx % self.batch_freq == 0 hasattr ( pl_module , \"log_images\" ) and callable ( pl_module . log_images ) and self . max_images > 0 ): logger = type ( pl_module . logger ) is_train = pl_module . training if is_train : pl_module . eval () with torch . no_grad (): images = pl_module . log_images ( batch , split = split , ** self . log_images_kwargs ) for k in images : N = min ( images [ k ] . shape [ 0 ], self . max_images ) images [ k ] = images [ k ][: N ] if isinstance ( images [ k ], torch . Tensor ): images [ k ] = images [ k ] . detach () . cpu () if self . clamp : images [ k ] = torch . clamp ( images [ k ], - 1. , 1. ) self . log_local ( pl_module . logger . save_dir , split , images , pl_module . global_step , pl_module . current_epoch , batch_idx ) logger_log_images = self . logger_log_images . get ( logger , lambda * args , ** kwargs : None ) logger_log_images ( pl_module , images , pl_module . global_step , split ) if is_train : pl_module . train () def check_frequency ( self , check_idx ): if (( check_idx % self . batch_freq ) == 0 or ( check_idx in self . log_steps )) and ( check_idx > 0 or self . log_first_step ): try : self . log_steps . pop ( 0 ) except IndexError as e : print ( e ) pass return True return False def on_train_batch_end ( self , trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ): if not self . disabled and ( pl_module . global_step > 0 or self . log_first_step ): self . log_img ( pl_module , batch , batch_idx , split = \"train\" ) def on_validation_batch_end ( self , trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ): if not self . disabled and pl_module . global_step > 0 : self . log_img ( pl_module , batch , batch_idx , split = \"val\" ) if hasattr ( pl_module , 'calibrate_grad_norm' ): if ( pl_module . calibrate_grad_norm and batch_idx % 25 == 0 ) and batch_idx > 0 : self . log_gradients ( trainer , pl_module , batch_idx = batch_idx ) class CUDACallback ( Callback ): # see https://github.com/SeanNaren/minGPT/blob/master/mingpt/callback.py def on_train_epoch_start ( self , trainer , pl_module ): # Reset the memory use counter torch . cuda . reset_peak_memory_stats ( trainer . root_gpu ) torch . cuda . synchronize ( trainer . root_gpu ) self . start_time = time . time () def on_train_epoch_end ( self , trainer , pl_module ): torch . cuda . synchronize ( trainer . root_gpu ) max_memory = torch . cuda . max_memory_allocated ( trainer . root_gpu ) / 2 ** 20 epoch_time = time . time () - self . start_time try : max_memory = trainer . training_type_plugin . reduce ( max_memory ) epoch_time = trainer . training_type_plugin . reduce ( epoch_time ) rank_zero_info ( f \"Average Epoch time: { epoch_time : .2f } seconds\" ) rank_zero_info ( f \"Average Peak memory { max_memory : .2f } MiB\" ) except AttributeError : pass class ModeSwapCallback ( Callback ): def __init__ ( self , swap_step = 2000 ): super () . __init__ () self . is_frozen = False self . swap_step = swap_step def on_train_epoch_start ( self , trainer , pl_module ): if trainer . global_step < self . swap_step and not self . is_frozen : self . is_frozen = True trainer . optimizers = [ pl_module . configure_opt_embedding ()] if trainer . global_step > self . swap_step and self . is_frozen : self . is_frozen = False trainer . optimizers = [ pl_module . configure_opt_model ()] if __name__ == \"__main__\" : # custom parser to specify config files, train, test and debug mode, # postfix, resume. # `--key value` arguments are interpreted as arguments to the trainer. # `nested.key=value` arguments are interpreted as config parameters. # configs are merged from left-to-right followed by command line parameters. # model: # base_learning_rate: float # target: path to lightning module # params: # key: value # data: # target: main.DataModuleFromConfig # params: # batch_size: int # wrap: bool # train: # target: path to train dataset # params: # key: value # validation: # target: path to validation dataset # params: # key: value # test: # target: path to test dataset # params: # key: value # lightning: (optional, has sane defaults and can be specified on cmdline) # trainer: # additional arguments to trainer # logger: # logger to instantiate # modelcheckpoint: # modelcheckpoint to instantiate # callbacks: # callback1: # target: importpath # params: # key: value now = datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H-%M-%S\" ) # add cwd for convenience and to make classes in this file available when # running as `python main.py` # (in particular `main.DataModuleFromConfig`) sys . path . append ( os . getcwd ()) parser = get_parser () parser = Trainer . add_argparse_args ( parser ) opt , unknown = parser . parse_known_args () if opt . name and opt . resume : raise ValueError ( \"-n/--name and -r/--resume cannot be specified both.\" \"If you want to resume training in a new log folder, \" \"use -n/--name in combination with --resume_from_checkpoint\" ) if opt . resume : if not os . path . exists ( opt . resume ): raise ValueError ( \"Cannot find {} \" . format ( opt . resume )) if os . path . isfile ( opt . resume ): paths = opt . resume . split ( \"/\" ) # idx = len(paths)-paths[::-1].index(\"logs\")+1 # logdir = \"/\".join(paths[:idx]) logdir = \"/\" . join ( paths [: - 2 ]) ckpt = opt . resume else : assert os . path . isdir ( opt . resume ), opt . resume logdir = opt . resume . rstrip ( \"/\" ) ckpt = os . path . join ( logdir , \"checkpoints\" , \"last.ckpt\" ) opt . resume_from_checkpoint = ckpt base_configs = sorted ( glob . glob ( os . path . join ( logdir , \"configs/*.yaml\" ))) opt . base = base_configs + opt . base _tmp = logdir . split ( \"/\" ) nowname = _tmp [ - 1 ] else : if opt . name : name = \"_\" + opt . name elif opt . base : cfg_fname = os . path . split ( opt . base [ 0 ])[ - 1 ] cfg_name = os . path . splitext ( cfg_fname )[ 0 ] name = \"_\" + cfg_name else : name = \"\" if opt . datadir_in_name : now = os . path . basename ( os . path . normpath ( opt . data_root )) + now nowname = now + name + opt . postfix logdir = os . path . join ( opt . logdir , nowname ) ckptdir = os . path . join ( logdir , \"checkpoints\" ) cfgdir = os . path . join ( logdir , \"configs\" ) seed_everything ( opt . seed ) try : # init and save configs configs = [ OmegaConf . load ( cfg ) for cfg in opt . base ] cli = OmegaConf . from_dotlist ( unknown ) config = OmegaConf . merge ( * configs , cli ) lightning_config = config . pop ( \"lightning\" , OmegaConf . create ()) # merge trainer cli with config trainer_config = lightning_config . get ( \"trainer\" , OmegaConf . create ()) # default to ddp trainer_config [ \"accelerator\" ] = \"ddp\" for k in nondefault_trainer_args ( opt ): trainer_config [ k ] = getattr ( opt , k ) if not \"gpus\" in trainer_config : del trainer_config [ \"accelerator\" ] cpu = True else : gpuinfo = trainer_config [ \"gpus\" ] print ( f \"Running on GPUs { gpuinfo } \" ) cpu = False trainer_opt = argparse . Namespace ( ** trainer_config ) lightning_config . trainer = trainer_config # model # config.model.params.personalization_config.params.init_word = opt.init_word # config.model.params.personalization_config.params.embedding_manager_ckpt = opt.embedding_manager_ckpt # config.model.params.personalization_config.params.placeholder_tokens = opt.placeholder_tokens # if opt.init_word: # config.model.params.personalization_config.params.initializer_words[0] = opt.init_word config . data . params . train . params . placeholder_token = opt . class_word config . data . params . reg . params . placeholder_token = opt . class_word config . data . params . validation . params . placeholder_token = opt . class_word if opt . actual_resume : model = load_model_from_config ( config , opt . actual_resume ) else : model = instantiate_from_config ( config . model ) # trainer and callbacks trainer_kwargs = dict () # default logger configs default_logger_cfgs = { \"wandb\" : { \"target\" : \"pytorch_lightning.loggers.WandbLogger\" , \"params\" : { \"name\" : nowname , \"save_dir\" : logdir , \"offline\" : opt . debug , \"id\" : nowname , } }, \"testtube\" : { \"target\" : \"pytorch_lightning.loggers.TestTubeLogger\" , \"params\" : { \"name\" : \"testtube\" , \"save_dir\" : logdir , } }, } default_logger_cfg = default_logger_cfgs [ \"testtube\" ] if \"logger\" in lightning_config : logger_cfg = lightning_config . logger else : logger_cfg = OmegaConf . create () logger_cfg = OmegaConf . merge ( default_logger_cfg , logger_cfg ) trainer_kwargs [ \"logger\" ] = instantiate_from_config ( logger_cfg ) # modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to # specify which metric is used to determine best models default_modelckpt_cfg = { \"target\" : \"pytorch_lightning.callbacks.ModelCheckpoint\" , \"params\" : { \"dirpath\" : ckptdir , \"filename\" : \" {epoch:06} \" , \"verbose\" : True , \"save_last\" : True , } } if hasattr ( model , \"monitor\" ): print ( f \"Monitoring { model . monitor } as checkpoint metric.\" ) default_modelckpt_cfg [ \"params\" ][ \"monitor\" ] = model . monitor default_modelckpt_cfg [ \"params\" ][ \"save_top_k\" ] = 1 if \"modelcheckpoint\" in lightning_config : modelckpt_cfg = lightning_config . modelcheckpoint else : modelckpt_cfg = OmegaConf . create () modelckpt_cfg = OmegaConf . merge ( default_modelckpt_cfg , modelckpt_cfg ) print ( f \"Merged modelckpt-cfg: \\n { modelckpt_cfg } \" ) if version . parse ( pl . __version__ ) < version . parse ( '1.4.0' ): trainer_kwargs [ \"checkpoint_callback\" ] = instantiate_from_config ( modelckpt_cfg ) # add callback which sets up log directory default_callbacks_cfg = { \"setup_callback\" : { \"target\" : \"main.SetupCallback\" , \"params\" : { \"resume\" : opt . resume , \"now\" : now , \"logdir\" : logdir , \"ckptdir\" : ckptdir , \"cfgdir\" : cfgdir , \"config\" : config , \"lightning_config\" : lightning_config , } }, \"image_logger\" : { \"target\" : \"main.ImageLogger\" , \"params\" : { \"batch_frequency\" : 750 , \"max_images\" : 4 , \"clamp\" : True } }, \"learning_rate_logger\" : { \"target\" : \"main.LearningRateMonitor\" , \"params\" : { \"logging_interval\" : \"step\" , # \"log_momentum\": True } }, \"cuda_callback\" : { \"target\" : \"main.CUDACallback\" }, } if version . parse ( pl . __version__ ) >= version . parse ( '1.4.0' ): default_callbacks_cfg . update ({ 'checkpoint_callback' : modelckpt_cfg }) if \"callbacks\" in lightning_config : callbacks_cfg = lightning_config . callbacks else : callbacks_cfg = OmegaConf . create () if 'metrics_over_trainsteps_checkpoint' in callbacks_cfg : print ( 'Caution: Saving checkpoints every n train steps without deleting. This might require some free space.' ) default_metrics_over_trainsteps_ckpt_dict = { 'metrics_over_trainsteps_checkpoint' : { \"target\" : 'pytorch_lightning.callbacks.ModelCheckpoint' , 'params' : { \"dirpath\" : os . path . join ( ckptdir , 'trainstep_checkpoints' ), \"filename\" : \" {epoch:06} - {step:09} \" , \"verbose\" : True , 'save_top_k' : - 1 , 'every_n_train_steps' : 10000 , 'save_weights_only' : True } } } default_callbacks_cfg . update ( default_metrics_over_trainsteps_ckpt_dict ) callbacks_cfg = OmegaConf . merge ( default_callbacks_cfg , callbacks_cfg ) if 'ignore_keys_callback' in callbacks_cfg and hasattr ( trainer_opt , 'resume_from_checkpoint' ): callbacks_cfg . ignore_keys_callback . params [ 'ckpt_path' ] = trainer_opt . resume_from_checkpoint elif 'ignore_keys_callback' in callbacks_cfg : del callbacks_cfg [ 'ignore_keys_callback' ] trainer_kwargs [ \"callbacks\" ] = [ instantiate_from_config ( callbacks_cfg [ k ]) for k in callbacks_cfg ] trainer_kwargs [ \"max_steps\" ] = trainer_opt . max_steps trainer = Trainer . from_argparse_args ( trainer_opt , ** trainer_kwargs ) trainer . logdir = logdir ### # data config . data . params . train . params . data_root = opt . data_root config . data . params . reg . params . data_root = opt . reg_data_root config . data . params . validation . params . data_root = opt . data_root data = instantiate_from_config ( config . data ) data = instantiate_from_config ( config . data ) # NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html # calling these ourselves should not be necessary but it is. # lightning still takes care of proper multiprocessing though data . prepare_data () data . setup () print ( \"#### Data #####\" ) for k in data . datasets : print ( f \" { k } , { data . datasets [ k ] . __class__ . __name__ } , { len ( data . datasets [ k ]) } \" ) # configure learning rate bs , base_lr = config . data . params . batch_size , config . model . base_learning_rate if not cpu : ngpu = len ( lightning_config . trainer . gpus . strip ( \",\" ) . split ( ',' )) else : ngpu = 1 if 'accumulate_grad_batches' in lightning_config . trainer : accumulate_grad_batches = lightning_config . trainer . accumulate_grad_batches else : accumulate_grad_batches = 1 print ( f \"accumulate_grad_batches = { accumulate_grad_batches } \" ) lightning_config . trainer . accumulate_grad_batches = accumulate_grad_batches if opt . scale_lr : model . learning_rate = accumulate_grad_batches * ngpu * bs * base_lr print ( \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\" . format ( model . learning_rate , accumulate_grad_batches , ngpu , bs , base_lr )) else : model . learning_rate = base_lr print ( \"++++ NOT USING LR SCALING ++++\" ) print ( f \"Setting learning rate to { model . learning_rate : .2e } \" ) # allow checkpointing via USR1 def melk ( * args , ** kwargs ): # run all checkpoint hooks if trainer . global_rank == 0 : print ( \"Summoning checkpoint.\" ) ckpt_path = os . path . join ( ckptdir , \"last.ckpt\" ) trainer . save_checkpoint ( ckpt_path ) def divein ( * args , ** kwargs ): if trainer . global_rank == 0 : import pudb ; pudb . set_trace () import signal signal . signal ( signal . SIGUSR1 , melk ) signal . signal ( signal . SIGUSR2 , divein ) # run if opt . train : try : trainer . fit ( model , data ) except Exception : melk () raise if not opt . no_test and not trainer . interrupted : trainer . test ( model , data ) except Exception : if opt . debug and trainer . global_rank == 0 : try : import pudb as debugger except ImportError : import pdb as debugger debugger . post_mortem () raise finally : # move newly created debug project to debug_runs if opt . debug and not opt . resume and trainer . global_rank == 0 : dst , name = os . path . split ( logdir ) dst = os . path . join ( dst , \"debug_runs\" , name ) os . makedirs ( os . path . split ( dst )[ 0 ], exist_ok = True ) os . rename ( logdir , dst ) if trainer . global_rank == 0 : print ( trainer . profiler . summary ()) \u590d\u73b0\u7ed3\u679c \u5c0f\u7f8a \u5c0f\u7f8a\u662f\u4e00\u53ea\u53ef\u7231\u7684\u8428\u6469\u8036\u72ac, \u6211\u4eec\u5e0c\u671b\u5229\u7528\u6a21\u578b\u548cdreambooth\u7684\u65b9\u6cd5\u770b\u770b\u80fd\u5426\u590d\u73b0\u5c0f\u7f8a\u7684\u6837\u5b50\u3002 \u539f\u56fe \u751f\u6210 finetune\u524d\u540edog\u5bf9\u6bd4 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 100 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/xiaoyang.ckpt \\ --prompt \"a photo of a dog\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/xiaoyang/after_tune_dog # default seed 42 \u9009\u62e90, 5, 9, 10 \u8fd9\u56db\u5f20\u56fe\u7247 \u524d \u540e \u751f\u6210\u5176\u4ed6\u5185\u5bb9 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 2 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/xiaoyang.ckpt \\ --prompt \"a photo of a cat sitting on a green car\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/xiaoyang/others after finetune \u201ca photo of a cat sitting on a green car\u201d, seed = 42 before finetune \u201ca photo of a cat sitting on a green car\u201d, seed = 42 magic cube \u539f\u56fe \u751f\u6210 \u7ed3\u8bba \u8fdb\u884cfinetune\u80fd\u591f\u4f7f\u5f97\u6a21\u578b\u6355\u6349\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u7269\u4f53\u7684\u7279\u5f81 \u8fdb\u884cfinetune\u5bf9\u4e8e\u539f\u6765\u7684\u751f\u6210\u6548\u679c\u6709\u4e00\u5b9a\u7684\u6298\u6263\u3002 finetune\u6bd4\u8f83\u5feb, 800\u6b65\u8fed\u4ee3 \u6bcf\u4e2a\u7269\u4f53\u90fd\u9700\u898112G\u7684ckpt","title":"DreamBooth"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#dreambooth","text":"[toc]","title":"DreamBooth"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_1","text":"\u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u805a\u7126\u5982\u4f55\u63a7\u5236\u4e00\u4e2a\u56fa\u5b9a\u7269\u4f53\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0, \u5e76\u4e14\u7ec6\u8282\u4e5f\u5f88\u5230\u4f4d, \u8fd9\u91cc\u7684\u590d\u73b0\u5de5\u4f5c\u57fa\u4e8egoogle\u7684\u6587\u7ae0\u201cDreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\u201d , \u4ee3\u7801\u5219\u57fa\u4e8ehttps://github.com/XavierXiao/Dreambooth-Stable-Diffusion, \u8fd9\u662f\u57fa\u4e8estable-diffusion\u7684\u5b9e\u73b0\u3002 \u6211\u4eec\u5728\u8fd9\u91cc\u671f\u671b\u505a\u5230\u4ee5\u4e0b\u51e0\u70b9: \u590d\u73b0\u4ee3\u7801\u548c\u6587\u7ae0, \u770b\u770b\u6548\u679c \u5bf9\u6bd4finetune\u524d\u540e\u7684\u6a21\u578b, \u54ea\u4e9b\u53c2\u6570\u6539\u53d8\u6bd4\u8f83\u5927? \u6b63\u5219\u5316\u5bf9\u5176\u4ed6\u7c7b\u578b\u7684\u751f\u6210\u6709\u591a\u5927\u5f71\u54cd? \u5c1d\u8bd5fix\u4f4f\u4e00\u90e8\u5206\u53c2\u6570\u7684\u6548\u679c \u601d\u8003\u5982\u4f55\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002","title":"\u524d\u8a00"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_2","text":"\u8bdd\u4e0d\u591a\u8bf4, \u76f4\u63a5\u4e0a\u547d\u4ee4\u3002 git clone git@github.com:Xiang-cd/Dreambooth-Stable-Diffusion.git conda env create -f environment.yaml conda activate ldm \u63a5\u4e0b\u6765\u9700\u8981\u4e0b\u8f7d\u6a21\u578b, \u8fd9\u91cc\u9700\u8981\u5982\u4e0b\u547d\u4ee4, \u5e76\u4e14\u9700\u8981huggingface\u7684\u8d26\u6237, \u8fd0\u884c\u540e\u8f93\u5165\u5bc6\u7801\u5373\u53ef, \u968f\u540e\u4f1a\u5f00\u59cb\u4e0b\u8f7d, \u8bf7\u8bb0\u4f4f\u4f60\u4e0b\u8f7d\u7684\u8def\u5f84: wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt --user = Xiang-cd --ask-password https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt \u6839\u636e\u6587\u7ae0, \u4e3a\u4e86\u6b63\u5219\u5316\u6a21\u578b, \u9996\u5148\u4f1a\u8ba9\u6a21\u578b\u751f\u6210\u5f88\u591a\u7684\u540c\u7c7b\u56fe\u7247, \u9700\u8981\u6307\u5b9a\u79cd\u7c7b, \u4f8b\u5982\u4f60\u5e0c\u671b\u4f60\u5bb6\u7684\u72d7(\u4e00\u53ea\u7279\u5b9a\u7684\u72d7), \u51c6\u786e\u7684\u88ab\u751f\u6210, \u4f46\u540c\u65f6\u53c8\u5e0c\u671b\u540c\u65f6\u4fdd\u6301\u751f\u6210\u5176\u4ed6\u79cd\u7c7b\u7684\u72d7\u7684\u80fd\u529b, \u6240\u4ee5\u4f60\u9700\u8981\u51c6\u5907\u5f88\u591a\u5176\u4ed6\u72d7\u7684\u7167\u7247, \u5728finetune\u7684\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8bbe\u7f6e\u76f8\u5e94loss, \u4fdd\u6301\u6a21\u578b\u751f\u6210\u5176\u4ed6\u72d7\u7684\u80fd\u529b, \u6240\u4ee5, \u6211\u4eec\u8c03\u52a8\u751f\u6210\u5668, \u8fdb\u884c\u751f\u6210\u3002 \u7b2c\u4e00\u6b65\u6211\u4eec\u4f7f\u7528\u72d7\u4f5c\u4e3a\u6b63\u5219\u5316\u7684\u56fe\u7247\u3002 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 100 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ --prompt \"a photo of a dog\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/xiaoyang/reg_dog \u9b54\u65b9\u5176\u5b9e\u5728stable diffusion\u7684\u751f\u6210\u4e2d\u5176\u5b9e\u6ca1\u6709\u5f88\u597d\u7684\u6548\u679c, \u6211\u4eec\u60f3\u770b\u770b, \u5982\u679c\u5229\u7528finetune\u7684\u65b9\u6cd5, \u80fd\u4e0d\u80fd\u81f3\u5c11\u751f\u6210\u4e00\u4e2a\u50cf\u6837\u7684\u9b54\u65b9\u3002 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 100 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ --prompt \"a photo of a magic cube\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/cube/reg_cube \u7b2c\u4e8c\u6b65, \u540c\u65f6\u6211\u4eec\u53ef\u4ee5\u8fdb\u884cfinetune, \u56e0\u4e3a\u6587\u4ef6\u7cfb\u7edf\u78c1\u76d8\u7684\u539f\u56e0, \u628a\u8f93\u51fa\u7684\u6a21\u578b\u53c2\u6570\u7684\u4f4d\u7f6e\u6539\u4e86\u4e00\u4e0b, \u5177\u4f53\u7684\u811a\u672c\u542b\u4e49\u770b\u6e90\u4ee3\u7801\u4ed3\u5e93\u7684readme\u5c31\u53ef\u4ee5: python main.py \\ --base configs/stable-diffusion/v1-finetune_unfrozen.yaml \\ -t \\ --actual_resume ~/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ -n xiaoyang \\ --gpus 0 , \\ --data_root ~/Dreambooth-Stable-Diffusion/data/xiaoyang/example \\ --reg_data_root ~/Dreambooth-Stable-Diffusion/data/xiaoyang/reg_dog/samples \\ --class_word dog \\ --logdir ~/DreamBoothlog python main.py \\ --base configs/stable-diffusion/v1-finetune_unfrozen.yaml \\ -t \\ --actual_resume ~/stable-diffusion-ckpt/sd-v1-4-full-ema.ckpt \\ -n cube \\ --gpus 0 , \\ --data_root ~/Dreambooth-Stable-Diffusion/data/cube/example \\ --reg_data_root ~/Dreambooth-Stable-Diffusion/data/cube/reg_cube/samples \\ --class_word \"magic cube\" \u7b2c\u4e09\u6b65, \u751f\u6210 python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 \\ --n_samples 4 \\ --n_iter 1 \\ --scale 10 .0 \\ --ddim_steps 100 \\ --ckpt ~/Dreambooth-Stable-Diffusion/logs/example2022-10-31T16-56-41_xiaoyang/checkpoints/last.ckpt \\ --prompt \"photo of a sks dog\" python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 \\ --n_samples 4 \\ --n_iter 1 \\ --scale 10 .0 \\ --ddim_steps 100 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/cube.ckpt \\ --prompt \"photo of a sks magic cube\"","title":"\u590d\u73b0\u548c\u719f\u6089\u8fc7\u7a0b"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_3","text":"\u540c\u7406, \u6211\u4eec\u6765\u770b\u770b\u4ee3\u7801\u7684\u5b9e\u73b0\u662f\u600e\u4e48\u6837\u6765\u8fdb\u884c\u8bad\u7ec3\u7684\u5427! # load \u4e00\u4e9b\u5305 import argparse , os , sys , datetime , glob , importlib , csv import numpy as np import time import torch import torchvision import pytorch_lightning as pl from packaging import version from omegaconf import OmegaConf from torch.utils.data import random_split , DataLoader , Dataset , Subset from functools import partial from PIL import Image from pytorch_lightning import seed_everything from pytorch_lightning.trainer import Trainer from pytorch_lightning.callbacks import ModelCheckpoint , Callback , LearningRateMonitor from pytorch_lightning.utilities.distributed import rank_zero_only from pytorch_lightning.utilities import rank_zero_info from ldm.data.base import Txt2ImgIterableBaseDataset from ldm.util import instantiate_from_config # \u4f9d\u636econfig\u6587\u4ef6, \u4ececkpt\u8def\u5f84\u4e2d\u52a0\u8f7d\u6a21\u578b def load_model_from_config ( config , ckpt , verbose = False ): print ( f \"Loading model from { ckpt } \" ) pl_sd = torch . load ( ckpt , map_location = \"cpu\" ) sd = pl_sd [ \"state_dict\" ] config . model . params . ckpt_path = ckpt model = instantiate_from_config ( config . model ) m , u = model . load_state_dict ( sd , strict = False ) if len ( m ) > 0 and verbose : print ( \"missing keys:\" ) print ( m ) if len ( u ) > 0 and verbose : print ( \"unexpected keys:\" ) print ( u ) model . cuda () return model def get_parser ( ** parser_kwargs ): def str2bool ( v ): if isinstance ( v , bool ): return v if v . lower () in ( \"yes\" , \"true\" , \"t\" , \"y\" , \"1\" ): return True elif v . lower () in ( \"no\" , \"false\" , \"f\" , \"n\" , \"0\" ): return False else : raise argparse . ArgumentTypeError ( \"Boolean value expected.\" ) parser = argparse . ArgumentParser ( ** parser_kwargs ) parser . add_argument ( \"-n\" , \"--name\" , type = str , const = True , default = \"\" , nargs = \"?\" , help = \"postfix for logdir\" , ) parser . add_argument ( \"-r\" , \"--resume\" , type = str , const = True , default = \"\" , nargs = \"?\" , help = \"resume from logdir or checkpoint in logdir\" , ) parser . add_argument ( \"-b\" , \"--base\" , nargs = \"*\" , metavar = \"base_config.yaml\" , help = \"paths to base configs. Loaded from left-to-right. \" \"Parameters can be overwritten or added with command-line options of the form `--key value`.\" , default = list (), ) parser . add_argument ( \"-t\" , \"--train\" , type = str2bool , const = True , default = False , nargs = \"?\" , help = \"train\" , ) parser . add_argument ( \"--no-test\" , type = str2bool , const = True , default = False , nargs = \"?\" , help = \"disable test\" , ) parser . add_argument ( \"-p\" , \"--project\" , help = \"name of new or path to existing project\" ) parser . add_argument ( \"-d\" , \"--debug\" , type = str2bool , nargs = \"?\" , const = True , default = False , help = \"enable post-mortem debugging\" , ) parser . add_argument ( \"-s\" , \"--seed\" , type = int , default = 23 , help = \"seed for seed_everything\" , ) parser . add_argument ( \"-f\" , \"--postfix\" , type = str , default = \"\" , help = \"post-postfix for default name\" , ) parser . add_argument ( \"-l\" , \"--logdir\" , type = str , default = \"logs\" , help = \"directory for logging dat shit\" , ) parser . add_argument ( \"--scale_lr\" , type = str2bool , nargs = \"?\" , const = False , default = False , help = \"scale base-lr by ngpu * batch_size * n_accumulate\" , ) parser . add_argument ( \"--datadir_in_name\" , type = str2bool , nargs = \"?\" , const = True , default = True , help = \"Prepend the final directory in the data_root to the output directory name\" ) parser . add_argument ( \"--actual_resume\" , type = str , required = True , help = \"Path to model to actually resume from\" ) parser . add_argument ( \"--data_root\" , type = str , required = True , help = \"Path to directory with training images\" ) parser . add_argument ( \"--reg_data_root\" , type = str , required = True , help = \"Path to directory with regularization images\" ) parser . add_argument ( \"--embedding_manager_ckpt\" , type = str , default = \"\" , help = \"Initialize embedding manager from a checkpoint\" ) parser . add_argument ( \"--class_word\" , type = str , default = \"dog\" , help = \"Placeholder token which will be used to denote the concept in future prompts\" ) parser . add_argument ( \"--init_word\" , type = str , help = \"Word to use as source for initial token embedding\" ) return parser def nondefault_trainer_args ( opt ): parser = argparse . ArgumentParser () parser = Trainer . add_argparse_args ( parser ) args = parser . parse_args ([]) return sorted ( k for k in vars ( args ) if getattr ( opt , k ) != getattr ( args , k )) class WrappedDataset ( Dataset ): \"\"\"Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset\"\"\" def __init__ ( self , dataset ): self . data = dataset def __len__ ( self ): return len ( self . data ) def __getitem__ ( self , idx ): return self . data [ idx ] def worker_init_fn ( _ ): worker_info = torch . utils . data . get_worker_info () dataset = worker_info . dataset worker_id = worker_info . id if isinstance ( dataset , Txt2ImgIterableBaseDataset ): split_size = dataset . num_records // worker_info . num_workers # reset num_records to the true number to retain reliable length information dataset . sample_ids = dataset . valid_ids [ worker_id * split_size :( worker_id + 1 ) * split_size ] current_id = np . random . choice ( len ( np . random . get_state ()[ 1 ]), 1 ) return np . random . seed ( np . random . get_state ()[ 1 ][ current_id ] + worker_id ) else : return np . random . seed ( np . random . get_state ()[ 1 ][ 0 ] + worker_id ) class ConcatDataset ( Dataset ): def __init__ ( self , * datasets ): self . datasets = datasets def __getitem__ ( self , idx ): return tuple ( d [ idx ] for d in self . datasets ) def __len__ ( self ): return min ( len ( d ) for d in self . datasets ) class DataModuleFromConfig ( pl . LightningDataModule ): def __init__ ( self , batch_size , train = None , reg = None , validation = None , test = None , predict = None , wrap = False , num_workers = None , shuffle_test_loader = False , use_worker_init_fn = False , shuffle_val_dataloader = False ): super () . __init__ () self . batch_size = batch_size self . dataset_configs = dict () self . num_workers = num_workers if num_workers is not None else batch_size * 2 self . use_worker_init_fn = use_worker_init_fn if train is not None : self . dataset_configs [ \"train\" ] = train if reg is not None : self . dataset_configs [ \"reg\" ] = reg self . train_dataloader = self . _train_dataloader if validation is not None : self . dataset_configs [ \"validation\" ] = validation self . val_dataloader = partial ( self . _val_dataloader , shuffle = shuffle_val_dataloader ) if test is not None : self . dataset_configs [ \"test\" ] = test self . test_dataloader = partial ( self . _test_dataloader , shuffle = shuffle_test_loader ) if predict is not None : self . dataset_configs [ \"predict\" ] = predict self . predict_dataloader = self . _predict_dataloader self . wrap = wrap def prepare_data ( self ): for data_cfg in self . dataset_configs . values (): instantiate_from_config ( data_cfg ) def setup ( self , stage = None ): self . datasets = dict ( ( k , instantiate_from_config ( self . dataset_configs [ k ])) for k in self . dataset_configs ) if self . wrap : for k in self . datasets : self . datasets [ k ] = WrappedDataset ( self . datasets [ k ]) def _train_dataloader ( self ): is_iterable_dataset = isinstance ( self . datasets [ 'train' ], Txt2ImgIterableBaseDataset ) if is_iterable_dataset or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None train_set = self . datasets [ \"train\" ] reg_set = self . datasets [ \"reg\" ] concat_dataset = ConcatDataset ( train_set , reg_set ) return DataLoader ( concat_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False if is_iterable_dataset else True , worker_init_fn = init_fn ) def _val_dataloader ( self , shuffle = False ): if isinstance ( self . datasets [ 'validation' ], Txt2ImgIterableBaseDataset ) or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None return DataLoader ( self . datasets [ \"validation\" ], batch_size = self . batch_size , num_workers = self . num_workers , worker_init_fn = init_fn , shuffle = shuffle ) def _test_dataloader ( self , shuffle = False ): is_iterable_dataset = isinstance ( self . datasets [ 'train' ], Txt2ImgIterableBaseDataset ) if is_iterable_dataset or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None # do not shuffle dataloader for iterable dataset shuffle = shuffle and ( not is_iterable_dataset ) return DataLoader ( self . datasets [ \"test\" ], batch_size = self . batch_size , num_workers = self . num_workers , worker_init_fn = init_fn , shuffle = shuffle ) def _predict_dataloader ( self , shuffle = False ): if isinstance ( self . datasets [ 'predict' ], Txt2ImgIterableBaseDataset ) or self . use_worker_init_fn : init_fn = worker_init_fn else : init_fn = None return DataLoader ( self . datasets [ \"predict\" ], batch_size = self . batch_size , num_workers = self . num_workers , worker_init_fn = init_fn ) class SetupCallback ( Callback ): def __init__ ( self , resume , now , logdir , ckptdir , cfgdir , config , lightning_config ): super () . __init__ () self . resume = resume self . now = now self . logdir = logdir self . ckptdir = ckptdir self . cfgdir = cfgdir self . config = config self . lightning_config = lightning_config def on_keyboard_interrupt ( self , trainer , pl_module ): if trainer . global_rank == 0 : print ( \"Summoning checkpoint.\" ) ckpt_path = os . path . join ( self . ckptdir , \"last.ckpt\" ) trainer . save_checkpoint ( ckpt_path ) def on_pretrain_routine_start ( self , trainer , pl_module ): if trainer . global_rank == 0 : # Create logdirs and save configs os . makedirs ( self . logdir , exist_ok = True ) os . makedirs ( self . ckptdir , exist_ok = True ) os . makedirs ( self . cfgdir , exist_ok = True ) if \"callbacks\" in self . lightning_config : if 'metrics_over_trainsteps_checkpoint' in self . lightning_config [ 'callbacks' ]: os . makedirs ( os . path . join ( self . ckptdir , 'trainstep_checkpoints' ), exist_ok = True ) print ( \"Project config\" ) print ( OmegaConf . to_yaml ( self . config )) OmegaConf . save ( self . config , os . path . join ( self . cfgdir , \" {} -project.yaml\" . format ( self . now ))) print ( \"Lightning config\" ) print ( OmegaConf . to_yaml ( self . lightning_config )) OmegaConf . save ( OmegaConf . create ({ \"lightning\" : self . lightning_config }), os . path . join ( self . cfgdir , \" {} -lightning.yaml\" . format ( self . now ))) else : # ModelCheckpoint callback created log directory --- remove it if not self . resume and os . path . exists ( self . logdir ): dst , name = os . path . split ( self . logdir ) dst = os . path . join ( dst , \"child_runs\" , name ) os . makedirs ( os . path . split ( dst )[ 0 ], exist_ok = True ) try : os . rename ( self . logdir , dst ) except FileNotFoundError : pass class ImageLogger ( Callback ): def __init__ ( self , batch_frequency , max_images , clamp = True , increase_log_steps = True , rescale = True , disabled = False , log_on_batch_idx = False , log_first_step = False , log_images_kwargs = None ): super () . __init__ () self . rescale = rescale self . batch_freq = batch_frequency self . max_images = max_images self . logger_log_images = { pl . loggers . TestTubeLogger : self . _testtube , } self . log_steps = [ 2 ** n for n in range ( int ( np . log2 ( self . batch_freq )) + 1 )] if not increase_log_steps : self . log_steps = [ self . batch_freq ] self . clamp = clamp self . disabled = disabled self . log_on_batch_idx = log_on_batch_idx self . log_images_kwargs = log_images_kwargs if log_images_kwargs else {} self . log_first_step = log_first_step @rank_zero_only def _testtube ( self , pl_module , images , batch_idx , split ): for k in images : grid = torchvision . utils . make_grid ( images [ k ]) grid = ( grid + 1.0 ) / 2.0 # -1,1 -> 0,1; c,h,w tag = f \" { split } / { k } \" pl_module . logger . experiment . add_image ( tag , grid , global_step = pl_module . global_step ) @rank_zero_only def log_local ( self , save_dir , split , images , global_step , current_epoch , batch_idx ): root = os . path . join ( save_dir , \"images\" , split ) for k in images : grid = torchvision . utils . make_grid ( images [ k ], nrow = 4 ) if self . rescale : grid = ( grid + 1.0 ) / 2.0 # -1,1 -> 0,1; c,h,w grid = grid . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) . squeeze ( - 1 ) grid = grid . numpy () grid = ( grid * 255 ) . astype ( np . uint8 ) filename = \" {} _gs- {:06} _e- {:06} _b- {:06} .jpg\" . format ( k , global_step , current_epoch , batch_idx ) path = os . path . join ( root , filename ) os . makedirs ( os . path . split ( path )[ 0 ], exist_ok = True ) Image . fromarray ( grid ) . save ( path ) def log_img ( self , pl_module , batch , batch_idx , split = \"train\" ): check_idx = batch_idx if self . log_on_batch_idx else pl_module . global_step if ( self . check_frequency ( check_idx ) and # batch_idx % self.batch_freq == 0 hasattr ( pl_module , \"log_images\" ) and callable ( pl_module . log_images ) and self . max_images > 0 ): logger = type ( pl_module . logger ) is_train = pl_module . training if is_train : pl_module . eval () with torch . no_grad (): images = pl_module . log_images ( batch , split = split , ** self . log_images_kwargs ) for k in images : N = min ( images [ k ] . shape [ 0 ], self . max_images ) images [ k ] = images [ k ][: N ] if isinstance ( images [ k ], torch . Tensor ): images [ k ] = images [ k ] . detach () . cpu () if self . clamp : images [ k ] = torch . clamp ( images [ k ], - 1. , 1. ) self . log_local ( pl_module . logger . save_dir , split , images , pl_module . global_step , pl_module . current_epoch , batch_idx ) logger_log_images = self . logger_log_images . get ( logger , lambda * args , ** kwargs : None ) logger_log_images ( pl_module , images , pl_module . global_step , split ) if is_train : pl_module . train () def check_frequency ( self , check_idx ): if (( check_idx % self . batch_freq ) == 0 or ( check_idx in self . log_steps )) and ( check_idx > 0 or self . log_first_step ): try : self . log_steps . pop ( 0 ) except IndexError as e : print ( e ) pass return True return False def on_train_batch_end ( self , trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ): if not self . disabled and ( pl_module . global_step > 0 or self . log_first_step ): self . log_img ( pl_module , batch , batch_idx , split = \"train\" ) def on_validation_batch_end ( self , trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ): if not self . disabled and pl_module . global_step > 0 : self . log_img ( pl_module , batch , batch_idx , split = \"val\" ) if hasattr ( pl_module , 'calibrate_grad_norm' ): if ( pl_module . calibrate_grad_norm and batch_idx % 25 == 0 ) and batch_idx > 0 : self . log_gradients ( trainer , pl_module , batch_idx = batch_idx ) class CUDACallback ( Callback ): # see https://github.com/SeanNaren/minGPT/blob/master/mingpt/callback.py def on_train_epoch_start ( self , trainer , pl_module ): # Reset the memory use counter torch . cuda . reset_peak_memory_stats ( trainer . root_gpu ) torch . cuda . synchronize ( trainer . root_gpu ) self . start_time = time . time () def on_train_epoch_end ( self , trainer , pl_module ): torch . cuda . synchronize ( trainer . root_gpu ) max_memory = torch . cuda . max_memory_allocated ( trainer . root_gpu ) / 2 ** 20 epoch_time = time . time () - self . start_time try : max_memory = trainer . training_type_plugin . reduce ( max_memory ) epoch_time = trainer . training_type_plugin . reduce ( epoch_time ) rank_zero_info ( f \"Average Epoch time: { epoch_time : .2f } seconds\" ) rank_zero_info ( f \"Average Peak memory { max_memory : .2f } MiB\" ) except AttributeError : pass class ModeSwapCallback ( Callback ): def __init__ ( self , swap_step = 2000 ): super () . __init__ () self . is_frozen = False self . swap_step = swap_step def on_train_epoch_start ( self , trainer , pl_module ): if trainer . global_step < self . swap_step and not self . is_frozen : self . is_frozen = True trainer . optimizers = [ pl_module . configure_opt_embedding ()] if trainer . global_step > self . swap_step and self . is_frozen : self . is_frozen = False trainer . optimizers = [ pl_module . configure_opt_model ()] if __name__ == \"__main__\" : # custom parser to specify config files, train, test and debug mode, # postfix, resume. # `--key value` arguments are interpreted as arguments to the trainer. # `nested.key=value` arguments are interpreted as config parameters. # configs are merged from left-to-right followed by command line parameters. # model: # base_learning_rate: float # target: path to lightning module # params: # key: value # data: # target: main.DataModuleFromConfig # params: # batch_size: int # wrap: bool # train: # target: path to train dataset # params: # key: value # validation: # target: path to validation dataset # params: # key: value # test: # target: path to test dataset # params: # key: value # lightning: (optional, has sane defaults and can be specified on cmdline) # trainer: # additional arguments to trainer # logger: # logger to instantiate # modelcheckpoint: # modelcheckpoint to instantiate # callbacks: # callback1: # target: importpath # params: # key: value now = datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H-%M-%S\" ) # add cwd for convenience and to make classes in this file available when # running as `python main.py` # (in particular `main.DataModuleFromConfig`) sys . path . append ( os . getcwd ()) parser = get_parser () parser = Trainer . add_argparse_args ( parser ) opt , unknown = parser . parse_known_args () if opt . name and opt . resume : raise ValueError ( \"-n/--name and -r/--resume cannot be specified both.\" \"If you want to resume training in a new log folder, \" \"use -n/--name in combination with --resume_from_checkpoint\" ) if opt . resume : if not os . path . exists ( opt . resume ): raise ValueError ( \"Cannot find {} \" . format ( opt . resume )) if os . path . isfile ( opt . resume ): paths = opt . resume . split ( \"/\" ) # idx = len(paths)-paths[::-1].index(\"logs\")+1 # logdir = \"/\".join(paths[:idx]) logdir = \"/\" . join ( paths [: - 2 ]) ckpt = opt . resume else : assert os . path . isdir ( opt . resume ), opt . resume logdir = opt . resume . rstrip ( \"/\" ) ckpt = os . path . join ( logdir , \"checkpoints\" , \"last.ckpt\" ) opt . resume_from_checkpoint = ckpt base_configs = sorted ( glob . glob ( os . path . join ( logdir , \"configs/*.yaml\" ))) opt . base = base_configs + opt . base _tmp = logdir . split ( \"/\" ) nowname = _tmp [ - 1 ] else : if opt . name : name = \"_\" + opt . name elif opt . base : cfg_fname = os . path . split ( opt . base [ 0 ])[ - 1 ] cfg_name = os . path . splitext ( cfg_fname )[ 0 ] name = \"_\" + cfg_name else : name = \"\" if opt . datadir_in_name : now = os . path . basename ( os . path . normpath ( opt . data_root )) + now nowname = now + name + opt . postfix logdir = os . path . join ( opt . logdir , nowname ) ckptdir = os . path . join ( logdir , \"checkpoints\" ) cfgdir = os . path . join ( logdir , \"configs\" ) seed_everything ( opt . seed ) try : # init and save configs configs = [ OmegaConf . load ( cfg ) for cfg in opt . base ] cli = OmegaConf . from_dotlist ( unknown ) config = OmegaConf . merge ( * configs , cli ) lightning_config = config . pop ( \"lightning\" , OmegaConf . create ()) # merge trainer cli with config trainer_config = lightning_config . get ( \"trainer\" , OmegaConf . create ()) # default to ddp trainer_config [ \"accelerator\" ] = \"ddp\" for k in nondefault_trainer_args ( opt ): trainer_config [ k ] = getattr ( opt , k ) if not \"gpus\" in trainer_config : del trainer_config [ \"accelerator\" ] cpu = True else : gpuinfo = trainer_config [ \"gpus\" ] print ( f \"Running on GPUs { gpuinfo } \" ) cpu = False trainer_opt = argparse . Namespace ( ** trainer_config ) lightning_config . trainer = trainer_config # model # config.model.params.personalization_config.params.init_word = opt.init_word # config.model.params.personalization_config.params.embedding_manager_ckpt = opt.embedding_manager_ckpt # config.model.params.personalization_config.params.placeholder_tokens = opt.placeholder_tokens # if opt.init_word: # config.model.params.personalization_config.params.initializer_words[0] = opt.init_word config . data . params . train . params . placeholder_token = opt . class_word config . data . params . reg . params . placeholder_token = opt . class_word config . data . params . validation . params . placeholder_token = opt . class_word if opt . actual_resume : model = load_model_from_config ( config , opt . actual_resume ) else : model = instantiate_from_config ( config . model ) # trainer and callbacks trainer_kwargs = dict () # default logger configs default_logger_cfgs = { \"wandb\" : { \"target\" : \"pytorch_lightning.loggers.WandbLogger\" , \"params\" : { \"name\" : nowname , \"save_dir\" : logdir , \"offline\" : opt . debug , \"id\" : nowname , } }, \"testtube\" : { \"target\" : \"pytorch_lightning.loggers.TestTubeLogger\" , \"params\" : { \"name\" : \"testtube\" , \"save_dir\" : logdir , } }, } default_logger_cfg = default_logger_cfgs [ \"testtube\" ] if \"logger\" in lightning_config : logger_cfg = lightning_config . logger else : logger_cfg = OmegaConf . create () logger_cfg = OmegaConf . merge ( default_logger_cfg , logger_cfg ) trainer_kwargs [ \"logger\" ] = instantiate_from_config ( logger_cfg ) # modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to # specify which metric is used to determine best models default_modelckpt_cfg = { \"target\" : \"pytorch_lightning.callbacks.ModelCheckpoint\" , \"params\" : { \"dirpath\" : ckptdir , \"filename\" : \" {epoch:06} \" , \"verbose\" : True , \"save_last\" : True , } } if hasattr ( model , \"monitor\" ): print ( f \"Monitoring { model . monitor } as checkpoint metric.\" ) default_modelckpt_cfg [ \"params\" ][ \"monitor\" ] = model . monitor default_modelckpt_cfg [ \"params\" ][ \"save_top_k\" ] = 1 if \"modelcheckpoint\" in lightning_config : modelckpt_cfg = lightning_config . modelcheckpoint else : modelckpt_cfg = OmegaConf . create () modelckpt_cfg = OmegaConf . merge ( default_modelckpt_cfg , modelckpt_cfg ) print ( f \"Merged modelckpt-cfg: \\n { modelckpt_cfg } \" ) if version . parse ( pl . __version__ ) < version . parse ( '1.4.0' ): trainer_kwargs [ \"checkpoint_callback\" ] = instantiate_from_config ( modelckpt_cfg ) # add callback which sets up log directory default_callbacks_cfg = { \"setup_callback\" : { \"target\" : \"main.SetupCallback\" , \"params\" : { \"resume\" : opt . resume , \"now\" : now , \"logdir\" : logdir , \"ckptdir\" : ckptdir , \"cfgdir\" : cfgdir , \"config\" : config , \"lightning_config\" : lightning_config , } }, \"image_logger\" : { \"target\" : \"main.ImageLogger\" , \"params\" : { \"batch_frequency\" : 750 , \"max_images\" : 4 , \"clamp\" : True } }, \"learning_rate_logger\" : { \"target\" : \"main.LearningRateMonitor\" , \"params\" : { \"logging_interval\" : \"step\" , # \"log_momentum\": True } }, \"cuda_callback\" : { \"target\" : \"main.CUDACallback\" }, } if version . parse ( pl . __version__ ) >= version . parse ( '1.4.0' ): default_callbacks_cfg . update ({ 'checkpoint_callback' : modelckpt_cfg }) if \"callbacks\" in lightning_config : callbacks_cfg = lightning_config . callbacks else : callbacks_cfg = OmegaConf . create () if 'metrics_over_trainsteps_checkpoint' in callbacks_cfg : print ( 'Caution: Saving checkpoints every n train steps without deleting. This might require some free space.' ) default_metrics_over_trainsteps_ckpt_dict = { 'metrics_over_trainsteps_checkpoint' : { \"target\" : 'pytorch_lightning.callbacks.ModelCheckpoint' , 'params' : { \"dirpath\" : os . path . join ( ckptdir , 'trainstep_checkpoints' ), \"filename\" : \" {epoch:06} - {step:09} \" , \"verbose\" : True , 'save_top_k' : - 1 , 'every_n_train_steps' : 10000 , 'save_weights_only' : True } } } default_callbacks_cfg . update ( default_metrics_over_trainsteps_ckpt_dict ) callbacks_cfg = OmegaConf . merge ( default_callbacks_cfg , callbacks_cfg ) if 'ignore_keys_callback' in callbacks_cfg and hasattr ( trainer_opt , 'resume_from_checkpoint' ): callbacks_cfg . ignore_keys_callback . params [ 'ckpt_path' ] = trainer_opt . resume_from_checkpoint elif 'ignore_keys_callback' in callbacks_cfg : del callbacks_cfg [ 'ignore_keys_callback' ] trainer_kwargs [ \"callbacks\" ] = [ instantiate_from_config ( callbacks_cfg [ k ]) for k in callbacks_cfg ] trainer_kwargs [ \"max_steps\" ] = trainer_opt . max_steps trainer = Trainer . from_argparse_args ( trainer_opt , ** trainer_kwargs ) trainer . logdir = logdir ### # data config . data . params . train . params . data_root = opt . data_root config . data . params . reg . params . data_root = opt . reg_data_root config . data . params . validation . params . data_root = opt . data_root data = instantiate_from_config ( config . data ) data = instantiate_from_config ( config . data ) # NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html # calling these ourselves should not be necessary but it is. # lightning still takes care of proper multiprocessing though data . prepare_data () data . setup () print ( \"#### Data #####\" ) for k in data . datasets : print ( f \" { k } , { data . datasets [ k ] . __class__ . __name__ } , { len ( data . datasets [ k ]) } \" ) # configure learning rate bs , base_lr = config . data . params . batch_size , config . model . base_learning_rate if not cpu : ngpu = len ( lightning_config . trainer . gpus . strip ( \",\" ) . split ( ',' )) else : ngpu = 1 if 'accumulate_grad_batches' in lightning_config . trainer : accumulate_grad_batches = lightning_config . trainer . accumulate_grad_batches else : accumulate_grad_batches = 1 print ( f \"accumulate_grad_batches = { accumulate_grad_batches } \" ) lightning_config . trainer . accumulate_grad_batches = accumulate_grad_batches if opt . scale_lr : model . learning_rate = accumulate_grad_batches * ngpu * bs * base_lr print ( \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\" . format ( model . learning_rate , accumulate_grad_batches , ngpu , bs , base_lr )) else : model . learning_rate = base_lr print ( \"++++ NOT USING LR SCALING ++++\" ) print ( f \"Setting learning rate to { model . learning_rate : .2e } \" ) # allow checkpointing via USR1 def melk ( * args , ** kwargs ): # run all checkpoint hooks if trainer . global_rank == 0 : print ( \"Summoning checkpoint.\" ) ckpt_path = os . path . join ( ckptdir , \"last.ckpt\" ) trainer . save_checkpoint ( ckpt_path ) def divein ( * args , ** kwargs ): if trainer . global_rank == 0 : import pudb ; pudb . set_trace () import signal signal . signal ( signal . SIGUSR1 , melk ) signal . signal ( signal . SIGUSR2 , divein ) # run if opt . train : try : trainer . fit ( model , data ) except Exception : melk () raise if not opt . no_test and not trainer . interrupted : trainer . test ( model , data ) except Exception : if opt . debug and trainer . global_rank == 0 : try : import pudb as debugger except ImportError : import pdb as debugger debugger . post_mortem () raise finally : # move newly created debug project to debug_runs if opt . debug and not opt . resume and trainer . global_rank == 0 : dst , name = os . path . split ( logdir ) dst = os . path . join ( dst , \"debug_runs\" , name ) os . makedirs ( os . path . split ( dst )[ 0 ], exist_ok = True ) os . rename ( logdir , dst ) if trainer . global_rank == 0 : print ( trainer . profiler . summary ())","title":"\u7c97\u8bfb\u4ee3\u7801"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_4","text":"","title":"\u590d\u73b0\u7ed3\u679c"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_5","text":"\u5c0f\u7f8a\u662f\u4e00\u53ea\u53ef\u7231\u7684\u8428\u6469\u8036\u72ac, \u6211\u4eec\u5e0c\u671b\u5229\u7528\u6a21\u578b\u548cdreambooth\u7684\u65b9\u6cd5\u770b\u770b\u80fd\u5426\u590d\u73b0\u5c0f\u7f8a\u7684\u6837\u5b50\u3002","title":"\u5c0f\u7f8a"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_6","text":"","title":"\u539f\u56fe"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_7","text":"","title":"\u751f\u6210"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#finetunedog","text":"python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 100 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/xiaoyang.ckpt \\ --prompt \"a photo of a dog\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/xiaoyang/after_tune_dog # default seed 42 \u9009\u62e90, 5, 9, 10 \u8fd9\u56db\u5f20\u56fe\u7247 \u524d \u540e","title":"finetune\u524d\u540edog\u5bf9\u6bd4"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_8","text":"python scripts/stable_txt2img.py \\ --ddim_eta 0 .0 --n_samples 2 --n_iter 2 \\ --scale 10 .0 --ddim_steps 50 \\ --ckpt /mfs/xiangchendong19/stable-diffusion-ckpt/xiaoyang.ckpt \\ --prompt \"a photo of a cat sitting on a green car\" \\ --outdir /home/xiangchendong19/Dreambooth-Stable-Diffusion/data/xiaoyang/others","title":"\u751f\u6210\u5176\u4ed6\u5185\u5bb9"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#after-finetune","text":"\u201ca photo of a cat sitting on a green car\u201d, seed = 42","title":"after finetune"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#before-finetune","text":"\u201ca photo of a cat sitting on a green car\u201d, seed = 42","title":"before finetune"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#magic-cube","text":"","title":"magic cube"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_9","text":"","title":"\u539f\u56fe"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_10","text":"","title":"\u751f\u6210"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/dream_booth/#_11","text":"\u8fdb\u884cfinetune\u80fd\u591f\u4f7f\u5f97\u6a21\u578b\u6355\u6349\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u7269\u4f53\u7684\u7279\u5f81 \u8fdb\u884cfinetune\u5bf9\u4e8e\u539f\u6765\u7684\u751f\u6210\u6548\u679c\u6709\u4e00\u5b9a\u7684\u6298\u6263\u3002 finetune\u6bd4\u8f83\u5feb, 800\u6b65\u8fed\u4ee3 \u6bcf\u4e2a\u7269\u4f53\u90fd\u9700\u898112G\u7684ckpt","title":"\u7ed3\u8bba"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/","text":"attention map hand inject [toc] \u524d\u8a00 \u6839\u636e\u6211\u4eec\u5728\u201cPrompt-to-Prompt Image Editing with Cross Attention Control\u201d\u8fd9\u7bc7\u6587\u7ae0\u590d\u73b0\u8fc7\u7a0b\u4e2d\u7684\u63a2\u7d22, \u6211\u4eec\u5c1d\u8bd5\u4e86\u624b\u52a8\u63a7\u5236attention map\u7684\u65b9\u5f0f, \u53d1\u73b0\u786e\u5b9e\u53ef\u4ee5\u5f97\u5230\u4e00\u5b9a\u7684\u6548\u679c, \u8fd9\u4e00\u90e8\u5206\u7684\u7814\u7a76\u503c\u5f97\u6df1\u5165, \u9644\u5728\u6587\u7ae0\u590d\u73b0\u7684\u7ae0\u8282\u663e\u5f97\u7565\u5fae\u6563\u4e71, \u6240\u4ee5\u518d\u5f00\u4e00\u4e2a\u65b0\u7684\u7ae0\u8282\u6765\u8bb0\u5f55\u5b9e\u9a8c\u7ed3\u679c\u3002 \u53ef\u4ee5\u7814\u7a76\u548c\u9a8c\u8bc1\u7684\u65b9\u5411: [ ] \u9a8c\u8bc1value\u5728\u8fc7\u7a0b\u4e2d\u662f\u5426\u4e0d\u53d8 [x] \u5c1d\u8bd5\u4e0d\u540c\u7684inject\u65b9\u6cd5 [x] inject \u66f4\u591a\u5927\u5c0f\u7684map [ ] \u5c1d\u8bd5self attention [ ] \u5c1d\u8bd5\u4eceinit image\u4e2d\u83b7\u53d6attention map [ ] \u5c1d\u8bd5\u4ece\u4e0d\u540c\u7684seed\u4e2d\u83b7\u53d6attention map [ ] \u5c1d\u8bd5\u67d0\u79cd\u5f62\u5f0f\u7684guidance \u524d\u671f\u5b9e\u9a8c \u5728\u521d\u6b65\u9a8c\u8bc1\u624b\u52a8inject\u6709\u6548\u679c\u540e, \u6211\u4eec\u521d\u6b65\u5c1d\u8bd5\u4e86\u4e0d\u540cinject\u65b9\u5f0f\u4ee5\u53ca\u5176\u4ed6\u53c2\u6570\u7684\u5f71\u54cd, \u83b7\u5f97\u57fa\u672c\u7684\u53c2\u6570\u5f71\u54cd\u8ba4\u77e5\u3002 function scale \u672c\u5b9e\u9a8c\u5c1d\u8bd5\u4e86\u4e0d\u540c\u7684inject function\u7684\u5f71\u54cd, \u6bcf\u4e00\u5217\u4e3a\u4e00\u4e2afunction, \u5206\u522b\u4e3a def inject1 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute adding on softmaxed map, only on 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] += inject_map * inject_scale return attn_slice def inject2 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute replace on softmaxed map, only on 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] = inject_map * inject_scale return attn_slice def inject3 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute adding on attention scores, only on 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] += inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice def inject4 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute replacing on attention scores, only on 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] = inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice def inject5 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute adding on softmaxed map, on both 32*32 and 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] += inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attn_slice [:,:, word_index ] += inject_map * inject_scale return attn_slice def inject6 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute replace on softmaxed map, on both 32*32 and 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] = inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attn_slice [:,:, word_index ] = inject_map * inject_scale return attn_slice def inject7 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute adding on attention scores, on both 32*32 and 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] += inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attention_scores [:,:, word_index ] += inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice def inject8 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute replacing on attention scores, on both 32*32 and 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] = inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attention_scores [:,:, word_index ] = inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice inject_functions = [ inject1 , inject2 , inject3 , inject4 , inject5 , inject6 , inject7 , inject8 ] prompt_edit_tokens_start=0.8 scale in [0, 1, 2, 3, 4], \u4f9d\u6b21\u9012\u589e \u7ed3\u8bba\u662f, inject\u66f4\u591a\u7684map\u66f4\u6709\u6548, \u7edd\u5bf9\u503c\u52a0\u5728\u5f52\u4e00\u5316\u4e4b\u540e\u4f1a\u66f4\u6709\u6548, \u4f46\u4f1a\u53d8\u5f97\u4e0d\u7a33\u5b9a\u3002 map-scale \u6a2a\u5411scale [1., 1.5,2., 2.3, 2.7, 3., 3.2, 3.5] function = inject5, \u7b2c\u4e00\u884c\u4e3a\u540c\u65f6\u63a7\u5236car\u548canimal\u7684map, \u7b2c\u4e8c\u884c\u53ea\u63a7\u5236animal, \u7b2c\u4e09\u884c\u53ea\u63a7\u5236car \u53ef\u4ee5\u505a\u5230\u5206\u522b\u63a7\u5236, \u4f46\u662f\u63a7\u5236\u4f9d\u7136\u4e0d\u7a33\u5b9a\u3002 scale-cs for scale in [1., 1.5,2., 2.3, 2.7, 3., 3.2, 3.5]: for cs in [0.95, 0.9, 0.85, 0.8, 0.75, 0.6, 0.3]: \u6a2a\u5411, cs \u4ece0.3\u52300.95 \u7eb5\u5411, scale \u4ece1. \u52303.5 \u4f5c\u7528\u529b\u5ea6\u8fc7\u5f3a, \u4f5c\u7528\u65f6\u95f4\u8fc7\u957f, \u90fd\u5bb9\u6613\u751f\u6210\u7cdf\u7cd5\u7684\u56fe\u50cf\u3002 number \u4e4b\u524d\u6709\u6ce8\u610f\u5230, diffusion model\u5bf9\u4e8e\u6570\u91cf\u7684\u63a7\u5236\u4e0d\u662f\u5f88\u7406\u60f3, \u518d\u8054\u60f3\u5230\u5728\u4e4b\u524d\u7684attention map inject\u8fc7\u7a0b\u4e2d, \u8001\u864e\u56e0\u4e3a\u8eab\u4f53\u8fc7\u957f\u800c\u5bfc\u81f4\u5206\u88c2, \u51fa\u73b0\u4e24\u4e2a\u864e\u5934, \u8fd9\u53ef\u80fd\u8bf4\u660eattention\u533a\u57df\u7684\u6570\u91cf\u4ee5\u53ca\u5206\u79bb\u7a0b\u5ea6\u5f88\u53ef\u80fd\u5f71\u54cd\u56fe\u7247\u4e2d\u7269\u4f53\u7684\u6570\u91cf\u3002 \u5bfb\u627e\u7a33\u5b9a\u65b9\u5f0f \u6211\u4eec\u6ce8\u610f\u5230, \u4e0d\u540c\u9636\u6bb5, \u540c\u4e00\u4e2a\u8bcd\u5bf9\u5e94\u7684attention map\u7684\u5747\u503c\u6216\u8005\u8bf4\u7edd\u5bf9\u503c\u5927\u5c0f\u662f\u4e0d\u4e00\u6837\u7684, \u539f\u56e0\u662f\u5728\u83b7\u5f97attention score\u540e, \u662f\u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6, \u4e5f\u5c31\u662f\u5355\u8bcd\u7684\u7ef4\u5ea6\u8fdb\u884csoftmax\u3002\u4e5f\u5c31\u662f\u8bf4, \u5728\u540c\u4e00\u6b21\u8fed\u4ee3\u4e2d, \u4e0d\u540c\u8bcd\u5bf9\u5e94\u7684attention map\u7684\u6743\u91cd\u53ef\u80fd\u662f\u4e0d\u4e00\u6837\u7684, \u53ef\u80fd\u5728\u67d0\u4e00\u6b65, \u6a21\u578b\u6ce8\u91cd\u751f\u6210\u7b2c\u4e8c\u4e2a\u8bcd\u76f8\u5173\u7684\u5185\u5bb9, \u800c\u4e0b\u4e00\u6b65\u6a21\u578b\u53c8\u6ce8\u91cd\u7b2c\u56db\u4e2a\u8bcd\u7684\u751f\u6210\u4e86\u3002 \u5177\u4f53\u800c\u8a00, \u6211\u4eec\u6253\u5370\u4e8664x64\u7684attention map\u7684\u5747\u503c\u548cnorm: attn_slice = attention_scores . softmax ( dim =- 1 ) for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] if attention_scores . shape [ 1 ] == 64 * 64 : print ( attn_slice [:,:, word_index ] . mean ()) #print(torch.norm(attn_slice[:,:,word_index])) #\u5747\u503c tensor(0.0080, device='cuda:2') tensor(0.0080, device='cuda:2') tensor(0.0051, device='cuda:2') tensor(0.0148, device='cuda:2') tensor(0.0274, device='cuda:2') tensor(0.0076, device='cuda:2') tensor(0.0078, device='cuda:2') tensor(0.0049, device='cuda:2') tensor(0.0146, device='cuda:2') tensor(0.0282, device='cuda:2') tensor(0.0072, device='cuda:2') #nrom tensor(1.1415, device='cuda:2') tensor(5.9604, device='cuda:2') tensor(3.5211, device='cuda:2') tensor(0.9643, device='cuda:2') tensor(1.2418, device='cuda:2') tensor(1.1404, device='cuda:2') tensor(5.9061, device='cuda:2') tensor(3.6695, device='cuda:2') \u6240\u4ee5\u6211\u4eec\u51b3\u5b9a\u4f9d\u636enorm\u6765\u786e\u5b9a\u6211\u4eecinject\u7684scale, \u8fd9\u6837\u5c31\u4e0d\u4f1a\u56e0\u4e3a\u7edd\u5bf9\u503c\u4e0a\u7684inject\u800c\u5bfc\u81f4\u5f88\u96be\u8c03\u8282\u6216\u8005\u4e0d\u7a33\u5b9a\u7684\u60c5\u51b5, \u5b9e\u9a8c\u8fd8\u5728\u505a\u3002 \u6211\u4eec\u5c1d\u8bd5\u4e86\u8bb8\u591a\u7684\u548cnorm\u76f8\u5173\u7684\u8bbe\u7f6einject scale\u7684\u65b9\u6cd5, \u4f46\u662f\u4f9d\u7136\u4e0dwork, \u7a33\u5b9a\u7684\u63a7\u5236\u6548\u679c\u4e0d\u597d, \u63a7\u5236\u6548\u679c\u597d\u7684\u4e0d\u7a33\u5b9a\u3002 \u548c\u5b66\u957f\u4ea4\u6d41\u540e, \u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u67d0\u4e9bguidance\u7684\u65b9\u5f0f, \u6216\u8005\u7528\u522b\u7684\u65b9\u5f0f\u5f97\u5230\u6587\u7406\u6bd4\u8f83\u5408\u7406\u7684attention map\u3002 \u540c\u65f6\u6211\u4eec\u9700\u8981\u68c0\u67e5self attention\u5982\u4f55\u6539\u53d8, \u540c\u65f6, \u6211\u4eec\u9700\u8981\u9a8c\u8bc1value\u662f\u5426\u4e00\u76f4\u90fd\u4e0d\u53d8\u5316\u3002","title":"attention map hand inject"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#attention-map-hand-inject","text":"[toc]","title":"attention map hand inject"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#_1","text":"\u6839\u636e\u6211\u4eec\u5728\u201cPrompt-to-Prompt Image Editing with Cross Attention Control\u201d\u8fd9\u7bc7\u6587\u7ae0\u590d\u73b0\u8fc7\u7a0b\u4e2d\u7684\u63a2\u7d22, \u6211\u4eec\u5c1d\u8bd5\u4e86\u624b\u52a8\u63a7\u5236attention map\u7684\u65b9\u5f0f, \u53d1\u73b0\u786e\u5b9e\u53ef\u4ee5\u5f97\u5230\u4e00\u5b9a\u7684\u6548\u679c, \u8fd9\u4e00\u90e8\u5206\u7684\u7814\u7a76\u503c\u5f97\u6df1\u5165, \u9644\u5728\u6587\u7ae0\u590d\u73b0\u7684\u7ae0\u8282\u663e\u5f97\u7565\u5fae\u6563\u4e71, \u6240\u4ee5\u518d\u5f00\u4e00\u4e2a\u65b0\u7684\u7ae0\u8282\u6765\u8bb0\u5f55\u5b9e\u9a8c\u7ed3\u679c\u3002 \u53ef\u4ee5\u7814\u7a76\u548c\u9a8c\u8bc1\u7684\u65b9\u5411: [ ] \u9a8c\u8bc1value\u5728\u8fc7\u7a0b\u4e2d\u662f\u5426\u4e0d\u53d8 [x] \u5c1d\u8bd5\u4e0d\u540c\u7684inject\u65b9\u6cd5 [x] inject \u66f4\u591a\u5927\u5c0f\u7684map [ ] \u5c1d\u8bd5self attention [ ] \u5c1d\u8bd5\u4eceinit image\u4e2d\u83b7\u53d6attention map [ ] \u5c1d\u8bd5\u4ece\u4e0d\u540c\u7684seed\u4e2d\u83b7\u53d6attention map [ ] \u5c1d\u8bd5\u67d0\u79cd\u5f62\u5f0f\u7684guidance","title":"\u524d\u8a00"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#_2","text":"\u5728\u521d\u6b65\u9a8c\u8bc1\u624b\u52a8inject\u6709\u6548\u679c\u540e, \u6211\u4eec\u521d\u6b65\u5c1d\u8bd5\u4e86\u4e0d\u540cinject\u65b9\u5f0f\u4ee5\u53ca\u5176\u4ed6\u53c2\u6570\u7684\u5f71\u54cd, \u83b7\u5f97\u57fa\u672c\u7684\u53c2\u6570\u5f71\u54cd\u8ba4\u77e5\u3002","title":"\u524d\u671f\u5b9e\u9a8c"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#function-scale","text":"\u672c\u5b9e\u9a8c\u5c1d\u8bd5\u4e86\u4e0d\u540c\u7684inject function\u7684\u5f71\u54cd, \u6bcf\u4e00\u5217\u4e3a\u4e00\u4e2afunction, \u5206\u522b\u4e3a def inject1 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute adding on softmaxed map, only on 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] += inject_map * inject_scale return attn_slice def inject2 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute replace on softmaxed map, only on 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] = inject_map * inject_scale return attn_slice def inject3 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute adding on attention scores, only on 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] += inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice def inject4 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute replacing on attention scores, only on 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] = inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice def inject5 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute adding on softmaxed map, on both 32*32 and 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] += inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attn_slice [:,:, word_index ] += inject_map * inject_scale return attn_slice def inject6 ( attention_scores , inject_map_dicts , inject_scale = 3 ): \"\"\" absolute replace on softmaxed map, on both 32*32 and 64*64 maps \"\"\" attn_slice = attention_scores . softmax ( dim =- 1 ) if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attn_slice [:,:, word_index ] = inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attn_slice [:,:, word_index ] = inject_map * inject_scale return attn_slice def inject7 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute adding on attention scores, on both 32*32 and 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] += inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attention_scores [:,:, word_index ] += inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice def inject8 ( attention_scores , inject_map_dicts , inject_scale = 30 ): \"\"\" absolute replacing on attention scores, on both 32*32 and 64*64 maps \"\"\" if attention_scores . shape [ 1 ] == 64 * 64 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map\" ] attention_scores [:,:, word_index ] = inject_map * inject_scale elif attention_scores . shape [ 1 ] == 32 * 32 : for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] inject_map = dic [ \"inject_map_32\" ] attention_scores [:,:, word_index ] = inject_map * inject_scale attn_slice = attention_scores . softmax ( dim =- 1 ) return attn_slice inject_functions = [ inject1 , inject2 , inject3 , inject4 , inject5 , inject6 , inject7 , inject8 ] prompt_edit_tokens_start=0.8 scale in [0, 1, 2, 3, 4], \u4f9d\u6b21\u9012\u589e","title":"function scale"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#injectmap","text":"","title":"\u7ed3\u8bba\u662f, inject\u66f4\u591a\u7684map\u66f4\u6709\u6548, \u7edd\u5bf9\u503c\u52a0\u5728\u5f52\u4e00\u5316\u4e4b\u540e\u4f1a\u66f4\u6709\u6548, \u4f46\u4f1a\u53d8\u5f97\u4e0d\u7a33\u5b9a\u3002"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#map-scale","text":"\u6a2a\u5411scale [1., 1.5,2., 2.3, 2.7, 3., 3.2, 3.5] function = inject5, \u7b2c\u4e00\u884c\u4e3a\u540c\u65f6\u63a7\u5236car\u548canimal\u7684map, \u7b2c\u4e8c\u884c\u53ea\u63a7\u5236animal, \u7b2c\u4e09\u884c\u53ea\u63a7\u5236car \u53ef\u4ee5\u505a\u5230\u5206\u522b\u63a7\u5236, \u4f46\u662f\u63a7\u5236\u4f9d\u7136\u4e0d\u7a33\u5b9a\u3002","title":"map-scale"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#scale-cs","text":"for scale in [1., 1.5,2., 2.3, 2.7, 3., 3.2, 3.5]: for cs in [0.95, 0.9, 0.85, 0.8, 0.75, 0.6, 0.3]: \u6a2a\u5411, cs \u4ece0.3\u52300.95 \u7eb5\u5411, scale \u4ece1. \u52303.5 \u4f5c\u7528\u529b\u5ea6\u8fc7\u5f3a, \u4f5c\u7528\u65f6\u95f4\u8fc7\u957f, \u90fd\u5bb9\u6613\u751f\u6210\u7cdf\u7cd5\u7684\u56fe\u50cf\u3002","title":"scale-cs"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#number","text":"\u4e4b\u524d\u6709\u6ce8\u610f\u5230, diffusion model\u5bf9\u4e8e\u6570\u91cf\u7684\u63a7\u5236\u4e0d\u662f\u5f88\u7406\u60f3, \u518d\u8054\u60f3\u5230\u5728\u4e4b\u524d\u7684attention map inject\u8fc7\u7a0b\u4e2d, \u8001\u864e\u56e0\u4e3a\u8eab\u4f53\u8fc7\u957f\u800c\u5bfc\u81f4\u5206\u88c2, \u51fa\u73b0\u4e24\u4e2a\u864e\u5934, \u8fd9\u53ef\u80fd\u8bf4\u660eattention\u533a\u57df\u7684\u6570\u91cf\u4ee5\u53ca\u5206\u79bb\u7a0b\u5ea6\u5f88\u53ef\u80fd\u5f71\u54cd\u56fe\u7247\u4e2d\u7269\u4f53\u7684\u6570\u91cf\u3002","title":"number"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/hand_inject/#_3","text":"\u6211\u4eec\u6ce8\u610f\u5230, \u4e0d\u540c\u9636\u6bb5, \u540c\u4e00\u4e2a\u8bcd\u5bf9\u5e94\u7684attention map\u7684\u5747\u503c\u6216\u8005\u8bf4\u7edd\u5bf9\u503c\u5927\u5c0f\u662f\u4e0d\u4e00\u6837\u7684, \u539f\u56e0\u662f\u5728\u83b7\u5f97attention score\u540e, \u662f\u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6, \u4e5f\u5c31\u662f\u5355\u8bcd\u7684\u7ef4\u5ea6\u8fdb\u884csoftmax\u3002\u4e5f\u5c31\u662f\u8bf4, \u5728\u540c\u4e00\u6b21\u8fed\u4ee3\u4e2d, \u4e0d\u540c\u8bcd\u5bf9\u5e94\u7684attention map\u7684\u6743\u91cd\u53ef\u80fd\u662f\u4e0d\u4e00\u6837\u7684, \u53ef\u80fd\u5728\u67d0\u4e00\u6b65, \u6a21\u578b\u6ce8\u91cd\u751f\u6210\u7b2c\u4e8c\u4e2a\u8bcd\u76f8\u5173\u7684\u5185\u5bb9, \u800c\u4e0b\u4e00\u6b65\u6a21\u578b\u53c8\u6ce8\u91cd\u7b2c\u56db\u4e2a\u8bcd\u7684\u751f\u6210\u4e86\u3002 \u5177\u4f53\u800c\u8a00, \u6211\u4eec\u6253\u5370\u4e8664x64\u7684attention map\u7684\u5747\u503c\u548cnorm: attn_slice = attention_scores . softmax ( dim =- 1 ) for dic in inject_map_dicts : word_index = dic [ \"word_index\" ] if attention_scores . shape [ 1 ] == 64 * 64 : print ( attn_slice [:,:, word_index ] . mean ()) #print(torch.norm(attn_slice[:,:,word_index])) #\u5747\u503c tensor(0.0080, device='cuda:2') tensor(0.0080, device='cuda:2') tensor(0.0051, device='cuda:2') tensor(0.0148, device='cuda:2') tensor(0.0274, device='cuda:2') tensor(0.0076, device='cuda:2') tensor(0.0078, device='cuda:2') tensor(0.0049, device='cuda:2') tensor(0.0146, device='cuda:2') tensor(0.0282, device='cuda:2') tensor(0.0072, device='cuda:2') #nrom tensor(1.1415, device='cuda:2') tensor(5.9604, device='cuda:2') tensor(3.5211, device='cuda:2') tensor(0.9643, device='cuda:2') tensor(1.2418, device='cuda:2') tensor(1.1404, device='cuda:2') tensor(5.9061, device='cuda:2') tensor(3.6695, device='cuda:2') \u6240\u4ee5\u6211\u4eec\u51b3\u5b9a\u4f9d\u636enorm\u6765\u786e\u5b9a\u6211\u4eecinject\u7684scale, \u8fd9\u6837\u5c31\u4e0d\u4f1a\u56e0\u4e3a\u7edd\u5bf9\u503c\u4e0a\u7684inject\u800c\u5bfc\u81f4\u5f88\u96be\u8c03\u8282\u6216\u8005\u4e0d\u7a33\u5b9a\u7684\u60c5\u51b5, \u5b9e\u9a8c\u8fd8\u5728\u505a\u3002 \u6211\u4eec\u5c1d\u8bd5\u4e86\u8bb8\u591a\u7684\u548cnorm\u76f8\u5173\u7684\u8bbe\u7f6einject scale\u7684\u65b9\u6cd5, \u4f46\u662f\u4f9d\u7136\u4e0dwork, \u7a33\u5b9a\u7684\u63a7\u5236\u6548\u679c\u4e0d\u597d, \u63a7\u5236\u6548\u679c\u597d\u7684\u4e0d\u7a33\u5b9a\u3002 \u548c\u5b66\u957f\u4ea4\u6d41\u540e, \u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u67d0\u4e9bguidance\u7684\u65b9\u5f0f, \u6216\u8005\u7528\u522b\u7684\u65b9\u5f0f\u5f97\u5230\u6587\u7406\u6bd4\u8f83\u5408\u7406\u7684attention map\u3002 \u540c\u65f6\u6211\u4eec\u9700\u8981\u68c0\u67e5self attention\u5982\u4f55\u6539\u53d8, \u540c\u65f6, \u6211\u4eec\u9700\u8981\u9a8c\u8bc1value\u662f\u5426\u4e00\u76f4\u90fd\u4e0d\u53d8\u5316\u3002","title":"\u5bfb\u627e\u7a33\u5b9a\u65b9\u5f0f"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/","text":"prompt-to-prompt [toc] \u524d\u8a00 \u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u662f\u805a\u7126\u7814\u7a76\u5728diffusion model\u751f\u6210\u7684\u8fc7\u7a0b\u4e2dAttention map\u662f\u5982\u4f55\u8d77\u4f5c\u7528\u7684, \u53ef\u4ee5\u7528\u4ec0\u4e48\u6837\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u5229\u7528attentionmap\u6765\u63a7\u5236\u6a21\u578b\u7684\u751f\u6210\u3002\u8fd9\u4e2a\u90e8\u5206\u6700\u4e3b\u8981\u7684\u542f\u53d1\u5de5\u4f5c\u6765\u6e90\u4e8egoogle\u7684\u201cPrompt-to-Prompt Image Editing with Cross Attention Control\u201d, \u4e5f\u662f\u6211\u4eec\u5e0c\u671b\u590d\u73b0\u548c\u5b66\u4e60\u7684\u6587\u7ae0\u3002 \u6807\u8bb0\u8bf4\u660e Cross Attention map\u7684\u66ff\u6362\u4eceprompt_edit_tokens_end\u5f00\u59cb, \u5230prompt_edit_tokens_start\u7ed3\u675f Self Attention map\u7684\u66ff\u6362\u4eceprompt_edit_spatial_end\u5f00\u59cb, prompt_edit_spatial_start\u7ed3\u675f ss \u662f self Attention inject start \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_spatial_start , \u800c\u9ed8\u8ba4\u7684 prompt_edit_tokens_end=1.0 , \u751f\u6210\u8fc7\u7a0b\u662f1.0 -> 0, \u6240\u4ee5map inject \u4ece\u4e00\u5f00\u59cb\u5c31\u5f00\u59cb, \u5230 prompt_edit_spatial_start \u7ed3\u675f, \u4f8b\u5982\u751f\u6210\u8fc7\u7a0b50\u6b65, prompt_edit_spatial_start=0.8 , \u8fd9\u610f\u5473\u7740\u524d 50*(1-0.8)=10 \u6b65\u662f\u6709map inject, \u540e\u7eed\u5219\u6ca1\u6709\u3002\u5982\u679c prompt_edit_spatial_start=prompt_edit_spatial_end=0 , \u5219\u5168\u79f0\u65e0inject, \u751f\u6210src\u539f\u56fe, \u5373\u56fe\u7247\u77e9\u9635\u6700\u53f3\u4e0b\u89d2\u7684\u56fe\u7247\u3002 cs \u662f cross Attention inject start \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_tokens_start , \u4f5c\u7528\u673a\u5236\u540c\u7406\u3002 ce \u662f corss Attention inject end \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_tokens_end \u3002 se \u662f self Attention inject end \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_spatial_end \u3002 \u590d\u73b0\u548c\u719f\u6089\u8fc7\u7a0b \u4e4b\u524d\u5728github\u4e0a\u770b\u5230\u4e86\u6709\u4eba\u5bf9google\u7684\u8fd9\u7bc7\u6587\u7ae0\u8fdb\u884c\u4e86\u590d\u73b0, \u5728\u8fd9\u91cc\u8fdb\u884c\u5c1d\u8bd5, \u590d\u73b0\u4ed6\u7684\u7ed3\u679c, \u7136\u540e\u7814\u7a76\u4e00\u4e0b\u4ed6\u662f\u600e\u4e48\u505a, \u4ee5\u53ca\u6211\u4eec\u53ef\u4ee5\u600e\u4e48\u505a\u5427\u3002\u76f4\u63a5\u4e0a\u4ee3\u7801\u3002 git clone git@github.com:bloc97/CrossAttentionControl.git pip install torch transformers diffusers == 0 .4.1 numpy pillow tqdm jupyter jupyter-notebook # \u5982\u679c\u662fssh\u5728\u670d\u52a1\u5668\u4e0a, \u5219\u9700\u8981\u901a\u8fc7ssh\u4f20\u9012 jupyter-notebook --no-browser --port = 1234 # on server ssh -NL localhost:1234:localhost:1234 g5 # on your pc, then open link in server jupyter, notice port need to be your host port \u8dd1\u662f\u80fd\u8dd1\u7684, \u4e0d\u65ad\u6309shift + enter, \u9664\u4e86\u6a21\u578b\u4e0b\u8f7d\u6162\u4e00\u70b9, \u5176\u4ed6\u90fd\u53ef\u4ee5\u3002 \u7c97\u8bfb\u4ee3\u7801 \u5f15\u5165\u5305, load\u6a21\u578b import torch from transformers import CLIPModel , CLIPTextModel , CLIPTokenizer from diffusers import AutoencoderKL , UNet2DConditionModel #NOTE: Last tested working diffusers version is diffusers==0.4.1, https://github.com/huggingface/diffusers/releases/tag/v0.4.1 #Init CLIP tokenizer and model model_path_clip = \"openai/clip-vit-large-patch14\" clip_tokenizer = CLIPTokenizer . from_pretrained ( model_path_clip ) clip_model = CLIPModel . from_pretrained ( model_path_clip , torch_dtype = torch . float16 ) clip = clip_model . text_model #Init diffusion model auth_token = \"\u8fd9\u4e2a\u662fhugging face \u7684access token\" #Replace this with huggingface auth token as a string if model is not already downloaded model_path_diffusion = \"CompVis/stable-diffusion-v1-4\" # \u770b\u8d77\u6765stable diffusion\u7528\u7684diffusion model\u5c31\u662f\u8fd9\u4e00\u4e2a, \u4f46\u662f\u5bf9\u4e8e\u6a21\u578b\u53c2\u6570\u800c\u8a00, \u4e0d\u77e5\u9053\u4ed6\u5230\u5e95\u4e0b\u8f7d\u7684\u662f\u53c2\u6570\u7684\u90a3\u4e00\u90e8\u5206, \u800c\u4e14\u7528\u4e86\u534a\u7cbe\u5ea6 # \u4ee3\u7801\u5728\u8fd9\u91cc https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py unet = UNet2DConditionModel . from_pretrained ( model_path_diffusion , subfolder = \"unet\" , use_auth_token = auth_token , revision = \"fp16\" , torch_dtype = torch . float16 ) # \u8fd9\u770b\u8d77\u6765\u5c31\u662fstable diffusion\u7528\u7684vae vae = AutoencoderKL . from_pretrained ( model_path_diffusion , subfolder = \"vae\" , use_auth_token = auth_token , revision = \"fp16\" , torch_dtype = torch . float16 ) #Move to GPU device = \"cuda\" unet . to ( device ) vae . to ( device ) clip . to ( device ) print ( \"Loaded all models\" ) \u5199\u7684\u4e00\u4e9b\u6709\u5173attention\u7684\u51fd\u6570 import numpy as np import random from PIL import Image from diffusers import LMSDiscreteScheduler # https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py # \u8fd9\u4e2a\u7c7b\u7684\u4ee3\u7801\u5728\u8fd9\u91cc, \u4f46\u662f\u5177\u4f53\u7684\u4f5c\u7528\u76ee\u524d\u8fd8\u4e0d\u77e5\u9053 from tqdm.auto import tqdm from torch import autocast # \u6df7\u5408\u7cbe\u5ea6\u7684\u4e1c\u897f from difflib import SequenceMatcher # \u6bd4\u8f83\u4e24\u4e2a\u5e8f\u5217 def init_attention_weights ( weight_tuples ): \"\"\" \u521d\u59cb\u5316\u6743\u91cd, \u5982\u679c\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684\u4f4d\u7f6e\u7684\u6743\u91cd, \u5219\u66ff\u6362, \u5426\u5219\u5168\u4e3a1 \"\"\" tokens_length = clip_tokenizer . model_max_length weights = torch . ones ( tokens_length ) for i , w in weight_tuples : if i < tokens_length and i >= 0 : weights [ i ] = w # TODO \u6743\u91cd\u662f\u4e00\u4e2a\u5e8f\u5217\u957f\u5ea6\u7684\u4e00\u7ef4\u5411\u91cf, \u88ab\u521d\u59cb\u5316\u5230last_attn_slice_weights, \u8fd9\u662f\u4ec0\u4e48\u4e1c\u897f for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . last_attn_slice_weights = weights . to ( device ) if module_name == \"CrossAttention\" and \"attn1\" in name : module . last_attn_slice_weights = None def init_attention_edit ( tokens , tokens_edit ): \"\"\" \"\"\" tokens_length = clip_tokenizer . model_max_length mask = torch . zeros ( tokens_length ) indices_target = torch . arange ( tokens_length , dtype = torch . long ) # 0, 1, 2... indices = torch . zeros ( tokens_length , dtype = torch . long ) # 0, 0, 0 ... tokens = tokens . input_ids . numpy ()[ 0 ] # \u5b57\u5178index? tokens_edit = tokens_edit . input_ids . numpy ()[ 0 ] for name , a0 , a1 , b0 , b1 in SequenceMatcher ( None , tokens , tokens_edit ) . get_opcodes (): if b0 < tokens_length : if name == \"equal\" or ( name == \"replace\" and a1 - a0 == b1 - b0 ): mask [ b0 : b1 ] = 1 indices [ b0 : b1 ] = indices_target [ a0 : a1 ] for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . last_attn_slice_mask = mask . to ( device ) module . last_attn_slice_indices = indices . to ( device ) if module_name == \"CrossAttention\" and \"attn1\" in name : module . last_attn_slice_mask = None module . last_attn_slice_indices = None # TODO \u5f04\u6e05last_attn_slice\u662f\u4ec0\u4e48, sliced_attention, attention\u4e24\u4e2a\u51fd\u6570\u7684\u533a\u522b def init_attention_func (): #ORIGINAL SOURCE CODE: https://github.com/huggingface/diffusers/blob/91ddd2a25b848df0fa1262d4f1cd98c7ccb87750/src/diffusers/models/attention.py#L276 def new_attention ( self , query , key , value ): # TODO: use baddbmm for better performance # query \u548c key \u901a\u8fc7\u77e9\u9635\u4e58, \u518d\u901a\u8fc7softmax\u5f97\u5230attention map attention_scores = torch . matmul ( query , key . transpose ( - 1 , - 2 )) * self . scale attn_slice = attention_scores . softmax ( dim =- 1 ) # compute attention output if self . use_last_attn_slice : if self . last_attn_slice_mask is not None : new_attn_slice = torch . index_select ( self . last_attn_slice , - 1 , self . last_attn_slice_indices ) # \u5728\u5355\u8bcd\u7ef4\u5ea6\u8fdb\u884c\u9009\u62e9 attn_slice = attn_slice * ( 1 - self . last_attn_slice_mask ) + new_attn_slice * self . last_attn_slice_mask else : attn_slice = self . last_attn_slice self . use_last_attn_slice = False # \u8fd9\u4e00\u6b65\u662f\u5728\u8fdb\u884cedit\u4e4b\u524d\u7684\u6267\u884c\u7684\u4e00\u6b21\u4f7f\u7528\u539f\u59cbprompt\u8fdb\u884cforward\uff0c\u7136\u540e\u4fdd\u5b58attn map if self . save_last_attn_slice : self . last_attn_slice = attn_slice self . save_last_attn_slice = False if self . use_last_attn_weights and self . last_attn_slice_weights is not None : attn_slice = attn_slice * self . last_attn_slice_weights self . use_last_attn_weights = False # \u5982\u679c\u6ca1\u6709injection, \u5219\u76f4\u63a5\u518d\u77e9\u9635\u4e58value, \u5c31\u5f97\u5230\u4e86\u4e0b\u4e00\u5c42\u7684\u8f93\u51fa hidden_states = torch . matmul ( attn_slice , value ) # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states def new_sliced_attention ( self , query , key , value , sequence_length , dim ): batch_size_attention = query . shape [ 0 ] hidden_states = torch . zeros ( ( batch_size_attention , sequence_length , dim // self . heads ), device = query . device , dtype = query . dtype ) slice_size = self . _slice_size if self . _slice_size is not None else hidden_states . shape [ 0 ] for i in range ( hidden_states . shape [ 0 ] // slice_size ): start_idx = i * slice_size end_idx = ( i + 1 ) * slice_size attn_slice = ( torch . matmul ( query [ start_idx : end_idx ], key [ start_idx : end_idx ] . transpose ( 1 , 2 )) * self . scale ) # TODO: use baddbmm for better performance attn_slice = attn_slice . softmax ( dim =- 1 ) if self . use_last_attn_slice : if self . last_attn_slice_mask is not None : new_attn_slice = torch . index_select ( self . last_attn_slice , - 1 , self . last_attn_slice_indices ) attn_slice = attn_slice * ( 1 - self . last_attn_slice_mask ) + new_attn_slice * self . last_attn_slice_mask else : attn_slice = self . last_attn_slice self . use_last_attn_slice = False if self . save_last_attn_slice : self . last_attn_slice = attn_slice self . save_last_attn_slice = False if self . use_last_attn_weights and self . last_attn_slice_weights is not None : attn_slice = attn_slice * self . last_attn_slice_weights self . use_last_attn_weights = False attn_slice = torch . matmul ( attn_slice , value [ start_idx : end_idx ]) hidden_states [ start_idx : end_idx ] = attn_slice # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" : module . last_attn_slice = None module . use_last_attn_slice = False module . use_last_attn_weights = False module . save_last_attn_slice = False # \u4ee5\u4e0b\u662f\u4e00\u4e2a\u63cf\u8ff0\u5668\u529f\u80fd, \u6b64\u540e\u8c03\u7528module._sliced_attention, \u7b49\u4ef7\u4e8e\u8c03\u7528new_sliced_attention # \u4f46\u662f\u4e3a\u4ec0\u4e48\u4e0d\u80fd\u76f4\u63a5\u628a\u51fd\u6570\u5730\u5740\u4f20\u8fdb\u53bb\u5462, \u4e5f\u5c31\u662fmodule._sliced_attention = new_sliced_attention # \u8fd9\u662f\u56e0\u4e3a\u51fd\u6570\u4e2d\u6709\u53c2\u6570self,\u5982\u679c\u6309\u5730\u5740\u4f20, self\u4e5f\u9700\u8981\u663e\u793a\u53c2\u6570\u63d0\u4f9b, \u800c\u4f7f\u7528\u63cf\u8ff0\u5668\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f20\u6211\u4eec\u5b9a\u4e49\u7684\u53c2\u6570\u4e86 module . _sliced_attention = new_sliced_attention . __get__ ( module , type ( module )) module . _attention = new_attention . __get__ ( module , type ( module )) def use_last_tokens_attention ( use = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . use_last_attn_slice = use def use_last_tokens_attention_weights ( use = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . use_last_attn_weights = use def use_last_self_attention ( use = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn1\" in name : module . use_last_attn_slice = use def save_last_tokens_attention ( save = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . save_last_attn_slice = save def save_last_self_attention ( save = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn1\" in name : module . save_last_attn_slice = save \u8fd9\u91cc\u6211\u4eec\u9700\u8981\u770b\u4e00\u4e0bunet\u4e2d\u7684CrossAttention\u662f\u5982\u4f55\u5b9a\u4e49\u7684, \u6709\u54ea\u4e9b\u65b9\u6cd5, \u4ee5\u53ca\u5177\u4f53\u7684\u8fd0\u7b97\u903b\u8f91\u65f6\u600e\u4e48\u6837\u7684\u3002 # https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py class CrossAttention ( nn . Module ): r \"\"\" A cross attention layer. Parameters: query_dim (:obj:`int`): The number of channels in the query. context_dim (:obj:`int`, *optional*): The number of channels in the context. If not given, defaults to `query_dim`. heads (:obj:`int`, *optional*, defaults to 8): The number of heads to use for multi-head attention. dim_head (:obj:`int`, *optional*, defaults to 64): The number of channels in each head. dropout (:obj:`float`, *optional*, defaults to 0.0): The dropout probability to use. \"\"\" def __init__ ( self , query_dim : int , context_dim : Optional [ int ] = None , heads : int = 8 , dim_head : int = 64 , dropout : int = 0.0 ): super () . __init__ () inner_dim = dim_head * heads context_dim = context_dim if context_dim is not None else query_dim self . scale = dim_head **- 0.5 self . heads = heads # for slice_size > 0 the attention score computation # is split across the batch axis to save memory # You can set slice_size with `set_attention_slice` self . _slice_size = None self . to_q = nn . Linear ( query_dim , inner_dim , bias = False ) self . to_k = nn . Linear ( context_dim , inner_dim , bias = False ) self . to_v = nn . Linear ( context_dim , inner_dim , bias = False ) self . to_out = nn . Sequential ( nn . Linear ( inner_dim , query_dim ), nn . Dropout ( dropout )) def reshape_heads_to_batch_dim ( self , tensor ): batch_size , seq_len , dim = tensor . shape head_size = self . heads tensor = tensor . reshape ( batch_size , seq_len , head_size , dim // head_size ) tensor = tensor . permute ( 0 , 2 , 1 , 3 ) . reshape ( batch_size * head_size , seq_len , dim // head_size ) return tensor def reshape_batch_dim_to_heads ( self , tensor ): batch_size , seq_len , dim = tensor . shape head_size = self . heads tensor = tensor . reshape ( batch_size // head_size , head_size , seq_len , dim ) tensor = tensor . permute ( 0 , 2 , 1 , 3 ) . reshape ( batch_size // head_size , seq_len , dim * head_size ) return tensor def forward ( self , hidden_states , context = None , mask = None ): batch_size , sequence_length , _ = hidden_states . shape query = self . to_q ( hidden_states ) context = context if context is not None else hidden_states key = self . to_k ( context ) value = self . to_v ( context ) dim = query . shape [ - 1 ] query = self . reshape_heads_to_batch_dim ( query ) key = self . reshape_heads_to_batch_dim ( key ) value = self . reshape_heads_to_batch_dim ( value ) # TODO(PVP) - mask is currently never used. Remember to re-implement when used # attention, what we cannot get enough of if self . _slice_size is None or query . shape [ 0 ] // self . _slice_size == 1 : hidden_states = self . _attention ( query , key , value ) else : hidden_states = self . _sliced_attention ( query , key , value , sequence_length , dim ) return self . to_out ( hidden_states ) # \u6b64\u51fd\u6570\u88ab\u65b0\u5199\u7684\u51fd\u6570\u66ff\u6362 def _attention ( self , query , key , value ): # TODO: use baddbmm for better performance attention_scores = torch . matmul ( query , key . transpose ( - 1 , - 2 )) * self . scale attention_probs = attention_scores . softmax ( dim =- 1 ) # compute attention output hidden_states = torch . matmul ( attention_probs , value ) # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states # \u6b64\u51fd\u6570\u88ab\u65b0\u5199\u7684\u51fd\u6570\u66ff\u6362 def _sliced_attention ( self , query , key , value , sequence_length , dim ): batch_size_attention = query . shape [ 0 ] hidden_states = torch . zeros ( ( batch_size_attention , sequence_length , dim // self . heads ), device = query . device , dtype = query . dtype ) slice_size = self . _slice_size if self . _slice_size is not None else hidden_states . shape [ 0 ] for i in range ( hidden_states . shape [ 0 ] // slice_size ): start_idx = i * slice_size end_idx = ( i + 1 ) * slice_size attn_slice = ( torch . matmul ( query [ start_idx : end_idx ], key [ start_idx : end_idx ] . transpose ( 1 , 2 )) * self . scale ) # TODO: use baddbmm for better performance attn_slice = attn_slice . softmax ( dim =- 1 ) attn_slice = torch . matmul ( attn_slice , value [ start_idx : end_idx ]) hidden_states [ start_idx : end_idx ] = attn_slice # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states \u63a5\u4e0b\u6765\u662f\u770b\u6574\u4e2astablediffusion\u7684\u751f\u6210\u8fc7\u7a0b\u662f\u5982\u4f55\u8fd0\u4f5c\u7684\u3002 @torch . no_grad () def stablediffusion ( prompt = \"\" , prompt_edit = None , prompt_edit_token_weights = [], prompt_edit_tokens_start = 0.0 , prompt_edit_tokens_end = 1.0 , prompt_edit_spatial_start = 0.0 , prompt_edit_spatial_end = 1.0 , guidance_scale = 7.5 , steps = 50 , seed = None , width = 512 , height = 512 , init_image = None , init_image_strength = 0.5 ): #Change size to multiple of 64 to prevent size mismatches inside model width = width - width % 64 height = height - height % 64 #If seed is None, randomly select seed from 0 to 2^32-1 if seed is None : seed = random . randrange ( 2 ** 32 - 1 ) generator = torch . cuda . manual_seed ( seed ) #Set inference timesteps to scheduler scheduler = LMSDiscreteScheduler ( beta_start = 0.00085 , beta_end = 0.012 , beta_schedule = \"scaled_linear\" , num_train_timesteps = 1000 ) scheduler . set_timesteps ( steps ) #Preprocess image if it exists (img2img) if init_image is not None : #Resize and transpose for numpy b h w c -> torch b c h w init_image = init_image . resize (( width , height ), resample = Image . Resampling . LANCZOS ) init_image = np . array ( init_image ) . astype ( np . float32 ) / 255.0 * 2.0 - 1.0 init_image = torch . from_numpy ( init_image [ np . newaxis , ... ] . transpose ( 0 , 3 , 1 , 2 )) #If there is alpha channel, composite alpha for white, as the diffusion model does not support alpha channel if init_image . shape [ 1 ] > 3 : init_image = init_image [:, : 3 ] * init_image [:, 3 :] + ( 1 - init_image [:, 3 :]) #Move image to GPU init_image = init_image . to ( device ) #Encode image with autocast ( device ): init_latent = vae . encode ( init_image ) . latent_dist . sample ( generator = generator ) * 0.18215 t_start = steps - int ( steps * init_image_strength ) else : init_latent = torch . zeros (( 1 , unet . in_channels , height // 8 , width // 8 ), device = device ) t_start = 0 #Generate random normal noise noise = torch . randn ( init_latent . shape , generator = generator , device = device ) #latent = noise * scheduler.init_noise_sigma latent = scheduler . add_noise ( init_latent , noise , torch . tensor ([ scheduler . timesteps [ t_start ]], device = device )) . to ( device ) #Process clip with autocast ( device ): tokens_unconditional = clip_tokenizer ( \"\" , padding = \"max_length\" , max_length = clip_tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" , return_overflowing_tokens = True ) embedding_unconditional = clip ( tokens_unconditional . input_ids . to ( device )) . last_hidden_state tokens_conditional = clip_tokenizer ( prompt , padding = \"max_length\" , max_length = clip_tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" , return_overflowing_tokens = True ) embedding_conditional = clip ( tokens_conditional . input_ids . to ( device )) . last_hidden_state #Process prompt editing if prompt_edit is not None : tokens_conditional_edit = clip_tokenizer ( prompt_edit , padding = \"max_length\" , max_length = clip_tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" , return_overflowing_tokens = True ) embedding_conditional_edit = clip ( tokens_conditional_edit . input_ids . to ( device )) . last_hidden_state init_attention_edit ( tokens_conditional , tokens_conditional_edit ) init_attention_func () init_attention_weights ( prompt_edit_token_weights ) timesteps = scheduler . timesteps [ t_start :] for i , t in tqdm ( enumerate ( timesteps ), total = len ( timesteps )): t_index = t_start + i #sigma = scheduler.sigmas[t_index] latent_model_input = latent latent_model_input = scheduler . scale_model_input ( latent_model_input , t ) #Predict the unconditional noise residual # \u8fd9\u91cc\u662f\u4e3a\u4e86\u4f7f\u7528classifier free guidance noise_pred_uncond = unet ( latent_model_input , t , encoder_hidden_states = embedding_unconditional ) . sample #Prepare the Cross-Attention layers if prompt_edit is not None : save_last_tokens_attention () save_last_self_attention () else : #Use weights on non-edited prompt when edit is None use_last_tokens_attention_weights () #Predict the conditional noise residual and save the cross-attention layer activations noise_pred_cond = unet ( latent_model_input , t , encoder_hidden_states = embedding_conditional ) . sample #Edit the Cross-Attention layer activations if prompt_edit is not None : t_scale = t / scheduler . num_train_timesteps if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end : use_last_tokens_attention () if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end : use_last_self_attention () #Use weights on edited prompt use_last_tokens_attention_weights () #Predict the edited conditional noise residual using the cross-attention masks noise_pred_cond = unet ( latent_model_input , t , encoder_hidden_states = embedding_conditional_edit ) . sample #Perform guidance noise_pred = noise_pred_uncond + guidance_scale * ( noise_pred_cond - noise_pred_uncond ) latent = scheduler . step ( noise_pred , t_index , latent ) . prev_sample #scale and decode the image latents with vae # TODO 0.18215 \u8fd9\u4e2a\u6570\u5b57\u662f\u600e\u4e48\u6765\u7684\u5462 latent = latent / 0.18215 image = vae . decode ( latent . to ( vae . dtype )) . sample image = ( image / 2 + 0.5 ) . clamp ( 0 , 1 ) image = image . cpu () . permute ( 0 , 2 , 3 , 1 ) . numpy () image = ( image [ 0 ] * 255 ) . round () . astype ( \"uint8\" ) return Image . fromarray ( image ) \u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898 \u5f04\u6e05last_attn_slice\u662f\u4ec0\u4e48, sliced_attention, attention\u4e24\u4e2a\u51fd\u6570\u7684\u533a\u522b \u5f04\u6e05\u695aVAE\u7684\u539f\u7406, \u5f04\u6e05\u695aattention map\u4e2d\u7684\u4f4d\u7f6e\u662f\u5426\u4f53\u73b0\u51fa\u539f\u56fe\u4e2d\u7684\u7a7a\u57df\u4fe1\u606f \u5f04\u6e05\u695aattention\u6709\u51e0\u5c42, \u6bcf\u4e2a\u5c42\u7684attention\u5c42\u7684\u53d8\u5316\u60c5\u51b5, \u4ee5\u53ca\u54ea\u4e9b\u5c42\u662f\u91cd\u8981\u7684 attention \u7684\u5c42\u6570, \u5206\u5e03 \u8bdd\u4e0d\u591a\u8bf4, \u76f4\u63a5\u5199\u4ee3\u7801\u6765\u89e3\u51b3\u95ee\u9898\u3002 for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" : if \"attn2\" in name : attn2_layers += 1 attn_layers += 1 module . last_attn_slice = None module . use_last_attn_slice = False module . use_last_attn_weights = False module . save_last_attn_slice = False module . _sliced_attention = new_sliced_attention . __get__ ( module , type ( module )) module . _attention = new_attention . __get__ ( module , type ( module )) print ( \"attn layers:\" , attn_layers ) print ( \"atten2 layers:\" , attn2_layers ) \u8f93\u51fa: attn layers: 32 atten2 layers: 16 \u6839\u636e\u540e\u7eed\u7684\u8f93\u51fa, \u53ef\u77e5attn1\u548cattn2\u662f\u4ea4\u66ff\u51fa\u73b0\u7684 \u540c\u65f6\u67e5\u770bquery, key, value\u7684\u7ef4\u5ea6\u53c2\u6570, \u6211\u4eec\u53ef\u4ee5\u6ce8\u610f\u5230\u8fd9\u662f selfAttention \u4ee5\u53ca crossAttention , \u4ea4\u66ff\u4f7f\u7528, \u8fd9\u4e5f\u5370\u8bc1\u4e86\u4ee3\u7801\u4e2d\u9700\u8981\u533a\u5206attn1\u4ee5\u53caattn2, \u8fd9\u4e5f\u662f\u4e3a\u4ec0\u4e48\u4e3b\u8981inject\u7684\u662fattn2\u7684layer, \u800c\u4e0d\u662fattn1, \u5176\u6b21\u53ef\u4ee5\u901a\u8fc7attention map\u7684\u7ef4\u5ea6\u770b\u51fa, \u6a21\u578b\u5448\u73b0\u4e2d\u95f4attention map\u5c0f, \u4e24\u7aef\u7684attention map\u5927\u7684\u7279\u70b9, \u8fd9\u4e5f\u5c31\u662funet\u7684\u67b6\u6784\u3002 attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 1 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 2 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 3 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 4 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 5 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 6 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 7 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 8 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 9 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 10 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 11 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 12 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 64, 160]) key.shape torch.Size([8, 64, 160]) value.shape torch.Size([8, 64, 160]) 31 origin attn map: torch.Size([8, 64, 64]) new attn map: torch.Size([8, 64, 64]) attn2 query.shape torch.Size([8, 64, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 32 origin attn map: torch.Size([8, 64, 77]) new attn map: torch.Size([8, 64, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 13 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 14 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 15 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 16 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 17 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 18 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 19 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 20 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 21 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 22 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 23 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 24 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 25 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 26 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) 2 attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 27 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 28 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 29 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 30 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) sequence matcher\u5230\u5e95\u5728\u505a\u4ec0\u4e48? \u6211\u4eec\u5148\u7b80\u5355print\u4e00\u4e0b\u4e2d\u95f4\u53d8\u91cf, \u770b\u4e00\u4e0b\u7ed3\u679c: def init_attention_edit ( tokens , tokens_edit ): tokens_length = clip_tokenizer . model_max_length mask = torch . zeros ( tokens_length ) indices_target = torch . arange ( tokens_length , dtype = torch . long ) indices = torch . zeros ( tokens_length , dtype = torch . long ) tokens = tokens . input_ids . numpy ()[ 0 ] tokens_edit = tokens_edit . input_ids . numpy ()[ 0 ] if Debug : print ( \"init mask:\" , mask [: Debug_token_len ]) print ( tokens [: Debug_token_len ], tokens_edit [: Debug_token_len ], sep = \" \\n \" ) for name , a0 , a1 , b0 , b1 in SequenceMatcher ( None , tokens , tokens_edit ) . get_opcodes (): if Debug : print ( \"name:\" , name , \" \\n a0:\" , a0 , \"a1:\" , a1 , \" \\n b0:\" , b0 , \"b1:\" , b1 ) if b0 < tokens_length : if name == \"equal\" or ( name == \"replace\" and a1 - a0 == b1 - b0 ): mask [ b0 : b1 ] = 1 indices [ b0 : b1 ] = indices_target [ a0 : a1 ] if Debug : print ( \"final mask:\" , mask [: Debug_token_len ]) print ( \"final indices:\" , indices [: Debug_token_len ]) for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : # \u5bf9\u4e8ecrossAttention\u800c\u8a00, \u9700\u8981mask\u4ee5\u53caindices module . last_attn_slice_mask = mask . to ( device ) module . last_attn_slice_indices = indices . to ( device ) if module_name == \"CrossAttention\" and \"attn1\" in name : module . last_attn_slice_mask = None module . last_attn_slice_indices = None \u770b\u4e00\u4e0boutput: stablediffusion ( \"a cat sitting on a car\" , \"a smiling dog sitting on a car\" , prompt_edit_spatial_start = 0 .7, seed = 248396402679 ) ------- init mask: tensor ([ 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 . ]) [ 49406 320 { 2368 } 4919 525 320 1615 49407 49407 49407 49407 49407 49407 49407 49407 ] [ 49406 320 { 9200 1929 } 4919 525 320 1615 49407 49407 49407 49407 49407 49407 49407 ] name: equal a0: 0 a1: 2 b0: 0 b1: 2 name: replace a0: 2 a1: 3 b0: 2 b1: 4 name: equal a0: 3 a1: 76 b0: 4 b1: 77 name: delete a0: 76 a1: 77 b0: 77 b1: 77 final mask: tensor ([ 1 ., 1 ., 0 ., 0 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 . ]) final indices: tensor ([ 0 , 1 , 0 , 0 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ]) # mask \u4e3a1\u7684\u90e8\u5206, \u4f7f\u7528origin\u7684attention map, mask\u4e3a0\u7684\u90e8\u5206\u4f7f\u7528edit\u7684attention map # \u81f3\u4e8e\u6765\u81eaorigin\u7684attention map\u6765\u81ea\u90a3\u4e2aword_index, \u4ee5\u53ca\u8981\u653e\u5230\u54ea\u4e2aword_index, \u5219\u7531 indices\u8fdb\u884c\u9009\u62e9\u3002 mask\u548cindices\u6709\u4ec0\u4e48\u7528\u5462, \u4e3b\u8981\u7528\u5728\u8fd9\u91cc: \u5728\u6211\u4eec\u4fee\u6539\u53e5\u5b50\u6216\u8005\u66ff\u6362attention map\u65f6, \u6211\u4eec\u671f\u671b\u76f8\u540c\u7684\u5355\u8bcd\u8ba1\u7b97\u51fa\u7684attention map\u662f\u5bf9\u5e94\u7684, \u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u4e25\u683c\u6309\u7167\u5355\u8bcd\u987a\u5e8f\u6765\u5bf9\u5e94attention map, \u90a3\u4e48\"a smiling dog sitting on a car\"\u4e2d\u7684sitting\u5bf9\u5e94\u7684attention map\u4f1a\u88ab\u66ff\u6362\u4e3aa\u5bf9\u5e94\u7684attention map, \u4f46\u662f\u5bf9\u5e94\u7684value\u6ca1\u6709\u6539\u53d8, \u8fd9\u5c31\u5bfc\u81f4\u4e86attention map\u7684\u9519\u4f4d, \u5bfc\u81f4\u6700\u540e\u751f\u6210\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u597d\u3002 \u5982\u679c\u4e0d\u9002\u7528indices, \u800c\u76f4\u63a5\u91c7\u7528\u6309\u5e8f\u66ff\u6362\u6240\u6709attention map\u7684\u65b9\u5f0f, \u540c\u6837\u7684\u6761\u4ef6\u5f97\u5230\u5982\u4e0b\u7684\u56fe\u7247\u3002 \u5404\u4e2a\u53c2\u6570\u63a7\u5236\u539f\u7406 stable ( prompt = \"\" , # \u539f\u59cb\u751f\u6210\u8fc7\u7a0b prompt_edit = None , # \u540e\u7eed\u4fee\u6539\u7684\u8bed\u53e5 prompt_edit_token_weights = [], # \u4e00\u4e2atoken\u4f4d\u7f6e\u4ee5\u53ca\u6743\u91cd\u7ec4\u6210\u7684tuple\u7684\u5217\u8868, \u5728\u4f7f\u7528attention map\u65f6, \u4f1a\u8fdb\u884c\u4e00\u4e2areweight, \u9ed8\u8ba4\u90fd\u4e3a1 prompt_edit_tokens_start = 0.0 , # \u5728\u5904\u4e8e edit_tokens_start \u548cend \u4e4b\u95f4\u7684\u8fed\u4ee3\u4f1a\u5c06crossattention map \u8fdb\u884c\u66ff\u6362 prompt_edit_tokens_end = 1.0 , prompt_edit_spatial_start = 0.0 , # \u5728\u5904\u4e8eedit_spatial_start \u548cend\u4e4b\u95f4\u7684\u8fed\u4ee3\u4f1a\u8fdb\u884cselfattention map\u7684\u66ff\u6362 prompt_edit_spatial_end = 1.0 , guidance_scale = 7.5 , # CFG\u7684\u4e58\u6570 steps = 50 , # \u8fed\u4ee3\u8f6e\u6570 seed = None , # seed, readme\u4e2d\u8bf4seed\u76f8\u540c\u624d\u53ef\u4ee5\u8fdb\u884c\u4fee\u6539? width = 512 , height = 512 , init_image = None , # \u521d\u59cb\u5316\u7684\u56fe\u7247 init_image_strength = 0.5 ) # \u521d\u59cb\u5316\u7684\u5f3a\u5ea6 # \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u4e00\u4e2a\u5730\u65b9\u5728\u4e8e, \u751f\u6210\u63a7\u5236\u7684\u987a\u5e8f\u662f\u4ece1\u52300\u7684\u5c0f\u6570\u70b9, \u63a7\u5236\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u8c8c\u4f3c\u76f8\u53cd\u4e86 if prompt_edit is not None : t_scale = t / scheduler . num_train_timesteps print ( t_scale ) if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end : use_last_tokens_attention () if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end : use_last_self_attention () \"\"\" \u8fd9\u91cc\u5f97\u5230\u7684\u8f93\u51fa\u662f: tensor(0.9990, dtype=torch.float64) tensor(0.9786, dtype=torch.float64) tensor(0.9582, dtype=torch.float64) tensor(0.9378, dtype=torch.float64) tensor(0.9174, dtype=torch.float64) tensor(0.8971, dtype=torch.float64) tensor(0.8767, dtype=torch.float64) tensor(0.8563, dtype=torch.float64) tensor(0.8359, dtype=torch.float64) tensor(0.8155, dtype=torch.float64) tensor(0.7951, dtype=torch.float64) tensor(0.7747, dtype=torch.float64) tensor(0.7543, dtype=torch.float64) tensor(0.7340, dtype=torch.float64) tensor(0.7136, dtype=torch.float64) \"\"\" sliced_attention\u5728\u505a\u4ec0\u4e48? \u8c8c\u4f3c\u771f\u7684\u6ca1\u8c03\u7528.... \u7b97\u6cd5\u662f\u4e0d\u662f\u6709\u95ee\u9898? \u6ce8\u610f\u5230, \u8fd9\u4e2a\u4e0d\u5e26\u661f\u7684z\u8fc7\u7a0b\u4e2d, \u4ece\u59cb\u81f3\u7ec8\u90fd\u5b58\u5728\u4e00\u4e2a\u5b8c\u6210\u7684\u751f\u6210\u94fe, \u4e5f\u5c31\u662f\u6700\u540e\u7684z0, \u5c31\u662f\u539f\u59cb\u751f\u6210\u7684\u56fe\u7247, \u800cz*\u662fedit\u540e\u7684\u56fe\u7247\u3002\u800c\u6211\u4eec\u76ee\u524d\u8dd1\u7684\u4ee3\u7801\u7684\u5b9e\u73b0\u662f, zt \u548c zt*\u5728\u53d6\u4e24\u4e2aMt\u7684\u8fc7\u7a0b\u4e2d\u662f\u5171\u4eab\u7684, \u4e5f\u5c31\u662f\u8bf4, \u8fd9\u91cc\u7684\u7b2c\u516d\u884c\u53d8\u4e3a$$z_{t-1}, M_{t} \\leftarrow DM(z_{t}^{*}, P, t, s)$$, \u800c$$z_{t-1}$$\u662f\u88ab\u5b8c\u6574\u629b\u5f03\u7684\u3002 \u4e3a\u4e86\u8fd9\u4e00\u70b9, \u6211\u4eec\u5fc5\u987b\u518d\u6539\u4ee3\u7801\u6765\u9a8c\u8bc1, \u8fd9\u548ccondition\u7684\u6539\u53d8\u662f\u6709\u5173\u7cfb\u7684, \u5177\u4f53\u7684\u90e8\u5206\u89c1\u201cimage condition\u7684\u4e0d\u540c\u7ec4\u5408\u3002 attention map \u5230\u5e95\u5982\u4f55\u8d77\u4f5c\u7528? \u524d\u63d0\u63d0\u8981 \u6211\u4eec\u6ce8\u610f\u5230, seed\u4e5f\u53ef\u80fd\u662f\u51b3\u5b9a\u56fe\u7247\u5e03\u5c40\u7684\u4e00\u4e2a\u56e0\u7d20\u3002\u6211\u4eec\u4e3e\u4e2a\u4f8b\u5b50: seed case1 seed=248396402679, steps=50 a cat sitting on a car a smiling dog sitting on a car a dog sitting on a car a hamster sitting on a car a tiger sitting on a car a lion sitting on a car \u6211\u4eec\u671f\u671b\u5bfb\u627e\u4e00\u4e2aseed, \u4f7f\u5f97\u4e0d\u540c\u7684prompt\u5f97\u5230\u7684\u56fe\u7247\u5e03\u5c40\u6709\u6bd4\u4ef7\u660e\u663e\u7684\u4e0d\u4e00\u81f4\u6027, \u8fd9\u6837\u624d\u80fd\u66f4\u52a0\u5145\u5206\u4f53\u73b0\u6211\u4eec\u4f7f\u7528attention map\u8fdb\u884c\u7f16\u8f91\u7684\u7528\u5904\u3002 seed case2 seed = 24839640267, steps=50 a cat sitting on a car a smiling dog sitting on a car a dog sitting on a car a hamster sitting on a car a tiger sitting on a car a lion sitting on a car crossattention\u662f\u5426\u6709\u7528? selfattention \u662f\u5426\u6709\u7528? cat-tiger origin:cat, new:tiger , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 , \u8d8a\u4e0a\u9762CrossAttention \u6301\u7eed\u7684\u8d8a\u4e45, \u8d8a\u5de6\u8fb9, selfAttention\u6301\u7eed\u7684\u8d8a\u4e45 \u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c attention map\u5bf9\u4e8e\u7a7a\u57df\u63a7\u5236\u786e\u5b9e\u662f\u6709\u4f5c\u7528\u7684 \uff0c\u800c\u5c06value\u66ff\u6362\u4e3a\u8001\u864e\u7684value\u540c\u65f6\u4e5f\u5bfc\u81f4\u80cc\u540e\u8f66\u53d8\u4e3a\u8f66\uff0c\u8fd9\u4e5f\u6709\u53ef\u80fd\u548cattention map\u7684\u6269\u6563\u6709\u5173\uff0c\u5373\u8001\u864e\u7684attention map\u6743\u91cd\u5927\u7684\u7a7a\u57df\u5e76\u4e0d\u5168\u7a0b\u5728\u8001\u864e\u8eab\u4e0a\u3002 dog-hamster origin:dog, new:hamster , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 \u5de6\u4e0a\u89d2, \u7a7a\u57df\u63a7\u5236\u8fc7\u5f3a, \u5bfc\u81f4\u65e0\u6cd5\u751f\u6210\u8001\u9f20\u7684\u5f62\u6001, \u6ce8\u610f\u5230\u6700\u540e\u4e00\u884c, \u6b64\u65f6crossAttention\u5e76\u65e0inject, \u4ec5\u4ec5selfAttention inject, \u6b64\u65f6\u7a7a\u57df\u9650\u5236\u5c31\u4e0d\u662f\u5f88\u5f3a\u3002\u6709\u6bd4\u8f83\u597d\u7684\u6548\u679c, \u4f46\u662f\u5374\u548c\u6587\u7ae0\u4e2d\u7684selfAttention\u4e0d\u591f\u642d\u8fb9\u4e86, \u6700\u540e\u4e00\u6b65\u7a81\u7136\u8df3\u5230\u539f\u56fe, \u4e5f\u662f\u6709\u4e9b\u532a\u5937\u6240\u601d, \u9700\u8981\u518d\u7ec6\u81f4\u5206\u6790\u3002 dog-hamster-detail left to right ss=0.6-1.0, up to down cs=0.6-1.0 ce=se=1 hamster-dog origin:hamster, new:dog , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 \u8fd9\u91cc\u8fd8\u662f\u4f53\u73b0\u51fa\u4e86\u7a7a\u57df\u63a7\u5236\u5f88\u5f3a\u7684\u6548\u679c, \u5bfc\u81f4\u72d7\u7684\u6bdb\u53d1\u6210\u8272\u90fd\u548c\u8001\u9f20\u6bd4\u8f83\u76f8\u8fd1\u3002\u6ce8\u610f\u5230\u6700\u53f3\u8fb9\u7684\u5217, \u6b64\u65f6\u53ea\u6709crossAttention inject, \u6ca1\u6709selfAttention inject, \u5927\u5e45\u5ea6\u7684inject\u8303\u56f4\u6539\u53d8\u90fd\u6ca1\u6709\u5bf9\u6784\u56fe\u548c\u98ce\u683c\u6709\u660e\u663e\u53d8\u5316, \u8fd9\u4e5f\u503c\u5f97\u601d\u8003\u3002\u6700\u540e\u4e00\u884c\u7684\u884c\u4e3a\u4e5f\u975e\u5e38\u5947\u602a\u3002 hamster-dog-detail 0.6-1.0, \u6ce8\u610f\u5230 \u521d\u59cb\u7684inject\u5bf9\u6784\u56fe\u7684\u53d8\u5316\u7a0b\u5ea6\u8fd8\u662f\u6bd4\u8f83\u660e\u663e \u7684, \u4f8b\u5982\u51fa\u73b0\u4e86\u6234\u773c\u955c\u72d7\u7684\u56fe\u7247\u3002 dog-smiling origin:dog, new:smiling dog , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 dog-smiling-detail left to right: ss0.6 to ss1.0, up to down: cs0.6 to cs1.0 dog-smiling-end \u8fd9\u91cc\u8bf4\u660e\u4e86, \u524d\u671f\u662f\u5b9a\u7ed3\u6784\u4f4d\u7f6e\u7684\u5173\u952e\u671f, \u524d\u671f\u4e00\u5b9a, \u540e\u7eed\u518d\u6539\u6bd4\u8f83\u9ebb\u70e6, \u5f53\u7136, \u8fd9\u4e5f\u662f\u56e0\u4e3a\u8fd9\u91cc\u4ee3\u7801\u5b9e\u73b0\u7684Attention map\u662f\u4f9d\u8d56\u4e8e\u524d\u4e00\u4e2alatent\u7684input\u7684, \u800c\u4e0d\u662f\u72ec\u7acb\u7684\u3002 left to right: ce0.6 to ce1.0, up to down: se0.6 to se1.0, ss=cs=0.3 cake \u5b9e\u9a8c cake ss0.7, cs0.7, se=ce=1 apple cheese chocolate jello lego matcha pistachio pumpkin \u82f9\u679c \u829d\u58eb \u5de7\u514b\u529b \u679c\u51bb \u4e50\u9ad8 \u62b9\u8336 \u5f00\u5fc3\u679c \u5357\u74dc lemon-cheese ss0.0-1.0 cs0.0-1.0 lemon cheese lemon-pistachio ss0.7-1.0, cs0.7-1.0 lemon pistachio value, map\u4e0d\u540c\u7ec4\u5408 \u4ee5\u4e0a\u7684\u77e9\u9635\u5c31\u4f53\u73b0\u51fa\u4e86value, map\u7684\u4e0d\u540c\u7ec4\u5408, \u6709\u4e00\u5b9a\u7684\u8d8b\u52bf\u53ef\u4ee5\u8bc1\u660emap\u63a7\u5236\u4f4d\u7f6e, value\u63a7\u5236\u5185\u5bb9, \u4f46\u8fd9\u8fd8\u9700\u8981image condition\u7684\u9a8c\u8bc1\u3002 image condition\u7684\u4e0d\u540c\u7ec4\u5408(\u5e76\u884cinject) \u8fd9\u91cc\u5176\u5b9e\u5c31\u662f\u6307\u5e76\u884cinject, \u540c\u65f6\u4e5f\u89e3\u91ca\u4e86\u7b97\u6cd5\u662f\u5426\u6709\u95ee\u9898\u7684\u95ee\u9898\u3002 hamster-dog-self left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0, se=ce=1 margin dog-smiling-self dog-smiling-self \u6253\u5370\u6240\u6709\u7684\u751f\u6210\u8fc7\u7a0b\u4ee5\u53caattention \u624b\u52a8inject \u65b9\u6cd51 \u5b9e\u9a8c\u65b9\u6cd5, \u9009\u5b9a\u56fe\u7247\u7684\u4e00\u5757\u65b9\u5f62\u533a\u57df, \u7136\u540e\u5728Attention map\u8fd9\u4e2a\u533a\u57df\u4e2d\u4f7f\u5f97\u8be5\u90e8\u5206\u7684\u6743\u91cd\u589e\u52a0, \u589e\u52a0\u65b9\u5f0f\u8fd8\u6709\u5f85\u5c1d\u8bd5\u3002 \u53eainject 64X64\u7684attention map \u7edd\u5bf9inject\uff0cmast\u76f8\u52a0\u540e\u4e0d\u505asoftmax\uff0c\u76f4\u63a5\u4e58value inject scale = 10 \u8fd9\u91cc\u9996\u5148\u9610\u660e\u4e86\u6211\u4eecinjcet\u7684\u4f4d\u7f6e\u7684\u5927\u81f4\u533a\u57df\uff0c \u4e3a\u6697\u7ea2\u8272\u90e8\u5206\u3002 inject scale=4 inject scale=3 \u503c\u5f97\u63a2\u7d22\u7684\u65b9\u5411 self Attention \u6a21\u578b\u7ed3\u6784 dreambooth \u52a8\u8bcd inpainting() \u518d\u8fc7\u4e00\u6b21\u5f52\u4e00\u5316(\u5c1d\u8bd5\u5404\u79cd\u65b9\u6cd5) \u8c03\u7a33\u5b9a\u4e00\u4e9b \u624b\u52a8inject\u7ed3\u8bba \u76ee\u524d\u53eainject 64X64, inject 32X32 \u6216\u8bb8\u6548\u679c\u4f1a\u66f4\u597d(\u5df2\u7ecf\u9a8c\u8bc1) inject \u7684\u9762\u79ef\u672a\u5fc5\u80fd\u591f\u9650\u5236\u751f\u6210\u52a8\u7269\u7684\u9762\u79ef \u6709\u4e00\u5b9a\u7684\u63a7\u5236\u4f5c\u7528 \u505asoftmax\u5f52\u4e00\u5316\u4f3c\u4e4e\u6ca1\u6709\u6548\u679c, \u53ef\u80fd\u662fsoftmax\u7684\u7ef4\u5ea6 \u7ed3\u8bba map\u51b3\u5b9a\u7a7a\u57df, value\u51b3\u5b9a\u5185\u5bb9\u57fa\u672c\u6210\u7acb \u4ee5\u4e0a\u4e8c\u8005\u5747\u53d7\u5230image condition\u5f71\u54cd \u5e76\u884cinject \u66f4\u5408\u7406 t z4 z3 z2 z1 z0 \u732b ... t* z4* z3* z2* z1* \u72d7 ... z4 z3 z3end z2 z2end \u200b z3* z2* \u95ee\u9898: dog-smiling \u51fa\u73b0\u4e86\u4eba \u51fa\u73b0\u50cf\u7d20\u5316 \u63a7\u5236\u7684\u4e0d\u662f\u5f88\u7406\u60f3 \u6ce8\u610f\u5230\u7684\u95ee\u9898 \u5176\u5b9e\u751f\u6210\u63a7\u5236\u6ca1\u6709\u5f88\u7406\u60f3 \u6211\u4eec\u6ce8\u610f\u5230\u4ee3\u7801\u4e2d\u7ed9\u7684\u6837\u4f8b\u770b\u8d77\u6765\u4e0d\u9519, \u4f46\u662f\u7a0d\u5fae\u6dfb\u52a0\u4e00\u4e9b\u4fee\u6539, \u5c31\u4f1a\u51fa\u73b0\u4e00\u4e9b\u95ee\u9898, \u5f53\u7136\u8fd9\u4efb\u7136\u9700\u8981\u518d\u56de\u5934\u770b\u4e00\u4e0b\u8bba\u6587\u5e76\u8fdb\u884c\u4fee\u6539\u3002 \u4f8b\u5982, \u539f\u6765\u7684\u5b9e\u73b0\u4e2d, \u4f7f\u7528\u7684\u539f\u6761\u4ef6\u751f\u6210 \u201ca cat sitting on a car\u201d, seed=248396402679 , \u5f97\u5230\u5982\u4e0b\u56fe\u7247: attention inject\u7684\u65b9\u5f0f, \u53c2\u6570\u4e3a \"a cat sitting on a car\", \"a smiling dog sitting on a car\", prompt_edit_spatial_start=0.7, seed=248396402679 \u5219\u5982\u4e0b\u56fe: \u5982\u679c\u5c06\u53c2\u6570\u6539\u53d8\u4e3a \"a cat sitting on a car\", \"a dog sitting on a car\", prompt_edit_spatial_start=0.7,seed=248396402679,steps=50 , \u5219\u5f97\u5230\u5982\u4e0b\u56fe\u7247, \u6bd4\u8f83\u6050\u6016: value, map\u4e0d\u540c\u7ec4\u5408, \u629b\u5f03selfattention , new_sliced_attention , seq compare","title":"prompt-to-prompt"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#prompt-to-prompt","text":"[toc]","title":"prompt-to-prompt"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_1","text":"\u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u662f\u805a\u7126\u7814\u7a76\u5728diffusion model\u751f\u6210\u7684\u8fc7\u7a0b\u4e2dAttention map\u662f\u5982\u4f55\u8d77\u4f5c\u7528\u7684, \u53ef\u4ee5\u7528\u4ec0\u4e48\u6837\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u5229\u7528attentionmap\u6765\u63a7\u5236\u6a21\u578b\u7684\u751f\u6210\u3002\u8fd9\u4e2a\u90e8\u5206\u6700\u4e3b\u8981\u7684\u542f\u53d1\u5de5\u4f5c\u6765\u6e90\u4e8egoogle\u7684\u201cPrompt-to-Prompt Image Editing with Cross Attention Control\u201d, \u4e5f\u662f\u6211\u4eec\u5e0c\u671b\u590d\u73b0\u548c\u5b66\u4e60\u7684\u6587\u7ae0\u3002","title":"\u524d\u8a00"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_2","text":"","title":"\u6807\u8bb0\u8bf4\u660e"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#cross-attention-mapprompt_edit_tokens_end-prompt_edit_tokens_start","text":"","title":"Cross Attention map\u7684\u66ff\u6362\u4eceprompt_edit_tokens_end\u5f00\u59cb, \u5230prompt_edit_tokens_start\u7ed3\u675f"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#self-attention-mapprompt_edit_spatial_end-prompt_edit_spatial_start","text":"ss \u662f self Attention inject start \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_spatial_start , \u800c\u9ed8\u8ba4\u7684 prompt_edit_tokens_end=1.0 , \u751f\u6210\u8fc7\u7a0b\u662f1.0 -> 0, \u6240\u4ee5map inject \u4ece\u4e00\u5f00\u59cb\u5c31\u5f00\u59cb, \u5230 prompt_edit_spatial_start \u7ed3\u675f, \u4f8b\u5982\u751f\u6210\u8fc7\u7a0b50\u6b65, prompt_edit_spatial_start=0.8 , \u8fd9\u610f\u5473\u7740\u524d 50*(1-0.8)=10 \u6b65\u662f\u6709map inject, \u540e\u7eed\u5219\u6ca1\u6709\u3002\u5982\u679c prompt_edit_spatial_start=prompt_edit_spatial_end=0 , \u5219\u5168\u79f0\u65e0inject, \u751f\u6210src\u539f\u56fe, \u5373\u56fe\u7247\u77e9\u9635\u6700\u53f3\u4e0b\u89d2\u7684\u56fe\u7247\u3002 cs \u662f cross Attention inject start \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_tokens_start , \u4f5c\u7528\u673a\u5236\u540c\u7406\u3002 ce \u662f corss Attention inject end \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_tokens_end \u3002 se \u662f self Attention inject end \u7684\u7b80\u79f0, \u4e5f\u5373 prompt_edit_spatial_end \u3002","title":"Self Attention map\u7684\u66ff\u6362\u4eceprompt_edit_spatial_end\u5f00\u59cb, prompt_edit_spatial_start\u7ed3\u675f"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_3","text":"\u4e4b\u524d\u5728github\u4e0a\u770b\u5230\u4e86\u6709\u4eba\u5bf9google\u7684\u8fd9\u7bc7\u6587\u7ae0\u8fdb\u884c\u4e86\u590d\u73b0, \u5728\u8fd9\u91cc\u8fdb\u884c\u5c1d\u8bd5, \u590d\u73b0\u4ed6\u7684\u7ed3\u679c, \u7136\u540e\u7814\u7a76\u4e00\u4e0b\u4ed6\u662f\u600e\u4e48\u505a, \u4ee5\u53ca\u6211\u4eec\u53ef\u4ee5\u600e\u4e48\u505a\u5427\u3002\u76f4\u63a5\u4e0a\u4ee3\u7801\u3002 git clone git@github.com:bloc97/CrossAttentionControl.git pip install torch transformers diffusers == 0 .4.1 numpy pillow tqdm jupyter jupyter-notebook # \u5982\u679c\u662fssh\u5728\u670d\u52a1\u5668\u4e0a, \u5219\u9700\u8981\u901a\u8fc7ssh\u4f20\u9012 jupyter-notebook --no-browser --port = 1234 # on server ssh -NL localhost:1234:localhost:1234 g5 # on your pc, then open link in server jupyter, notice port need to be your host port \u8dd1\u662f\u80fd\u8dd1\u7684, \u4e0d\u65ad\u6309shift + enter, \u9664\u4e86\u6a21\u578b\u4e0b\u8f7d\u6162\u4e00\u70b9, \u5176\u4ed6\u90fd\u53ef\u4ee5\u3002","title":"\u590d\u73b0\u548c\u719f\u6089\u8fc7\u7a0b"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_4","text":"\u5f15\u5165\u5305, load\u6a21\u578b import torch from transformers import CLIPModel , CLIPTextModel , CLIPTokenizer from diffusers import AutoencoderKL , UNet2DConditionModel #NOTE: Last tested working diffusers version is diffusers==0.4.1, https://github.com/huggingface/diffusers/releases/tag/v0.4.1 #Init CLIP tokenizer and model model_path_clip = \"openai/clip-vit-large-patch14\" clip_tokenizer = CLIPTokenizer . from_pretrained ( model_path_clip ) clip_model = CLIPModel . from_pretrained ( model_path_clip , torch_dtype = torch . float16 ) clip = clip_model . text_model #Init diffusion model auth_token = \"\u8fd9\u4e2a\u662fhugging face \u7684access token\" #Replace this with huggingface auth token as a string if model is not already downloaded model_path_diffusion = \"CompVis/stable-diffusion-v1-4\" # \u770b\u8d77\u6765stable diffusion\u7528\u7684diffusion model\u5c31\u662f\u8fd9\u4e00\u4e2a, \u4f46\u662f\u5bf9\u4e8e\u6a21\u578b\u53c2\u6570\u800c\u8a00, \u4e0d\u77e5\u9053\u4ed6\u5230\u5e95\u4e0b\u8f7d\u7684\u662f\u53c2\u6570\u7684\u90a3\u4e00\u90e8\u5206, \u800c\u4e14\u7528\u4e86\u534a\u7cbe\u5ea6 # \u4ee3\u7801\u5728\u8fd9\u91cc https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py unet = UNet2DConditionModel . from_pretrained ( model_path_diffusion , subfolder = \"unet\" , use_auth_token = auth_token , revision = \"fp16\" , torch_dtype = torch . float16 ) # \u8fd9\u770b\u8d77\u6765\u5c31\u662fstable diffusion\u7528\u7684vae vae = AutoencoderKL . from_pretrained ( model_path_diffusion , subfolder = \"vae\" , use_auth_token = auth_token , revision = \"fp16\" , torch_dtype = torch . float16 ) #Move to GPU device = \"cuda\" unet . to ( device ) vae . to ( device ) clip . to ( device ) print ( \"Loaded all models\" ) \u5199\u7684\u4e00\u4e9b\u6709\u5173attention\u7684\u51fd\u6570 import numpy as np import random from PIL import Image from diffusers import LMSDiscreteScheduler # https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py # \u8fd9\u4e2a\u7c7b\u7684\u4ee3\u7801\u5728\u8fd9\u91cc, \u4f46\u662f\u5177\u4f53\u7684\u4f5c\u7528\u76ee\u524d\u8fd8\u4e0d\u77e5\u9053 from tqdm.auto import tqdm from torch import autocast # \u6df7\u5408\u7cbe\u5ea6\u7684\u4e1c\u897f from difflib import SequenceMatcher # \u6bd4\u8f83\u4e24\u4e2a\u5e8f\u5217 def init_attention_weights ( weight_tuples ): \"\"\" \u521d\u59cb\u5316\u6743\u91cd, \u5982\u679c\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684\u4f4d\u7f6e\u7684\u6743\u91cd, \u5219\u66ff\u6362, \u5426\u5219\u5168\u4e3a1 \"\"\" tokens_length = clip_tokenizer . model_max_length weights = torch . ones ( tokens_length ) for i , w in weight_tuples : if i < tokens_length and i >= 0 : weights [ i ] = w # TODO \u6743\u91cd\u662f\u4e00\u4e2a\u5e8f\u5217\u957f\u5ea6\u7684\u4e00\u7ef4\u5411\u91cf, \u88ab\u521d\u59cb\u5316\u5230last_attn_slice_weights, \u8fd9\u662f\u4ec0\u4e48\u4e1c\u897f for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . last_attn_slice_weights = weights . to ( device ) if module_name == \"CrossAttention\" and \"attn1\" in name : module . last_attn_slice_weights = None def init_attention_edit ( tokens , tokens_edit ): \"\"\" \"\"\" tokens_length = clip_tokenizer . model_max_length mask = torch . zeros ( tokens_length ) indices_target = torch . arange ( tokens_length , dtype = torch . long ) # 0, 1, 2... indices = torch . zeros ( tokens_length , dtype = torch . long ) # 0, 0, 0 ... tokens = tokens . input_ids . numpy ()[ 0 ] # \u5b57\u5178index? tokens_edit = tokens_edit . input_ids . numpy ()[ 0 ] for name , a0 , a1 , b0 , b1 in SequenceMatcher ( None , tokens , tokens_edit ) . get_opcodes (): if b0 < tokens_length : if name == \"equal\" or ( name == \"replace\" and a1 - a0 == b1 - b0 ): mask [ b0 : b1 ] = 1 indices [ b0 : b1 ] = indices_target [ a0 : a1 ] for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . last_attn_slice_mask = mask . to ( device ) module . last_attn_slice_indices = indices . to ( device ) if module_name == \"CrossAttention\" and \"attn1\" in name : module . last_attn_slice_mask = None module . last_attn_slice_indices = None # TODO \u5f04\u6e05last_attn_slice\u662f\u4ec0\u4e48, sliced_attention, attention\u4e24\u4e2a\u51fd\u6570\u7684\u533a\u522b def init_attention_func (): #ORIGINAL SOURCE CODE: https://github.com/huggingface/diffusers/blob/91ddd2a25b848df0fa1262d4f1cd98c7ccb87750/src/diffusers/models/attention.py#L276 def new_attention ( self , query , key , value ): # TODO: use baddbmm for better performance # query \u548c key \u901a\u8fc7\u77e9\u9635\u4e58, \u518d\u901a\u8fc7softmax\u5f97\u5230attention map attention_scores = torch . matmul ( query , key . transpose ( - 1 , - 2 )) * self . scale attn_slice = attention_scores . softmax ( dim =- 1 ) # compute attention output if self . use_last_attn_slice : if self . last_attn_slice_mask is not None : new_attn_slice = torch . index_select ( self . last_attn_slice , - 1 , self . last_attn_slice_indices ) # \u5728\u5355\u8bcd\u7ef4\u5ea6\u8fdb\u884c\u9009\u62e9 attn_slice = attn_slice * ( 1 - self . last_attn_slice_mask ) + new_attn_slice * self . last_attn_slice_mask else : attn_slice = self . last_attn_slice self . use_last_attn_slice = False # \u8fd9\u4e00\u6b65\u662f\u5728\u8fdb\u884cedit\u4e4b\u524d\u7684\u6267\u884c\u7684\u4e00\u6b21\u4f7f\u7528\u539f\u59cbprompt\u8fdb\u884cforward\uff0c\u7136\u540e\u4fdd\u5b58attn map if self . save_last_attn_slice : self . last_attn_slice = attn_slice self . save_last_attn_slice = False if self . use_last_attn_weights and self . last_attn_slice_weights is not None : attn_slice = attn_slice * self . last_attn_slice_weights self . use_last_attn_weights = False # \u5982\u679c\u6ca1\u6709injection, \u5219\u76f4\u63a5\u518d\u77e9\u9635\u4e58value, \u5c31\u5f97\u5230\u4e86\u4e0b\u4e00\u5c42\u7684\u8f93\u51fa hidden_states = torch . matmul ( attn_slice , value ) # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states def new_sliced_attention ( self , query , key , value , sequence_length , dim ): batch_size_attention = query . shape [ 0 ] hidden_states = torch . zeros ( ( batch_size_attention , sequence_length , dim // self . heads ), device = query . device , dtype = query . dtype ) slice_size = self . _slice_size if self . _slice_size is not None else hidden_states . shape [ 0 ] for i in range ( hidden_states . shape [ 0 ] // slice_size ): start_idx = i * slice_size end_idx = ( i + 1 ) * slice_size attn_slice = ( torch . matmul ( query [ start_idx : end_idx ], key [ start_idx : end_idx ] . transpose ( 1 , 2 )) * self . scale ) # TODO: use baddbmm for better performance attn_slice = attn_slice . softmax ( dim =- 1 ) if self . use_last_attn_slice : if self . last_attn_slice_mask is not None : new_attn_slice = torch . index_select ( self . last_attn_slice , - 1 , self . last_attn_slice_indices ) attn_slice = attn_slice * ( 1 - self . last_attn_slice_mask ) + new_attn_slice * self . last_attn_slice_mask else : attn_slice = self . last_attn_slice self . use_last_attn_slice = False if self . save_last_attn_slice : self . last_attn_slice = attn_slice self . save_last_attn_slice = False if self . use_last_attn_weights and self . last_attn_slice_weights is not None : attn_slice = attn_slice * self . last_attn_slice_weights self . use_last_attn_weights = False attn_slice = torch . matmul ( attn_slice , value [ start_idx : end_idx ]) hidden_states [ start_idx : end_idx ] = attn_slice # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" : module . last_attn_slice = None module . use_last_attn_slice = False module . use_last_attn_weights = False module . save_last_attn_slice = False # \u4ee5\u4e0b\u662f\u4e00\u4e2a\u63cf\u8ff0\u5668\u529f\u80fd, \u6b64\u540e\u8c03\u7528module._sliced_attention, \u7b49\u4ef7\u4e8e\u8c03\u7528new_sliced_attention # \u4f46\u662f\u4e3a\u4ec0\u4e48\u4e0d\u80fd\u76f4\u63a5\u628a\u51fd\u6570\u5730\u5740\u4f20\u8fdb\u53bb\u5462, \u4e5f\u5c31\u662fmodule._sliced_attention = new_sliced_attention # \u8fd9\u662f\u56e0\u4e3a\u51fd\u6570\u4e2d\u6709\u53c2\u6570self,\u5982\u679c\u6309\u5730\u5740\u4f20, self\u4e5f\u9700\u8981\u663e\u793a\u53c2\u6570\u63d0\u4f9b, \u800c\u4f7f\u7528\u63cf\u8ff0\u5668\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f20\u6211\u4eec\u5b9a\u4e49\u7684\u53c2\u6570\u4e86 module . _sliced_attention = new_sliced_attention . __get__ ( module , type ( module )) module . _attention = new_attention . __get__ ( module , type ( module )) def use_last_tokens_attention ( use = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . use_last_attn_slice = use def use_last_tokens_attention_weights ( use = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . use_last_attn_weights = use def use_last_self_attention ( use = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn1\" in name : module . use_last_attn_slice = use def save_last_tokens_attention ( save = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : module . save_last_attn_slice = save def save_last_self_attention ( save = True ): for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn1\" in name : module . save_last_attn_slice = save \u8fd9\u91cc\u6211\u4eec\u9700\u8981\u770b\u4e00\u4e0bunet\u4e2d\u7684CrossAttention\u662f\u5982\u4f55\u5b9a\u4e49\u7684, \u6709\u54ea\u4e9b\u65b9\u6cd5, \u4ee5\u53ca\u5177\u4f53\u7684\u8fd0\u7b97\u903b\u8f91\u65f6\u600e\u4e48\u6837\u7684\u3002 # https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py class CrossAttention ( nn . Module ): r \"\"\" A cross attention layer. Parameters: query_dim (:obj:`int`): The number of channels in the query. context_dim (:obj:`int`, *optional*): The number of channels in the context. If not given, defaults to `query_dim`. heads (:obj:`int`, *optional*, defaults to 8): The number of heads to use for multi-head attention. dim_head (:obj:`int`, *optional*, defaults to 64): The number of channels in each head. dropout (:obj:`float`, *optional*, defaults to 0.0): The dropout probability to use. \"\"\" def __init__ ( self , query_dim : int , context_dim : Optional [ int ] = None , heads : int = 8 , dim_head : int = 64 , dropout : int = 0.0 ): super () . __init__ () inner_dim = dim_head * heads context_dim = context_dim if context_dim is not None else query_dim self . scale = dim_head **- 0.5 self . heads = heads # for slice_size > 0 the attention score computation # is split across the batch axis to save memory # You can set slice_size with `set_attention_slice` self . _slice_size = None self . to_q = nn . Linear ( query_dim , inner_dim , bias = False ) self . to_k = nn . Linear ( context_dim , inner_dim , bias = False ) self . to_v = nn . Linear ( context_dim , inner_dim , bias = False ) self . to_out = nn . Sequential ( nn . Linear ( inner_dim , query_dim ), nn . Dropout ( dropout )) def reshape_heads_to_batch_dim ( self , tensor ): batch_size , seq_len , dim = tensor . shape head_size = self . heads tensor = tensor . reshape ( batch_size , seq_len , head_size , dim // head_size ) tensor = tensor . permute ( 0 , 2 , 1 , 3 ) . reshape ( batch_size * head_size , seq_len , dim // head_size ) return tensor def reshape_batch_dim_to_heads ( self , tensor ): batch_size , seq_len , dim = tensor . shape head_size = self . heads tensor = tensor . reshape ( batch_size // head_size , head_size , seq_len , dim ) tensor = tensor . permute ( 0 , 2 , 1 , 3 ) . reshape ( batch_size // head_size , seq_len , dim * head_size ) return tensor def forward ( self , hidden_states , context = None , mask = None ): batch_size , sequence_length , _ = hidden_states . shape query = self . to_q ( hidden_states ) context = context if context is not None else hidden_states key = self . to_k ( context ) value = self . to_v ( context ) dim = query . shape [ - 1 ] query = self . reshape_heads_to_batch_dim ( query ) key = self . reshape_heads_to_batch_dim ( key ) value = self . reshape_heads_to_batch_dim ( value ) # TODO(PVP) - mask is currently never used. Remember to re-implement when used # attention, what we cannot get enough of if self . _slice_size is None or query . shape [ 0 ] // self . _slice_size == 1 : hidden_states = self . _attention ( query , key , value ) else : hidden_states = self . _sliced_attention ( query , key , value , sequence_length , dim ) return self . to_out ( hidden_states ) # \u6b64\u51fd\u6570\u88ab\u65b0\u5199\u7684\u51fd\u6570\u66ff\u6362 def _attention ( self , query , key , value ): # TODO: use baddbmm for better performance attention_scores = torch . matmul ( query , key . transpose ( - 1 , - 2 )) * self . scale attention_probs = attention_scores . softmax ( dim =- 1 ) # compute attention output hidden_states = torch . matmul ( attention_probs , value ) # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states # \u6b64\u51fd\u6570\u88ab\u65b0\u5199\u7684\u51fd\u6570\u66ff\u6362 def _sliced_attention ( self , query , key , value , sequence_length , dim ): batch_size_attention = query . shape [ 0 ] hidden_states = torch . zeros ( ( batch_size_attention , sequence_length , dim // self . heads ), device = query . device , dtype = query . dtype ) slice_size = self . _slice_size if self . _slice_size is not None else hidden_states . shape [ 0 ] for i in range ( hidden_states . shape [ 0 ] // slice_size ): start_idx = i * slice_size end_idx = ( i + 1 ) * slice_size attn_slice = ( torch . matmul ( query [ start_idx : end_idx ], key [ start_idx : end_idx ] . transpose ( 1 , 2 )) * self . scale ) # TODO: use baddbmm for better performance attn_slice = attn_slice . softmax ( dim =- 1 ) attn_slice = torch . matmul ( attn_slice , value [ start_idx : end_idx ]) hidden_states [ start_idx : end_idx ] = attn_slice # reshape hidden_states hidden_states = self . reshape_batch_dim_to_heads ( hidden_states ) return hidden_states \u63a5\u4e0b\u6765\u662f\u770b\u6574\u4e2astablediffusion\u7684\u751f\u6210\u8fc7\u7a0b\u662f\u5982\u4f55\u8fd0\u4f5c\u7684\u3002 @torch . no_grad () def stablediffusion ( prompt = \"\" , prompt_edit = None , prompt_edit_token_weights = [], prompt_edit_tokens_start = 0.0 , prompt_edit_tokens_end = 1.0 , prompt_edit_spatial_start = 0.0 , prompt_edit_spatial_end = 1.0 , guidance_scale = 7.5 , steps = 50 , seed = None , width = 512 , height = 512 , init_image = None , init_image_strength = 0.5 ): #Change size to multiple of 64 to prevent size mismatches inside model width = width - width % 64 height = height - height % 64 #If seed is None, randomly select seed from 0 to 2^32-1 if seed is None : seed = random . randrange ( 2 ** 32 - 1 ) generator = torch . cuda . manual_seed ( seed ) #Set inference timesteps to scheduler scheduler = LMSDiscreteScheduler ( beta_start = 0.00085 , beta_end = 0.012 , beta_schedule = \"scaled_linear\" , num_train_timesteps = 1000 ) scheduler . set_timesteps ( steps ) #Preprocess image if it exists (img2img) if init_image is not None : #Resize and transpose for numpy b h w c -> torch b c h w init_image = init_image . resize (( width , height ), resample = Image . Resampling . LANCZOS ) init_image = np . array ( init_image ) . astype ( np . float32 ) / 255.0 * 2.0 - 1.0 init_image = torch . from_numpy ( init_image [ np . newaxis , ... ] . transpose ( 0 , 3 , 1 , 2 )) #If there is alpha channel, composite alpha for white, as the diffusion model does not support alpha channel if init_image . shape [ 1 ] > 3 : init_image = init_image [:, : 3 ] * init_image [:, 3 :] + ( 1 - init_image [:, 3 :]) #Move image to GPU init_image = init_image . to ( device ) #Encode image with autocast ( device ): init_latent = vae . encode ( init_image ) . latent_dist . sample ( generator = generator ) * 0.18215 t_start = steps - int ( steps * init_image_strength ) else : init_latent = torch . zeros (( 1 , unet . in_channels , height // 8 , width // 8 ), device = device ) t_start = 0 #Generate random normal noise noise = torch . randn ( init_latent . shape , generator = generator , device = device ) #latent = noise * scheduler.init_noise_sigma latent = scheduler . add_noise ( init_latent , noise , torch . tensor ([ scheduler . timesteps [ t_start ]], device = device )) . to ( device ) #Process clip with autocast ( device ): tokens_unconditional = clip_tokenizer ( \"\" , padding = \"max_length\" , max_length = clip_tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" , return_overflowing_tokens = True ) embedding_unconditional = clip ( tokens_unconditional . input_ids . to ( device )) . last_hidden_state tokens_conditional = clip_tokenizer ( prompt , padding = \"max_length\" , max_length = clip_tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" , return_overflowing_tokens = True ) embedding_conditional = clip ( tokens_conditional . input_ids . to ( device )) . last_hidden_state #Process prompt editing if prompt_edit is not None : tokens_conditional_edit = clip_tokenizer ( prompt_edit , padding = \"max_length\" , max_length = clip_tokenizer . model_max_length , truncation = True , return_tensors = \"pt\" , return_overflowing_tokens = True ) embedding_conditional_edit = clip ( tokens_conditional_edit . input_ids . to ( device )) . last_hidden_state init_attention_edit ( tokens_conditional , tokens_conditional_edit ) init_attention_func () init_attention_weights ( prompt_edit_token_weights ) timesteps = scheduler . timesteps [ t_start :] for i , t in tqdm ( enumerate ( timesteps ), total = len ( timesteps )): t_index = t_start + i #sigma = scheduler.sigmas[t_index] latent_model_input = latent latent_model_input = scheduler . scale_model_input ( latent_model_input , t ) #Predict the unconditional noise residual # \u8fd9\u91cc\u662f\u4e3a\u4e86\u4f7f\u7528classifier free guidance noise_pred_uncond = unet ( latent_model_input , t , encoder_hidden_states = embedding_unconditional ) . sample #Prepare the Cross-Attention layers if prompt_edit is not None : save_last_tokens_attention () save_last_self_attention () else : #Use weights on non-edited prompt when edit is None use_last_tokens_attention_weights () #Predict the conditional noise residual and save the cross-attention layer activations noise_pred_cond = unet ( latent_model_input , t , encoder_hidden_states = embedding_conditional ) . sample #Edit the Cross-Attention layer activations if prompt_edit is not None : t_scale = t / scheduler . num_train_timesteps if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end : use_last_tokens_attention () if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end : use_last_self_attention () #Use weights on edited prompt use_last_tokens_attention_weights () #Predict the edited conditional noise residual using the cross-attention masks noise_pred_cond = unet ( latent_model_input , t , encoder_hidden_states = embedding_conditional_edit ) . sample #Perform guidance noise_pred = noise_pred_uncond + guidance_scale * ( noise_pred_cond - noise_pred_uncond ) latent = scheduler . step ( noise_pred , t_index , latent ) . prev_sample #scale and decode the image latents with vae # TODO 0.18215 \u8fd9\u4e2a\u6570\u5b57\u662f\u600e\u4e48\u6765\u7684\u5462 latent = latent / 0.18215 image = vae . decode ( latent . to ( vae . dtype )) . sample image = ( image / 2 + 0.5 ) . clamp ( 0 , 1 ) image = image . cpu () . permute ( 0 , 2 , 3 , 1 ) . numpy () image = ( image [ 0 ] * 255 ) . round () . astype ( \"uint8\" ) return Image . fromarray ( image )","title":"\u7c97\u8bfb\u4ee3\u7801"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_5","text":"\u5f04\u6e05last_attn_slice\u662f\u4ec0\u4e48, sliced_attention, attention\u4e24\u4e2a\u51fd\u6570\u7684\u533a\u522b \u5f04\u6e05\u695aVAE\u7684\u539f\u7406, \u5f04\u6e05\u695aattention map\u4e2d\u7684\u4f4d\u7f6e\u662f\u5426\u4f53\u73b0\u51fa\u539f\u56fe\u4e2d\u7684\u7a7a\u57df\u4fe1\u606f \u5f04\u6e05\u695aattention\u6709\u51e0\u5c42, \u6bcf\u4e2a\u5c42\u7684attention\u5c42\u7684\u53d8\u5316\u60c5\u51b5, \u4ee5\u53ca\u54ea\u4e9b\u5c42\u662f\u91cd\u8981\u7684","title":"\u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#attention","text":"\u8bdd\u4e0d\u591a\u8bf4, \u76f4\u63a5\u5199\u4ee3\u7801\u6765\u89e3\u51b3\u95ee\u9898\u3002 for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" : if \"attn2\" in name : attn2_layers += 1 attn_layers += 1 module . last_attn_slice = None module . use_last_attn_slice = False module . use_last_attn_weights = False module . save_last_attn_slice = False module . _sliced_attention = new_sliced_attention . __get__ ( module , type ( module )) module . _attention = new_attention . __get__ ( module , type ( module )) print ( \"attn layers:\" , attn_layers ) print ( \"atten2 layers:\" , attn2_layers ) \u8f93\u51fa: attn layers: 32 atten2 layers: 16 \u6839\u636e\u540e\u7eed\u7684\u8f93\u51fa, \u53ef\u77e5attn1\u548cattn2\u662f\u4ea4\u66ff\u51fa\u73b0\u7684 \u540c\u65f6\u67e5\u770bquery, key, value\u7684\u7ef4\u5ea6\u53c2\u6570, \u6211\u4eec\u53ef\u4ee5\u6ce8\u610f\u5230\u8fd9\u662f selfAttention \u4ee5\u53ca crossAttention , \u4ea4\u66ff\u4f7f\u7528, \u8fd9\u4e5f\u5370\u8bc1\u4e86\u4ee3\u7801\u4e2d\u9700\u8981\u533a\u5206attn1\u4ee5\u53caattn2, \u8fd9\u4e5f\u662f\u4e3a\u4ec0\u4e48\u4e3b\u8981inject\u7684\u662fattn2\u7684layer, \u800c\u4e0d\u662fattn1, \u5176\u6b21\u53ef\u4ee5\u901a\u8fc7attention map\u7684\u7ef4\u5ea6\u770b\u51fa, \u6a21\u578b\u5448\u73b0\u4e2d\u95f4attention map\u5c0f, \u4e24\u7aef\u7684attention map\u5927\u7684\u7279\u70b9, \u8fd9\u4e5f\u5c31\u662funet\u7684\u67b6\u6784\u3002 attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 1 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 2 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 3 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 4 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 5 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 6 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 7 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 8 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 9 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 10 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 11 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 12 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 64, 160]) key.shape torch.Size([8, 64, 160]) value.shape torch.Size([8, 64, 160]) 31 origin attn map: torch.Size([8, 64, 64]) new attn map: torch.Size([8, 64, 64]) attn2 query.shape torch.Size([8, 64, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 32 origin attn map: torch.Size([8, 64, 77]) new attn map: torch.Size([8, 64, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 13 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 14 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 15 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 16 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 256, 160]) value.shape torch.Size([8, 256, 160]) 17 origin attn map: torch.Size([8, 256, 256]) new attn map: torch.Size([8, 256, 256]) attn2 query.shape torch.Size([8, 256, 160]) key.shape torch.Size([8, 77, 160]) value.shape torch.Size([8, 77, 160]) 18 origin attn map: torch.Size([8, 256, 77]) new attn map: torch.Size([8, 256, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 19 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 20 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 21 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 22 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 1024, 80]) value.shape torch.Size([8, 1024, 80]) 23 origin attn map: torch.Size([8, 1024, 1024]) new attn map: torch.Size([8, 1024, 1024]) attn2 query.shape torch.Size([8, 1024, 80]) key.shape torch.Size([8, 77, 80]) value.shape torch.Size([8, 77, 80]) 24 origin attn map: torch.Size([8, 1024, 77]) new attn map: torch.Size([8, 1024, 77]) attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 25 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 26 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) 2 attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 27 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 28 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77]) attn1 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 4096, 40]) value.shape torch.Size([8, 4096, 40]) 29 origin attn map: torch.Size([8, 4096, 4096]) new attn map: torch.Size([8, 4096, 4096]) attn2 query.shape torch.Size([8, 4096, 40]) key.shape torch.Size([8, 77, 40]) value.shape torch.Size([8, 77, 40]) 30 origin attn map: torch.Size([8, 4096, 77]) new attn map: torch.Size([8, 4096, 77])","title":"attention \u7684\u5c42\u6570, \u5206\u5e03"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#sequence-matcher","text":"\u6211\u4eec\u5148\u7b80\u5355print\u4e00\u4e0b\u4e2d\u95f4\u53d8\u91cf, \u770b\u4e00\u4e0b\u7ed3\u679c: def init_attention_edit ( tokens , tokens_edit ): tokens_length = clip_tokenizer . model_max_length mask = torch . zeros ( tokens_length ) indices_target = torch . arange ( tokens_length , dtype = torch . long ) indices = torch . zeros ( tokens_length , dtype = torch . long ) tokens = tokens . input_ids . numpy ()[ 0 ] tokens_edit = tokens_edit . input_ids . numpy ()[ 0 ] if Debug : print ( \"init mask:\" , mask [: Debug_token_len ]) print ( tokens [: Debug_token_len ], tokens_edit [: Debug_token_len ], sep = \" \\n \" ) for name , a0 , a1 , b0 , b1 in SequenceMatcher ( None , tokens , tokens_edit ) . get_opcodes (): if Debug : print ( \"name:\" , name , \" \\n a0:\" , a0 , \"a1:\" , a1 , \" \\n b0:\" , b0 , \"b1:\" , b1 ) if b0 < tokens_length : if name == \"equal\" or ( name == \"replace\" and a1 - a0 == b1 - b0 ): mask [ b0 : b1 ] = 1 indices [ b0 : b1 ] = indices_target [ a0 : a1 ] if Debug : print ( \"final mask:\" , mask [: Debug_token_len ]) print ( \"final indices:\" , indices [: Debug_token_len ]) for name , module in unet . named_modules (): module_name = type ( module ) . __name__ if module_name == \"CrossAttention\" and \"attn2\" in name : # \u5bf9\u4e8ecrossAttention\u800c\u8a00, \u9700\u8981mask\u4ee5\u53caindices module . last_attn_slice_mask = mask . to ( device ) module . last_attn_slice_indices = indices . to ( device ) if module_name == \"CrossAttention\" and \"attn1\" in name : module . last_attn_slice_mask = None module . last_attn_slice_indices = None \u770b\u4e00\u4e0boutput: stablediffusion ( \"a cat sitting on a car\" , \"a smiling dog sitting on a car\" , prompt_edit_spatial_start = 0 .7, seed = 248396402679 ) ------- init mask: tensor ([ 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 . ]) [ 49406 320 { 2368 } 4919 525 320 1615 49407 49407 49407 49407 49407 49407 49407 49407 ] [ 49406 320 { 9200 1929 } 4919 525 320 1615 49407 49407 49407 49407 49407 49407 49407 ] name: equal a0: 0 a1: 2 b0: 0 b1: 2 name: replace a0: 2 a1: 3 b0: 2 b1: 4 name: equal a0: 3 a1: 76 b0: 4 b1: 77 name: delete a0: 76 a1: 77 b0: 77 b1: 77 final mask: tensor ([ 1 ., 1 ., 0 ., 0 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 ., 1 . ]) final indices: tensor ([ 0 , 1 , 0 , 0 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ]) # mask \u4e3a1\u7684\u90e8\u5206, \u4f7f\u7528origin\u7684attention map, mask\u4e3a0\u7684\u90e8\u5206\u4f7f\u7528edit\u7684attention map # \u81f3\u4e8e\u6765\u81eaorigin\u7684attention map\u6765\u81ea\u90a3\u4e2aword_index, \u4ee5\u53ca\u8981\u653e\u5230\u54ea\u4e2aword_index, \u5219\u7531 indices\u8fdb\u884c\u9009\u62e9\u3002 mask\u548cindices\u6709\u4ec0\u4e48\u7528\u5462, \u4e3b\u8981\u7528\u5728\u8fd9\u91cc: \u5728\u6211\u4eec\u4fee\u6539\u53e5\u5b50\u6216\u8005\u66ff\u6362attention map\u65f6, \u6211\u4eec\u671f\u671b\u76f8\u540c\u7684\u5355\u8bcd\u8ba1\u7b97\u51fa\u7684attention map\u662f\u5bf9\u5e94\u7684, \u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u4e25\u683c\u6309\u7167\u5355\u8bcd\u987a\u5e8f\u6765\u5bf9\u5e94attention map, \u90a3\u4e48\"a smiling dog sitting on a car\"\u4e2d\u7684sitting\u5bf9\u5e94\u7684attention map\u4f1a\u88ab\u66ff\u6362\u4e3aa\u5bf9\u5e94\u7684attention map, \u4f46\u662f\u5bf9\u5e94\u7684value\u6ca1\u6709\u6539\u53d8, \u8fd9\u5c31\u5bfc\u81f4\u4e86attention map\u7684\u9519\u4f4d, \u5bfc\u81f4\u6700\u540e\u751f\u6210\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u597d\u3002 \u5982\u679c\u4e0d\u9002\u7528indices, \u800c\u76f4\u63a5\u91c7\u7528\u6309\u5e8f\u66ff\u6362\u6240\u6709attention map\u7684\u65b9\u5f0f, \u540c\u6837\u7684\u6761\u4ef6\u5f97\u5230\u5982\u4e0b\u7684\u56fe\u7247\u3002","title":"sequence matcher\u5230\u5e95\u5728\u505a\u4ec0\u4e48?"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_6","text":"stable ( prompt = \"\" , # \u539f\u59cb\u751f\u6210\u8fc7\u7a0b prompt_edit = None , # \u540e\u7eed\u4fee\u6539\u7684\u8bed\u53e5 prompt_edit_token_weights = [], # \u4e00\u4e2atoken\u4f4d\u7f6e\u4ee5\u53ca\u6743\u91cd\u7ec4\u6210\u7684tuple\u7684\u5217\u8868, \u5728\u4f7f\u7528attention map\u65f6, \u4f1a\u8fdb\u884c\u4e00\u4e2areweight, \u9ed8\u8ba4\u90fd\u4e3a1 prompt_edit_tokens_start = 0.0 , # \u5728\u5904\u4e8e edit_tokens_start \u548cend \u4e4b\u95f4\u7684\u8fed\u4ee3\u4f1a\u5c06crossattention map \u8fdb\u884c\u66ff\u6362 prompt_edit_tokens_end = 1.0 , prompt_edit_spatial_start = 0.0 , # \u5728\u5904\u4e8eedit_spatial_start \u548cend\u4e4b\u95f4\u7684\u8fed\u4ee3\u4f1a\u8fdb\u884cselfattention map\u7684\u66ff\u6362 prompt_edit_spatial_end = 1.0 , guidance_scale = 7.5 , # CFG\u7684\u4e58\u6570 steps = 50 , # \u8fed\u4ee3\u8f6e\u6570 seed = None , # seed, readme\u4e2d\u8bf4seed\u76f8\u540c\u624d\u53ef\u4ee5\u8fdb\u884c\u4fee\u6539? width = 512 , height = 512 , init_image = None , # \u521d\u59cb\u5316\u7684\u56fe\u7247 init_image_strength = 0.5 ) # \u521d\u59cb\u5316\u7684\u5f3a\u5ea6 # \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u4e00\u4e2a\u5730\u65b9\u5728\u4e8e, \u751f\u6210\u63a7\u5236\u7684\u987a\u5e8f\u662f\u4ece1\u52300\u7684\u5c0f\u6570\u70b9, \u63a7\u5236\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u8c8c\u4f3c\u76f8\u53cd\u4e86 if prompt_edit is not None : t_scale = t / scheduler . num_train_timesteps print ( t_scale ) if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end : use_last_tokens_attention () if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end : use_last_self_attention () \"\"\" \u8fd9\u91cc\u5f97\u5230\u7684\u8f93\u51fa\u662f: tensor(0.9990, dtype=torch.float64) tensor(0.9786, dtype=torch.float64) tensor(0.9582, dtype=torch.float64) tensor(0.9378, dtype=torch.float64) tensor(0.9174, dtype=torch.float64) tensor(0.8971, dtype=torch.float64) tensor(0.8767, dtype=torch.float64) tensor(0.8563, dtype=torch.float64) tensor(0.8359, dtype=torch.float64) tensor(0.8155, dtype=torch.float64) tensor(0.7951, dtype=torch.float64) tensor(0.7747, dtype=torch.float64) tensor(0.7543, dtype=torch.float64) tensor(0.7340, dtype=torch.float64) tensor(0.7136, dtype=torch.float64) \"\"\"","title":"\u5404\u4e2a\u53c2\u6570\u63a7\u5236\u539f\u7406"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#sliced_attention","text":"\u8c8c\u4f3c\u771f\u7684\u6ca1\u8c03\u7528....","title":"sliced_attention\u5728\u505a\u4ec0\u4e48?"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_7","text":"\u6ce8\u610f\u5230, \u8fd9\u4e2a\u4e0d\u5e26\u661f\u7684z\u8fc7\u7a0b\u4e2d, \u4ece\u59cb\u81f3\u7ec8\u90fd\u5b58\u5728\u4e00\u4e2a\u5b8c\u6210\u7684\u751f\u6210\u94fe, \u4e5f\u5c31\u662f\u6700\u540e\u7684z0, \u5c31\u662f\u539f\u59cb\u751f\u6210\u7684\u56fe\u7247, \u800cz*\u662fedit\u540e\u7684\u56fe\u7247\u3002\u800c\u6211\u4eec\u76ee\u524d\u8dd1\u7684\u4ee3\u7801\u7684\u5b9e\u73b0\u662f, zt \u548c zt*\u5728\u53d6\u4e24\u4e2aMt\u7684\u8fc7\u7a0b\u4e2d\u662f\u5171\u4eab\u7684, \u4e5f\u5c31\u662f\u8bf4, \u8fd9\u91cc\u7684\u7b2c\u516d\u884c\u53d8\u4e3a$$z_{t-1}, M_{t} \\leftarrow DM(z_{t}^{*}, P, t, s)$$, \u800c$$z_{t-1}$$\u662f\u88ab\u5b8c\u6574\u629b\u5f03\u7684\u3002 \u4e3a\u4e86\u8fd9\u4e00\u70b9, \u6211\u4eec\u5fc5\u987b\u518d\u6539\u4ee3\u7801\u6765\u9a8c\u8bc1, \u8fd9\u548ccondition\u7684\u6539\u53d8\u662f\u6709\u5173\u7cfb\u7684, \u5177\u4f53\u7684\u90e8\u5206\u89c1\u201cimage condition\u7684\u4e0d\u540c\u7ec4\u5408\u3002","title":"\u7b97\u6cd5\u662f\u4e0d\u662f\u6709\u95ee\u9898?"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#attention-map","text":"","title":"attention map \u5230\u5e95\u5982\u4f55\u8d77\u4f5c\u7528?"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_8","text":"\u6211\u4eec\u6ce8\u610f\u5230, seed\u4e5f\u53ef\u80fd\u662f\u51b3\u5b9a\u56fe\u7247\u5e03\u5c40\u7684\u4e00\u4e2a\u56e0\u7d20\u3002\u6211\u4eec\u4e3e\u4e2a\u4f8b\u5b50:","title":"\u524d\u63d0\u63d0\u8981"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#seed-case1","text":"seed=248396402679, steps=50 a cat sitting on a car a smiling dog sitting on a car a dog sitting on a car a hamster sitting on a car a tiger sitting on a car a lion sitting on a car \u6211\u4eec\u671f\u671b\u5bfb\u627e\u4e00\u4e2aseed, \u4f7f\u5f97\u4e0d\u540c\u7684prompt\u5f97\u5230\u7684\u56fe\u7247\u5e03\u5c40\u6709\u6bd4\u4ef7\u660e\u663e\u7684\u4e0d\u4e00\u81f4\u6027, \u8fd9\u6837\u624d\u80fd\u66f4\u52a0\u5145\u5206\u4f53\u73b0\u6211\u4eec\u4f7f\u7528attention map\u8fdb\u884c\u7f16\u8f91\u7684\u7528\u5904\u3002","title":"seed case1"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#seed-case2","text":"seed = 24839640267, steps=50 a cat sitting on a car a smiling dog sitting on a car a dog sitting on a car a hamster sitting on a car a tiger sitting on a car a lion sitting on a car","title":"seed case2"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#crossattention-selfattention","text":"","title":"crossattention\u662f\u5426\u6709\u7528? selfattention \u662f\u5426\u6709\u7528?"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#cat-tiger","text":"origin:cat, new:tiger , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 , \u8d8a\u4e0a\u9762CrossAttention \u6301\u7eed\u7684\u8d8a\u4e45, \u8d8a\u5de6\u8fb9, selfAttention\u6301\u7eed\u7684\u8d8a\u4e45 \u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c attention map\u5bf9\u4e8e\u7a7a\u57df\u63a7\u5236\u786e\u5b9e\u662f\u6709\u4f5c\u7528\u7684 \uff0c\u800c\u5c06value\u66ff\u6362\u4e3a\u8001\u864e\u7684value\u540c\u65f6\u4e5f\u5bfc\u81f4\u80cc\u540e\u8f66\u53d8\u4e3a\u8f66\uff0c\u8fd9\u4e5f\u6709\u53ef\u80fd\u548cattention map\u7684\u6269\u6563\u6709\u5173\uff0c\u5373\u8001\u864e\u7684attention map\u6743\u91cd\u5927\u7684\u7a7a\u57df\u5e76\u4e0d\u5168\u7a0b\u5728\u8001\u864e\u8eab\u4e0a\u3002","title":"cat-tiger"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-hamster","text":"origin:dog, new:hamster , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 \u5de6\u4e0a\u89d2, \u7a7a\u57df\u63a7\u5236\u8fc7\u5f3a, \u5bfc\u81f4\u65e0\u6cd5\u751f\u6210\u8001\u9f20\u7684\u5f62\u6001, \u6ce8\u610f\u5230\u6700\u540e\u4e00\u884c, \u6b64\u65f6crossAttention\u5e76\u65e0inject, \u4ec5\u4ec5selfAttention inject, \u6b64\u65f6\u7a7a\u57df\u9650\u5236\u5c31\u4e0d\u662f\u5f88\u5f3a\u3002\u6709\u6bd4\u8f83\u597d\u7684\u6548\u679c, \u4f46\u662f\u5374\u548c\u6587\u7ae0\u4e2d\u7684selfAttention\u4e0d\u591f\u642d\u8fb9\u4e86, \u6700\u540e\u4e00\u6b65\u7a81\u7136\u8df3\u5230\u539f\u56fe, \u4e5f\u662f\u6709\u4e9b\u532a\u5937\u6240\u601d, \u9700\u8981\u518d\u7ec6\u81f4\u5206\u6790\u3002","title":"dog-hamster"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-hamster-detail","text":"left to right ss=0.6-1.0, up to down cs=0.6-1.0 ce=se=1","title":"dog-hamster-detail"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#hamster-dog","text":"origin:hamster, new:dog , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0 \u8fd9\u91cc\u8fd8\u662f\u4f53\u73b0\u51fa\u4e86\u7a7a\u57df\u63a7\u5236\u5f88\u5f3a\u7684\u6548\u679c, \u5bfc\u81f4\u72d7\u7684\u6bdb\u53d1\u6210\u8272\u90fd\u548c\u8001\u9f20\u6bd4\u8f83\u76f8\u8fd1\u3002\u6ce8\u610f\u5230\u6700\u53f3\u8fb9\u7684\u5217, \u6b64\u65f6\u53ea\u6709crossAttention inject, \u6ca1\u6709selfAttention inject, \u5927\u5e45\u5ea6\u7684inject\u8303\u56f4\u6539\u53d8\u90fd\u6ca1\u6709\u5bf9\u6784\u56fe\u548c\u98ce\u683c\u6709\u660e\u663e\u53d8\u5316, \u8fd9\u4e5f\u503c\u5f97\u601d\u8003\u3002\u6700\u540e\u4e00\u884c\u7684\u884c\u4e3a\u4e5f\u975e\u5e38\u5947\u602a\u3002","title":"hamster-dog"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#hamster-dog-detail","text":"0.6-1.0, \u6ce8\u610f\u5230 \u521d\u59cb\u7684inject\u5bf9\u6784\u56fe\u7684\u53d8\u5316\u7a0b\u5ea6\u8fd8\u662f\u6bd4\u8f83\u660e\u663e \u7684, \u4f8b\u5982\u51fa\u73b0\u4e86\u6234\u773c\u955c\u72d7\u7684\u56fe\u7247\u3002","title":"hamster-dog-detail"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-smiling","text":"origin:dog, new:smiling dog , left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0","title":"dog-smiling"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-smiling-detail","text":"left to right: ss0.6 to ss1.0, up to down: cs0.6 to cs1.0","title":"dog-smiling-detail"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-smiling-end","text":"\u8fd9\u91cc\u8bf4\u660e\u4e86, \u524d\u671f\u662f\u5b9a\u7ed3\u6784\u4f4d\u7f6e\u7684\u5173\u952e\u671f, \u524d\u671f\u4e00\u5b9a, \u540e\u7eed\u518d\u6539\u6bd4\u8f83\u9ebb\u70e6, \u5f53\u7136, \u8fd9\u4e5f\u662f\u56e0\u4e3a\u8fd9\u91cc\u4ee3\u7801\u5b9e\u73b0\u7684Attention map\u662f\u4f9d\u8d56\u4e8e\u524d\u4e00\u4e2alatent\u7684input\u7684, \u800c\u4e0d\u662f\u72ec\u7acb\u7684\u3002 left to right: ce0.6 to ce1.0, up to down: se0.6 to se1.0, ss=cs=0.3","title":"dog-smiling-end"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#cake","text":"","title":"cake \u5b9e\u9a8c"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#cake_1","text":"ss0.7, cs0.7, se=ce=1 apple cheese chocolate jello lego matcha pistachio pumpkin \u82f9\u679c \u829d\u58eb \u5de7\u514b\u529b \u679c\u51bb \u4e50\u9ad8 \u62b9\u8336 \u5f00\u5fc3\u679c \u5357\u74dc","title":"cake"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#lemon-cheese","text":"ss0.0-1.0 cs0.0-1.0 lemon cheese","title":"lemon-cheese"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#lemon-pistachio","text":"ss0.7-1.0, cs0.7-1.0 lemon pistachio","title":"lemon-pistachio"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#value-map","text":"\u4ee5\u4e0a\u7684\u77e9\u9635\u5c31\u4f53\u73b0\u51fa\u4e86value, map\u7684\u4e0d\u540c\u7ec4\u5408, \u6709\u4e00\u5b9a\u7684\u8d8b\u52bf\u53ef\u4ee5\u8bc1\u660emap\u63a7\u5236\u4f4d\u7f6e, value\u63a7\u5236\u5185\u5bb9, \u4f46\u8fd9\u8fd8\u9700\u8981image condition\u7684\u9a8c\u8bc1\u3002","title":"value, map\u4e0d\u540c\u7ec4\u5408"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#image-conditioninject","text":"\u8fd9\u91cc\u5176\u5b9e\u5c31\u662f\u6307\u5e76\u884cinject, \u540c\u65f6\u4e5f\u89e3\u91ca\u4e86\u7b97\u6cd5\u662f\u5426\u6709\u95ee\u9898\u7684\u95ee\u9898\u3002","title":"image condition\u7684\u4e0d\u540c\u7ec4\u5408(\u5e76\u884cinject)"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#hamster-dog-self","text":"left to right: ss0.0 to ss1.0, up to down: cs0.0 to cs1.0, se=ce=1","title":"hamster-dog-self"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#margin","text":"","title":"margin"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-smiling-self","text":"","title":"dog-smiling-self"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#dog-smiling-self_1","text":"","title":"dog-smiling-self"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#attention_1","text":"","title":"\u6253\u5370\u6240\u6709\u7684\u751f\u6210\u8fc7\u7a0b\u4ee5\u53caattention"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#inject","text":"","title":"\u624b\u52a8inject"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#1","text":"\u5b9e\u9a8c\u65b9\u6cd5, \u9009\u5b9a\u56fe\u7247\u7684\u4e00\u5757\u65b9\u5f62\u533a\u57df, \u7136\u540e\u5728Attention map\u8fd9\u4e2a\u533a\u57df\u4e2d\u4f7f\u5f97\u8be5\u90e8\u5206\u7684\u6743\u91cd\u589e\u52a0, \u589e\u52a0\u65b9\u5f0f\u8fd8\u6709\u5f85\u5c1d\u8bd5\u3002 \u53eainject 64X64\u7684attention map \u7edd\u5bf9inject\uff0cmast\u76f8\u52a0\u540e\u4e0d\u505asoftmax\uff0c\u76f4\u63a5\u4e58value","title":"\u65b9\u6cd51"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#inject-scale-10","text":"\u8fd9\u91cc\u9996\u5148\u9610\u660e\u4e86\u6211\u4eecinjcet\u7684\u4f4d\u7f6e\u7684\u5927\u81f4\u533a\u57df\uff0c \u4e3a\u6697\u7ea2\u8272\u90e8\u5206\u3002","title":"inject scale = 10"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#inject-scale4","text":"","title":"inject scale=4"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#inject-scale3","text":"","title":"inject scale=3"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_9","text":"self Attention \u6a21\u578b\u7ed3\u6784 dreambooth \u52a8\u8bcd inpainting() \u518d\u8fc7\u4e00\u6b21\u5f52\u4e00\u5316(\u5c1d\u8bd5\u5404\u79cd\u65b9\u6cd5) \u8c03\u7a33\u5b9a\u4e00\u4e9b","title":"\u503c\u5f97\u63a2\u7d22\u7684\u65b9\u5411"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#inject_1","text":"\u76ee\u524d\u53eainject 64X64, inject 32X32 \u6216\u8bb8\u6548\u679c\u4f1a\u66f4\u597d(\u5df2\u7ecf\u9a8c\u8bc1) inject \u7684\u9762\u79ef\u672a\u5fc5\u80fd\u591f\u9650\u5236\u751f\u6210\u52a8\u7269\u7684\u9762\u79ef \u6709\u4e00\u5b9a\u7684\u63a7\u5236\u4f5c\u7528 \u505asoftmax\u5f52\u4e00\u5316\u4f3c\u4e4e\u6ca1\u6709\u6548\u679c, \u53ef\u80fd\u662fsoftmax\u7684\u7ef4\u5ea6","title":"\u624b\u52a8inject\u7ed3\u8bba"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_10","text":"map\u51b3\u5b9a\u7a7a\u57df, value\u51b3\u5b9a\u5185\u5bb9\u57fa\u672c\u6210\u7acb \u4ee5\u4e0a\u4e8c\u8005\u5747\u53d7\u5230image condition\u5f71\u54cd \u5e76\u884cinject \u66f4\u5408\u7406 t z4 z3 z2 z1 z0 \u732b ... t* z4* z3* z2* z1* \u72d7 ... z4 z3 z3end z2 z2end \u200b z3* z2*","title":"\u7ed3\u8bba"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_11","text":"dog-smiling \u51fa\u73b0\u4e86\u4eba \u51fa\u73b0\u50cf\u7d20\u5316 \u63a7\u5236\u7684\u4e0d\u662f\u5f88\u7406\u60f3","title":"\u95ee\u9898:"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_12","text":"","title":"\u6ce8\u610f\u5230\u7684\u95ee\u9898"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/prompt-to-prompt/#_13","text":"\u6211\u4eec\u6ce8\u610f\u5230\u4ee3\u7801\u4e2d\u7ed9\u7684\u6837\u4f8b\u770b\u8d77\u6765\u4e0d\u9519, \u4f46\u662f\u7a0d\u5fae\u6dfb\u52a0\u4e00\u4e9b\u4fee\u6539, \u5c31\u4f1a\u51fa\u73b0\u4e00\u4e9b\u95ee\u9898, \u5f53\u7136\u8fd9\u4efb\u7136\u9700\u8981\u518d\u56de\u5934\u770b\u4e00\u4e0b\u8bba\u6587\u5e76\u8fdb\u884c\u4fee\u6539\u3002 \u4f8b\u5982, \u539f\u6765\u7684\u5b9e\u73b0\u4e2d, \u4f7f\u7528\u7684\u539f\u6761\u4ef6\u751f\u6210 \u201ca cat sitting on a car\u201d, seed=248396402679 , \u5f97\u5230\u5982\u4e0b\u56fe\u7247: attention inject\u7684\u65b9\u5f0f, \u53c2\u6570\u4e3a \"a cat sitting on a car\", \"a smiling dog sitting on a car\", prompt_edit_spatial_start=0.7, seed=248396402679 \u5219\u5982\u4e0b\u56fe: \u5982\u679c\u5c06\u53c2\u6570\u6539\u53d8\u4e3a \"a cat sitting on a car\", \"a dog sitting on a car\", prompt_edit_spatial_start=0.7,seed=248396402679,steps=50 , \u5219\u5f97\u5230\u5982\u4e0b\u56fe\u7247, \u6bd4\u8f83\u6050\u6016: value, map\u4e0d\u540c\u7ec4\u5408, \u629b\u5f03selfattention , new_sliced_attention , seq compare","title":"\u5176\u5b9e\u751f\u6210\u63a7\u5236\u6ca1\u6709\u5f88\u7406\u60f3"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/","text":"Textual inversion [toc] \u524d\u8a00 \u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u805a\u7126\u5982\u4f55\u63a7\u5236\u4e00\u4e2a\u56fa\u5b9a\u7269\u4f53\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0, \u4f46\u662f\u5374\u4e0d\u6539\u53d8\u6a21\u578b\u7684\u53c2\u6570, \u8fd9\u91cc\u7684\u590d\u73b0\u5de5\u4f5c\u4e3b\u8981\u57fa\u4e8e\u201cAn Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\u201d, \u4ee3\u7801\u5219\u57fa\u4e8ehttps://github.com/rinongal/textual_inversion, \u4f7f\u7528\u7684\u6a21\u578b\u662fldm, \u4e0d\u662fstable-diffusion\u3002 \u6211\u4eec\u5728\u8fd9\u91cc\u671f\u671b\u505a\u5230\u4ee5\u4e0b\u51e0\u70b9: \u590d\u73b0\u4ee3\u7801, \u770b\u770b\u6548\u679c \u5c1d\u8bd5\u7406\u89e3\u539f\u7406 \u603b\u7ed3\u4f18\u70b9\u548c\u7f3a\u70b9 \u56e0\u4e3a\u6587\u7ae0\u601d\u8def\u5f88\u7b80\u5355, \u611f\u89c9\u6ca1\u6709\u5341\u5206\u503c\u5f97\u8ba8\u8bba\u7684\u5730\u65b9, \u6211\u4eec\u53ea\u9700\u8981\u770b\u4ee5\u4e0b\u6548\u679c\u3002 cat a cake in style of cat airpods a red airpods a picture of airpods magic cube photo of maigc cube red magic cube magic cube by origin elephant elephant drinking water elephant running \u7ed3\u8bba \u4e0d\u53d8\u53c2\u6570, flexible \u6709\u4e00\u5b9a\u6548\u679c \u8bad\u7ec31-2\u5c0f\u65f6 \u6709\u4e0d\u7406\u60f3\u7684\u5730\u65b9","title":"Textual inversion"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#textual-inversion","text":"[toc]","title":"Textual inversion"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#_1","text":"\u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u805a\u7126\u5982\u4f55\u63a7\u5236\u4e00\u4e2a\u56fa\u5b9a\u7269\u4f53\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0, \u4f46\u662f\u5374\u4e0d\u6539\u53d8\u6a21\u578b\u7684\u53c2\u6570, \u8fd9\u91cc\u7684\u590d\u73b0\u5de5\u4f5c\u4e3b\u8981\u57fa\u4e8e\u201cAn Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\u201d, \u4ee3\u7801\u5219\u57fa\u4e8ehttps://github.com/rinongal/textual_inversion, \u4f7f\u7528\u7684\u6a21\u578b\u662fldm, \u4e0d\u662fstable-diffusion\u3002 \u6211\u4eec\u5728\u8fd9\u91cc\u671f\u671b\u505a\u5230\u4ee5\u4e0b\u51e0\u70b9: \u590d\u73b0\u4ee3\u7801, \u770b\u770b\u6548\u679c \u5c1d\u8bd5\u7406\u89e3\u539f\u7406 \u603b\u7ed3\u4f18\u70b9\u548c\u7f3a\u70b9 \u56e0\u4e3a\u6587\u7ae0\u601d\u8def\u5f88\u7b80\u5355, \u611f\u89c9\u6ca1\u6709\u5341\u5206\u503c\u5f97\u8ba8\u8bba\u7684\u5730\u65b9, \u6211\u4eec\u53ea\u9700\u8981\u770b\u4ee5\u4e0b\u6548\u679c\u3002","title":"\u524d\u8a00"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#cat","text":"","title":"cat"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#a-cake-in-style-of-cat","text":"","title":"a cake in style of cat"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#airpods","text":"","title":"airpods"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#a-red-airpods","text":"","title":"a red airpods"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#a-picture-of-airpods","text":"","title":"a picture of airpods"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#magic-cube","text":"","title":"magic cube"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#photo-of-maigc-cube","text":"","title":"photo of maigc cube"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#red-magic-cube","text":"","title":"red magic cube"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#magic-cube-by-origin","text":"","title":"magic cube by origin"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#elephant","text":"","title":"elephant"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#elephant-drinking-water","text":"","title":"elephant drinking water"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#elephant-running","text":"","title":"elephant running"},{"location":"%E7%A7%91%E7%A0%94/%E5%9B%BE%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B/textual_inversion/#_2","text":"\u4e0d\u53d8\u53c2\u6570, flexible \u6709\u4e00\u5b9a\u6548\u679c \u8bad\u7ec31-2\u5c0f\u65f6 \u6709\u4e0d\u7406\u60f3\u7684\u5730\u65b9","title":"\u7ed3\u8bba"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/","text":"\u7b80\u4ecb \u4ec0\u4e48\u662f\u73e0\u7b97? \u770b\u8fd9\u91cc \u76ee\u524d\u53c2\u4e0e\u7684\u73e0\u7b97\u7684\u4e3b\u8981\u5de5\u4f5c\u5185\u5bb9\u5c31\u662f\u5c06\u73e0\u7b97\u6846\u67b6\u5728pytorch\u4e0b\u5b9e\u73b0\u51fa\u6765, \u7136\u540e\u518d\u653e\u5230pypi\u4e0a\u3002 \u8fd9\u4e2a\u9879\u76ee\u4ece2022\u6691\u5047\u5f00\u59cb, 2022\u5e7410\u6708\u524d\u5e94\u5f53\u57fa\u672c\u7ed3\u675f\u3002 2022\u5e7411\u670821\u65e5 \u548c\u9648\u952e\u98de\u8001\u5e08\u804a\u4e86\u4e00\u4e0b\u5f53\u524d\u73e0\u7b97\u7684\u8fdb\u5c55, \u8001\u5e08\u6307\u51fa\u5f53\u524d\u7684\u6846\u67b6\u8fd8\u662f\u4e0d\u662f\u5f88\u597d\u7528, \u73b0\u5728\u8fd9\u4e2a\u6846\u67b6\u7684\u4e3b\u8981\u76ee\u7684\u662f\u597d\u7528, \u800c\u4e0d\u662f\u652f\u6301\u66f4\u5148\u8fdb\u7684\u7b97\u6cd5\u3002\u65e2\u7136\u8f6c\u79fb\u5230pytorch\u4e0a\u4e86, \u90a3\u76ee\u7684\u5c31\u662f\u66f4\u597d\u7528\u3002\u8001\u5e08\u6307\u51fa\u5f53\u524d\u6846\u67b6\u505a\u7684\u4e8b\u60c5\u592a\u5c11, \u7a0b\u5e8f\u5458\u505a\u7684\u4e8b\u60c5\u592a\u591a, \u56de\u53bb\u5c31\u628a\u4e00\u4e2a\u7b80\u5355\u7684\u65b9\u4fbf\u7a0b\u5e8f\u5458\u7f16\u5199\u7684\u70b9\u7ed9\u6539\u4e86, \u4f46\u662f\u56fe\u6784\u5efa\u4ee5\u53cainference\u7684\u90e8\u5206\u5982\u4f55\u4f18\u5316, \u5c31\u8981\u6c42\u5bf9\u8d1d\u53f6\u65af\u6846\u67b6\u4ee5\u53ca\u8f6f\u4ef6\u5de5\u7a0b\u6709\u6bd4\u8f83\u6df1\u5165\u7684\u7406\u89e3\u4e86\u3002","title":"\u7b80\u4ecb"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/#_1","text":"\u4ec0\u4e48\u662f\u73e0\u7b97? \u770b\u8fd9\u91cc \u76ee\u524d\u53c2\u4e0e\u7684\u73e0\u7b97\u7684\u4e3b\u8981\u5de5\u4f5c\u5185\u5bb9\u5c31\u662f\u5c06\u73e0\u7b97\u6846\u67b6\u5728pytorch\u4e0b\u5b9e\u73b0\u51fa\u6765, \u7136\u540e\u518d\u653e\u5230pypi\u4e0a\u3002 \u8fd9\u4e2a\u9879\u76ee\u4ece2022\u6691\u5047\u5f00\u59cb, 2022\u5e7410\u6708\u524d\u5e94\u5f53\u57fa\u672c\u7ed3\u675f\u3002","title":"\u7b80\u4ecb"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/#20221121","text":"\u548c\u9648\u952e\u98de\u8001\u5e08\u804a\u4e86\u4e00\u4e0b\u5f53\u524d\u73e0\u7b97\u7684\u8fdb\u5c55, \u8001\u5e08\u6307\u51fa\u5f53\u524d\u7684\u6846\u67b6\u8fd8\u662f\u4e0d\u662f\u5f88\u597d\u7528, \u73b0\u5728\u8fd9\u4e2a\u6846\u67b6\u7684\u4e3b\u8981\u76ee\u7684\u662f\u597d\u7528, \u800c\u4e0d\u662f\u652f\u6301\u66f4\u5148\u8fdb\u7684\u7b97\u6cd5\u3002\u65e2\u7136\u8f6c\u79fb\u5230pytorch\u4e0a\u4e86, \u90a3\u76ee\u7684\u5c31\u662f\u66f4\u597d\u7528\u3002\u8001\u5e08\u6307\u51fa\u5f53\u524d\u6846\u67b6\u505a\u7684\u4e8b\u60c5\u592a\u5c11, \u7a0b\u5e8f\u5458\u505a\u7684\u4e8b\u60c5\u592a\u591a, \u56de\u53bb\u5c31\u628a\u4e00\u4e2a\u7b80\u5355\u7684\u65b9\u4fbf\u7a0b\u5e8f\u5458\u7f16\u5199\u7684\u70b9\u7ed9\u6539\u4e86, \u4f46\u662f\u56fe\u6784\u5efa\u4ee5\u53cainference\u7684\u90e8\u5206\u5982\u4f55\u4f18\u5316, \u5c31\u8981\u6c42\u5bf9\u8d1d\u53f6\u65af\u6846\u67b6\u4ee5\u53ca\u8f6f\u4ef6\u5de5\u7a0b\u6709\u6bd4\u8f83\u6df1\u5165\u7684\u7406\u89e3\u4e86\u3002","title":"2022\u5e7411\u670821\u65e5"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/","text":"\u5de5\u4f5c\u65e5\u5fd7 \u73e0\u7b97\u7684\u5de5\u4f5c\u8fc7\u7a0b\u4e3b\u8981\u662f\u5f00\u53d1\u8fc7\u7a0b, \u5728\u5f00\u53d1\u4e2d\u4e0d\u65ad\u719f\u6089\u4ee3\u7801\u7684\u6846\u67b6, \u5bf9\u4e8e\u5de5\u4f5c\u65e5\u5fd7\u800c\u8a00, \u4e3b\u8981\u5c31\u662f\u8bb0\u5f55\u4ee5\u4e0b\u9047\u5230\u7684\u95ee\u9898, \u4f46\u662f\u5982\u679c\u4e0d\u6e05\u695a\u95ee\u9898\u7684\u524d\u63d0\u63d0\u8981, \u9884\u8ba1\u662f\u770b\u4e0d\u61c2\u95ee\u9898\u5728\u8bf4\u4ec0\u4e48, \u6240\u4ee5\u8fd9\u90e8\u5206\u4e3b\u8981\u662f\u7b14\u8005\u81ea\u5df1\u770b, \u4ed6\u4eba\u968f\u610f\u5c31\u597d\u3002 \u5b89\u88c5pytorch\u73e0\u7b97\u73af\u5883 pytorch \u73e0\u7b97 \u5b89\u88c5scipy pip3 install cython pybind11 pip3 install --no-binary :all: --no-use-pep517 numpy pip3 install pythran brew install openblas gfortran export OPENBLAS = /opt/homebrew/opt/openblas/lib/ pip3 install --no-binary :all: --no-use-pep517 scipy pip install torch pip install matplotlib \u6216\u8005\u76f4\u63a5\u9009\u62e9 conda install scipy \u81f3\u6b64, \u53ef\u4ee5\u8dd1\u901a python -m unittest discover -v \u3002 coverage report --include=\"zhusuan/*\" \u62a5\u544a No data to report. \u6682\u65f6\u5148\u4e0d\u7ba1\u3002 TensorFlow \u73e0\u7b97 \u76f4\u63a5clone\uff0c pip install . HMC \u95ee\u9898 \u4e3b\u8981\u7684\u8868\u73b0\u95ee\u9898\u4e3a, test_mcmc\u4e2d\u6d4b\u8bd5HMC\u4e2d\u6709\u8f83\u5927\u7684\u4f30\u8ba1\u8bef\u5dee, \u5176\u6b21\u8fd8\u6ca1\u80fd\u8dd1\u51fatoy example\u4e2d\u65adgaussian.py, \u63a5\u4e0b\u6765\u4e3b\u8981\u4ece\u51e0\u4e2a\u65b9\u9762\u6765\u8ba8\u8bba\u548c\u5206\u6790\u6211\u4eec\u76ee\u524d\u9047\u5230\u95ee\u9898\u7684\u53ef\u80fd\u7684\u539f\u56e0: torch\u7248\u672c\u548ctf\u7248\u672c\u5b9e\u73b0\u7684\u533a\u522b while \u5faa\u73af\u7684\u662f\u5426\u5141\u8bb8\u68af\u5ea6 _ , q , p = tf . while_loop ( loop_cond , loop_body , [ i , q , p ], back_prop = False , parallel_iterations = 1 ) \u4f46\u662f\u5728torch\u7248\u672c\u4e2d, \u5982\u679c\u7981\u7528\u68af\u5ea6, \u662f\u7a0b\u5e8f\u662f\u65e0\u6cd5\u8fd0\u884c\u7684 log_joint \u548c \u68af\u5ea6\u7684\u95ee\u9898 log_joint \u6bcf\u6b21\u90fd\u9700\u8981\u5355\u72ec\u7684sn? \u4ee5\u4e0b\u4ee3\u7801\u4e0d\u80fd\u8fd0\u884c class Gaussian ( BayesianNet ): def __init__ ( self , n_x , std , n_particles ): super ( Gaussian , self ) . __init__ () self . _n_x = n_x self . _std = std self . _n_particles = n_particles dist = Normal ( torch . zeros ([ n_x ], dtype = torch . float32 ), std = self . _std ) self . sn ( dist , \"x\" , n_samples = self . _n_particles ) def forward ( self , observed ): self . observe ( observed ) return self \u4ee5\u4e0b\u4ee3\u7801\u53ef\u4ee5\u8fd0\u884c: class Gaussian ( BayesianNet ): def __init__ ( self , n_x , std , n_particles ): super ( Gaussian , self ) . __init__ () self . _n_x = n_x self . _std = std self . _n_particles = n_particles def forward ( self , observed ): self . observe ( observed ) dist = Normal ( torch . zeros ([ n_x ], dtype = torch . float32 ), std = self . _std ) self . sn ( dist , \"x\" , n_samples = self . _n_particles ) return self \u68af\u5ea6\u95ee\u9898 self . q = [ v . clone () . requires_grad_ () for v in latent_v ] #\u5982\u679c\u4e0dclone\u5e76\u4e14\u5f00\u542f\u68af\u5ea6, gaussian \u4e5f\u4f1a\u62a5\u9519 gaussian \u95ee\u9898 \u4e3a\u4ec0\u4e48\u65b9\u5dee\u4f1a\u548cn_chain \u6709\u5173, n_chains \u8d8a\u5927, \u65b9\u5dee\u8d8a\u5927 \u5728i=7\u65f6\u770b\u68af\u5ea6, \u53d1\u73b0\u51fa\u73b0\u4e86\u8d85\u5927\u7684\u68af\u5ea6 \u6df1\u5ea6de HMC\u7684bug \u6211\u4eec\u9884\u8ba1\u662f\u6c42\u5bfc\u7684\u51fd\u6570\u8ba1\u7b97\u5012\u6570\u9519\u8bef\u5bfc\u81f4\u4e86\u91c7\u6837\u7684\u9519\u8bef, \u6b63\u786e\u7684\u8ba1\u7b97\u8def\u5f84\u5e94\u8be5\u662f\u6839\u636e\u5f53\u524d\u7ed9\u7684varlist, \u6839\u636e\u5bc6\u5ea6\u51fd\u6570\u8ba1\u7b97\u68af\u5ea6, \u4f46\u662f\u76ee\u524d\u770b\u8d77\u6765\u8ba1\u7b97\u7684\u662f\u76f8\u52a0\u7684\u68af\u5ea6\u3002 \u7ec6\u67e5\u6587\u6863 \u662f\u5426\u8981\u6dfb\u52a0conda \u5b89\u88c5\u7684\u547d\u4ee4","title":"\u5de5\u4f5c\u65e5\u5fd7"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#_1","text":"\u73e0\u7b97\u7684\u5de5\u4f5c\u8fc7\u7a0b\u4e3b\u8981\u662f\u5f00\u53d1\u8fc7\u7a0b, \u5728\u5f00\u53d1\u4e2d\u4e0d\u65ad\u719f\u6089\u4ee3\u7801\u7684\u6846\u67b6, \u5bf9\u4e8e\u5de5\u4f5c\u65e5\u5fd7\u800c\u8a00, \u4e3b\u8981\u5c31\u662f\u8bb0\u5f55\u4ee5\u4e0b\u9047\u5230\u7684\u95ee\u9898, \u4f46\u662f\u5982\u679c\u4e0d\u6e05\u695a\u95ee\u9898\u7684\u524d\u63d0\u63d0\u8981, \u9884\u8ba1\u662f\u770b\u4e0d\u61c2\u95ee\u9898\u5728\u8bf4\u4ec0\u4e48, \u6240\u4ee5\u8fd9\u90e8\u5206\u4e3b\u8981\u662f\u7b14\u8005\u81ea\u5df1\u770b, \u4ed6\u4eba\u968f\u610f\u5c31\u597d\u3002","title":"\u5de5\u4f5c\u65e5\u5fd7"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#pytorch","text":"","title":"\u5b89\u88c5pytorch\u73e0\u7b97\u73af\u5883"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#pytorch_1","text":"\u5b89\u88c5scipy pip3 install cython pybind11 pip3 install --no-binary :all: --no-use-pep517 numpy pip3 install pythran brew install openblas gfortran export OPENBLAS = /opt/homebrew/opt/openblas/lib/ pip3 install --no-binary :all: --no-use-pep517 scipy pip install torch pip install matplotlib \u6216\u8005\u76f4\u63a5\u9009\u62e9 conda install scipy \u81f3\u6b64, \u53ef\u4ee5\u8dd1\u901a python -m unittest discover -v \u3002 coverage report --include=\"zhusuan/*\" \u62a5\u544a No data to report. \u6682\u65f6\u5148\u4e0d\u7ba1\u3002","title":"pytorch \u73e0\u7b97"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#tensorflow","text":"\u76f4\u63a5clone\uff0c pip install .","title":"TensorFlow \u73e0\u7b97"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#hmc","text":"\u4e3b\u8981\u7684\u8868\u73b0\u95ee\u9898\u4e3a, test_mcmc\u4e2d\u6d4b\u8bd5HMC\u4e2d\u6709\u8f83\u5927\u7684\u4f30\u8ba1\u8bef\u5dee, \u5176\u6b21\u8fd8\u6ca1\u80fd\u8dd1\u51fatoy example\u4e2d\u65adgaussian.py, \u63a5\u4e0b\u6765\u4e3b\u8981\u4ece\u51e0\u4e2a\u65b9\u9762\u6765\u8ba8\u8bba\u548c\u5206\u6790\u6211\u4eec\u76ee\u524d\u9047\u5230\u95ee\u9898\u7684\u53ef\u80fd\u7684\u539f\u56e0:","title":"HMC \u95ee\u9898"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#torchtf","text":"","title":"torch\u7248\u672c\u548ctf\u7248\u672c\u5b9e\u73b0\u7684\u533a\u522b"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#while","text":"_ , q , p = tf . while_loop ( loop_cond , loop_body , [ i , q , p ], back_prop = False , parallel_iterations = 1 )","title":"while \u5faa\u73af\u7684\u662f\u5426\u5141\u8bb8\u68af\u5ea6"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#torch","text":"","title":"\u4f46\u662f\u5728torch\u7248\u672c\u4e2d, \u5982\u679c\u7981\u7528\u68af\u5ea6, \u662f\u7a0b\u5e8f\u662f\u65e0\u6cd5\u8fd0\u884c\u7684"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#log_joint","text":"","title":"log_joint \u548c \u68af\u5ea6\u7684\u95ee\u9898"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#log_joint-sn","text":"\u4ee5\u4e0b\u4ee3\u7801\u4e0d\u80fd\u8fd0\u884c class Gaussian ( BayesianNet ): def __init__ ( self , n_x , std , n_particles ): super ( Gaussian , self ) . __init__ () self . _n_x = n_x self . _std = std self . _n_particles = n_particles dist = Normal ( torch . zeros ([ n_x ], dtype = torch . float32 ), std = self . _std ) self . sn ( dist , \"x\" , n_samples = self . _n_particles ) def forward ( self , observed ): self . observe ( observed ) return self \u4ee5\u4e0b\u4ee3\u7801\u53ef\u4ee5\u8fd0\u884c: class Gaussian ( BayesianNet ): def __init__ ( self , n_x , std , n_particles ): super ( Gaussian , self ) . __init__ () self . _n_x = n_x self . _std = std self . _n_particles = n_particles def forward ( self , observed ): self . observe ( observed ) dist = Normal ( torch . zeros ([ n_x ], dtype = torch . float32 ), std = self . _std ) self . sn ( dist , \"x\" , n_samples = self . _n_particles ) return self","title":"log_joint \u6bcf\u6b21\u90fd\u9700\u8981\u5355\u72ec\u7684sn?"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#_2","text":"self . q = [ v . clone () . requires_grad_ () for v in latent_v ] #\u5982\u679c\u4e0dclone\u5e76\u4e14\u5f00\u542f\u68af\u5ea6, gaussian \u4e5f\u4f1a\u62a5\u9519","title":"\u68af\u5ea6\u95ee\u9898"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#gaussian","text":"\u4e3a\u4ec0\u4e48\u65b9\u5dee\u4f1a\u548cn_chain \u6709\u5173, n_chains \u8d8a\u5927, \u65b9\u5dee\u8d8a\u5927 \u5728i=7\u65f6\u770b\u68af\u5ea6, \u53d1\u73b0\u51fa\u73b0\u4e86\u8d85\u5927\u7684\u68af\u5ea6","title":"gaussian \u95ee\u9898"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#de-hmcbug","text":"\u6211\u4eec\u9884\u8ba1\u662f\u6c42\u5bfc\u7684\u51fd\u6570\u8ba1\u7b97\u5012\u6570\u9519\u8bef\u5bfc\u81f4\u4e86\u91c7\u6837\u7684\u9519\u8bef, \u6b63\u786e\u7684\u8ba1\u7b97\u8def\u5f84\u5e94\u8be5\u662f\u6839\u636e\u5f53\u524d\u7ed9\u7684varlist, \u6839\u636e\u5bc6\u5ea6\u51fd\u6570\u8ba1\u7b97\u68af\u5ea6, \u4f46\u662f\u76ee\u524d\u770b\u8d77\u6765\u8ba1\u7b97\u7684\u662f\u76f8\u52a0\u7684\u68af\u5ea6\u3002","title":"\u6df1\u5ea6de HMC\u7684bug"},{"location":"%E7%A7%91%E7%A0%94/%E7%8F%A0%E7%AE%97/zhusuan_worklog/#_3","text":"\u662f\u5426\u8981\u6dfb\u52a0conda \u5b89\u88c5\u7684\u547d\u4ee4","title":"\u7ec6\u67e5\u6587\u6863"}]}