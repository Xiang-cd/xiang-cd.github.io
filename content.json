{"posts":[{"title":"Emu3-practice","text":"最近北京智源放了一个模型叫做emu3，是一个多模态生成模型，恰巧机器学习课程需要有个project，然后就打算微调一下，体验一下AR model 的魅力。 就causal transformer本身其实并不复杂，但是比较复杂或者我不太熟悉的应该是tokenizer，特别是视觉tokenizer是如何组织的，生成过程中如何保证图像规格，各类特殊token的作用，AR模型的CFG是如何在代码层面实现的等问题，所以这篇博客就是带着问题来寻找答案，并记录下来。 emu3的vision tokenizervqvae本质上就是一个vqvae，但是要使得vae能够编解码视频，同时对视频在时间维度上的压缩做到4。对于原始的2Dvae的改进点在于，将res block中的2D conv换成causal 3Dconv，但是这些3Dconv并不同时压缩时间维度，只在2D维度完成压缩之后再单独进行两次的时间维度压缩，解码则是先扩展时间维度，然后再上采样空间维度。对于图片的处理，是将图片在时间维度重复四次作为视频进行压缩或者逆压缩， 重构后只取第一帧。 image token和text token的联合处理这里我想弄明白的是，vision token和text token是如何share code book的。在emu3中，主要通过Emu3Processor这个类来实现，初始化传入图片预处理器，vqvae以及text tokenizer。 1processor = Emu3Processor(image_processor, image_tokenizer, tokenizer) 其中最重要的函数是： 12345678910111213visual_template=(\"&lt;|visual token {token_id:0&gt;6d}|&gt;\", r\"&lt;\\|visual token (\\d+)\\|&gt;\")def to_imgstr(self, image_tokens): image_tokens = image_tokens.cpu().numpy().tolist() image_token_str = [ [ self.visual_template[0].format(token_id=token_id) for token_id in token_row ] for token_row in image_tokens ] image_row_str = [\"\".join(token_row) for token_row in image_token_str] imgstr = self.tokenizer.eol_token.join(image_row_str) return imgstr 以上函数说明白了两个问题： 视觉token到文本token的转化。视觉token会被转化为’&lt;|visual token 000000|&gt;’ 这样的文本token，这在文本tokenizer中相当于第151854个token，依次增长，也就是前151853个是普通本文token，后面的依次是视觉token。完成了token的映射。 视觉token的编排。可见，图像的编码是通过换行来进行的，没有对图像token做2D的位置编码，这和文章表述一致。 如何保证图像生成规格这里我想解决的问题是，AR模型生成token是估计token的概率分布，这意味着对于一个64X64的图片生成过程，模型可能在生成过程中第一行生成了64个visual token，第二行生成了67个visual token，这会导致图片并不规格。 对此，emu3的解决方案是约束当前token生成的范围，例如到了第65个token，则严格限制只能生成换行符号，具体的代码实现是通过传递一个PrefixConstrainedLogitsProcessor，并对这个类传递一个判断生成状态的函数。 12345constrained_fn = processor.build_prefix_constrained_fn(h, w)PrefixConstrainedLogitsProcessor( constrained_fn , num_beams=1,) 最核心的判断逻辑在： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Emu3PrefixConstrainedLogitsHelper: def __init__( self, height, width, img_token, eoi_token, eos_token, eol_token, eof_token, pad_token, visual_tokens, ): self.height = height self.width = width self.img_token = img_token self.eoi_token = eoi_token self.eos_token = eos_token self.eol_token = eol_token self.eof_token = eof_token self.pad_token = pad_token self.visual_tokens = visual_tokens self.offset_cache = {} def __call__(self, batch_id, input_ids): if batch_id not in self.offset_cache: position = torch.nonzero(input_ids == self.img_token, as_tuple=True)[0][0] self.offset_cache[batch_id] = position height = self.height[batch_id] if self.height.shape[0] &gt; 1 else self.height[0] width = self.width[batch_id] if self.width.shape[0] &gt; 1 else self.width[0] offset = input_ids.shape[0] - self.offset_cache[batch_id] height = height.to(offset.device) width = width.to(offset.device) if offset % (width + 1) == 0: return (self.eol_token, ) elif offset == (width + 1) * height + 1: return (self.eof_token, ) elif offset == (width + 1) * height + 2: return (self.eoi_token, ) elif offset == (width + 1) * height + 3: return (self.eos_token, ) elif offset &gt; (width + 1) * height + 3: return (self.pad_token, ) else: return self.visual_tokens 可见的，这个prefixconstrain导致了当前的生成策略只能生成一张图片。 根据文章和初步的代码判断，最终的token排列情况应当如下： 1234567891011121314151617181920[BOS] {caption text} // caption text tokens [SOV{boi}] \"{H}*{W}\" // h * w are meta info tokens [SOT{img_token}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] // end of line [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] // vs are vision tokens [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [EOF] // end of frame [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] // end of line [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] // vs are vision tokens [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [vs] [vs] [vs] [vs] [vs] [vs] [EOL{eol}] [EOF] // end of frame [EOV{eoi}] [EOS] 为了能够让模型实现多帧的生成，可以简单对constrain做简单的修改，并增加新的帧数量的超参。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Emu3PrefixConstrainedLogitsHelper: def __init__( self, height, width, img_token, eoi_token, eos_token, eol_token, eof_token, pad_token, visual_tokens, num_frame=1, ): self.height = height self.width = width self.img_token = img_token self.eoi_token = eoi_token self.eos_token = eos_token self.eol_token = eol_token self.eof_token = eof_token self.pad_token = pad_token self.visual_tokens = visual_tokens self.num_frame = num_frame self.offset_cache = {} self.frame_index_cache = {} def __call__(self, batch_id, input_ids): if batch_id not in self.offset_cache: position = torch.nonzero(input_ids == self.img_token, as_tuple=True)[0][0] self.offset_cache[batch_id] = position self.frame_index_cache[batch_id] = 0 height = self.height[batch_id] if self.height.shape[0] &gt; 1 else self.height[0] width = self.width[batch_id] if self.width.shape[0] &gt; 1 else self.width[0] offset = input_ids.shape[0] - self.offset_cache[batch_id] height = height.to(offset.device) width = width.to(offset.device) if (offset - self.frame_index_cache[batch_id]) % (width + 1) == 0: return (self.eol_token, ) elif offset % ((width + 1) * height + 1) == 0: self.frame_index_cache[batch_id] += 1 return (self.eof_token, ) elif offset == ((width + 1) * height + 1) * self.num_frame + 1: return (self.eoi_token, ) elif offset == ((width + 1) * height + 1) * self.num_frame + 2: return (self.eos_token, ) elif offset &gt; ((width + 1) * height + 1) * self.num_frame + 3: return (self.pad_token, ) else: return self.visual_tokens 如何进行CFG在扩散模型中，classifier free guidance(CFG)就是对于模型的条件和无条件预测进行线性加权外推，使得模型朝着条件方向进行前进，其中包含一个超参guidance scale，具体公式如下： 1new_pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond) 这在扩散模型中能够显著提高生成质量，在AR模型用于视觉生成也被证明能够提高生成质量。在AR模型中，比较重要的是在哪个空间进行CFG，是在logits（未归一化），还是在归一化（softmax）之后的空间，还是在对数归一化（log_softmax)空间。 在emu3的代码中，实现CFG的代码位于UnbatchedClassifierFreeGuidanceLogitsProcessor这个类中。核心代码如下： 12345678910def __call__(self, input_ids, scores): scores = torch.nn.functional.log_softmax(scores, dim=-1) if self.guidance_scale == 1: return scores logits = self.get_unconditional_logits(input_ids) unconditional_logits = torch.nn.functional.log_softmax(logits[:, -1], dim=-1) scores_processed = self.guidance_scale * (scores - unconditional_logits) + unconditional_logits return scores_processed 可见，CFG在对数归一化空间进行。","link":"/blog/Emu3-practice/"},{"title":"My New Post","text":"This is an test blog post.code test1print(\"Hello, world!\") math testtest math , this was enabled by: 1npm install hexo-filter-mathjax set _config.yml as follows: 1234567891011mathjax: tags: all # or 'ams' or 'all' single_dollars: true # enable single dollar signs as in-line math delimiters cjk_width: 0.9 # relative CJK char width normal_width: 0.6 # relative normal (monospace) width append_css: true # add CSS to pages rendered by MathJax every_page: true # if true, every page will be rendered by MathJax regardless the `mathjax` setting in Front-matter packages: # extra packages to load extension_options: {} # you can put your extension options here # see http://docs.mathjax.org/en/latest/options/input/tex.html#tex-extension-options for more detail test quote list item 1 list item 2","link":"/blog/My-New-Post/"},{"title":"Sora authors","text":"前言Sora对于生成模型领域的impact, 远大于两年前的Dalle2, 因为Dalle2是将一个问题的性能提高一个层次, 从不可用变为可用。而Sora则是向世人展示一件事情是如何从不可能到可能的。其实从很多人的技术分析角度看, Sora背后的技术并不复杂, 我相信国内的公司也能够在一定的时间内复现出sora, 为什么说国内公司能复现, 因为他们已经看到了一件事情是可能的, 国内GPT百模大战的现象也是, 只有在OpenAI证明了一件事情是可行, 国内的公司才一窝蜂的上去做。 所以从这个角度看, 最顶尖的技术问题, 不是技术本身, 而是信念问题, 一种理性而坚定的信念。 而为什么OpenAI的团队能够有这样的信念呢, 如何在未知的情况下探索未知, 最后发现宝藏? 我想这一定和这群人有关系, 看清楚这群人有什么样的特质, 对未来个人发展规划, 技术公司寻找有潜力的创新人才, 总结美国在AI领域领先地位是如何形成的, 都有一定的益处, 所以这篇博客主要就是通过分析sora的作者们, 来看看有什么启发。 1. Tim Brooks Google Scholar 主页 github linked In 博士: 2019.8-2023.1(共四年), PhD at Berkeley AI Research advised by Alyosha Efros 博士论文 学士: CMU, 2013-2017。 从博士年限上来看, 应该是读了硕士的, 但是硕士学历并没有写在领英中, 看起来应该是gap了,否则应该有早于2019年的文章 , 这或许是有意思的点。 从学士角度看, 应该在1994年出生。 在2019年就有两篇CVPR oral, 主要从事图相关处理(超分辨率, 在Google), 生成模型(GAN, 在NVIDIA)。明星工作是instrucPix2Pix, 这篇文章最大的亮点在于利用GPT3自动生成训练数据, 这样的思想在2022chatgpt出现以前是非常领先的。同时在随后从事了Dalle3和Sora的工作, 还参与了GPT4技术报告。可以说在OpenAI都负责到了比较核心的工作。 从推特看, 他自己也对自己Sora的工作非常满意, 疯狂转发Sora生成的视频。 从体来说, 他的求学生涯还是比较漫长的, 读了十年的书, 在本科和gap或者硕士阶段还没有很多的产出。从影响力来看, 博士生初期的作品也没有很显著, 但是他工作一个很重要的特点就是质量高(两篇oral), 其次就是他的工作紧密的和工业界, 特别是大厂结合; 出色的大厂经历, 并且在每一段大厂的经历中做出相关的工作, 而非只是给大厂打工, 我想这一点是需要很强motivation的。他起飞的阶段就是积极利用OpenAI的GPT资源, 同时有着先进的合成数据思想, 并付诸实践, 我觉得这样的思想也体现在了Sora的生成模拟器类似风格的视频的迹象中。 其实合成数据这个话题一直在被使用, 特别是在语言模型领域, 但是真正将其用在图像视频生成领域并把一个东西狠狠的调work, 都是不容易的。 其实对于一般的博士生而言, 只要数据不是开源的, 就立马被难住了, 自己也不愿意去复现数据的处理, 更别提自己取创造新的数据, 这些工作很脏, 但是却直接影响一些事情能不能开展, 我觉得放低自己的身位, 认真把工作做好这样的心态和毅力都是十分难得的。 2. Bill Peebles Google Scholar 主页 github 博士论文 博士: 2019.8-2023.1(共四年), Berkeley AI Research advised by Alyosha Efros. 学士: undergrad at MIT where I was advised by Antonio Torralba. 从博士年限上看, 应该在1995年左右出生。 所有的工作, 不是Oral就是Spotlight, 博士期间一共6篇工作, 其中一作仅两篇。主要专注于生成模型, 博士期间曾在FAIR, Adobe, NVIDIA实习。 Tim Brooks和Bill Peebles有多次合作, 其中有一篇比较有意思的是Learning to learn with generative models of neural network checkpoints, 利用生成模型生成神经网络的参数, 这可以理解为一种meta learning, 虽然这篇文章没有投稿和中稿, 但是背后的探索和尝试还是比较有意思的, 体现出了他们敢想敢做的精神。我觉得同样的, 丰富的大厂经历和资源, 扎实的工作, 敢想敢干, 这些都非常重要。 我觉得反观我自己, 有时候参与的事情过多, 为了自己忙而忙, 没有深刻思考自己研究的动机和意义, 有时候有想法却不敢大胆的做出来, 大胆的去想办法做, 没有尝试去大厂获取更多的资源, 这些我觉得和他们还是有重要的差距。 3. Connor Holmes Google Scholar 推特 linked In 博士: 2017-2020, Colorado School of Mines, HPC 学士: 2013-2017, Colorado School of Mines, EE 是Sora的system leader, 主要优化大规模训练和推理的, 看得出是做系统出身的, 曾在微软实习和工作, 主要参与DeepSpeed的相关工作。系统相关的作品非常多, 一年能有五六篇。做过图相关的高性能计算, RNN gpu优化, GPU并行DFS, Data efficient training of LLM。在高性能领域还是有功底的。 系统的优化对于模型的训练, 特别是成本的节约有非常重要的作用, 但是这决定的是成本和实现时间，这或许对于他刚进入openAI几个月之内就搞出Sora是他工作实力的证明。 4. Will DePue注意, 这是一位大佬 主页 linked in 推特 github 学士: University of Michigan, 退学中….. 高中: Geffen Academy of UCLA 当过九个月的CEO, 工程能力超强, 工程能力强在何处? FIGMA-OS 使用figma构建的8bit计算机 WebGPT, 两个礼拜写完了, 获得了4k的github star Hyperlocal, 构建基于蓝牙的分布式点对点通信系统 DeepResearch, 数据分析和可视化系统 Built the first Redstone computer only using pistons. 第一个红石计算机, 有点猛。 在OpenAI 做过越狱和提示注入缓解、自定义模型、模型能力评估、API微调等工作, 这部分工作可能不是很和训练和设计相关, 确实也和工程很相关。 他说: I find them restrictive: my most “hard skill” is that I am the fastest and most curious learner I’ve ever met. 学习能力很强, 学习速度很快, 这一切也体现在他的工程能力中。人也很想得开OpenAI让他退学就退学, 坚持做自己感兴趣的事情, 这样的人才的培养其实是很难得的, 我觉得我们或多或少都被一些大家都追求的东西裹挟, 比如GPA, 工资等等。这位兄弟排到了第四, 想必是做了相当多的工作。 5. Yufei Guo (郭宇飞) 主页 github google scholar 博士: 2015-2020, 北大 学士: 2011-2015, 北京理工 主要做loss和优化相关的, 也做过一定的模型(尖峰神经网络)。这位同学22年还拿到过国家自然科学基金委的项目, 也有可能我理解有误, linkedIn的信息也非常有限, 这告诉我们要积极更新linkedIn给自己打广告呀。 6. Li Jing google scholar 主页 linkedIn 博士: 2014-2019, MIT的物理学博士 学士: 2010-2014, 北大物理 IPhO金牌, 这是国家队水准, 全国每年仅四人 在Meta做博后, 随后到OpenAI参与了Dalle3和Sora的工作。文章引用还是挺多的, 最有影响力的工作是和LeCun合作的Barlow twins: Self-supervised learning via redundancy reduction。因为是学物理的, 所以还用深度学习来进行粒子模拟和逆向设计, 发表在了Science advances上。我觉得他的物理功底和数学功底都是十分深厚的, 在Meta的工作也获得了比较大的影响力, 最后到OpenAI也是清理之中的, 但是无法从过去的工作中看出他在Sora中从事的工作和贡献。 7. David Schnurr linkedIn 主页 学士: 2008-2012, UCSB 是一个典型的工程师, 在Graphiq, Uber做可视化平台, 主要技能是JS和Python, OpenAI的Node.js API引擎就是他写的。 8. Joe Taylor linkedIn 学士: Academy of Art University 看起来是设计学专业的, 从事过网页设计以及图像设计, 前端设计和开发。 Working on early research. Helping accelerate research, build product intuition and direction, building 0 -&gt; 1 engineering systems. Announcement post; 其实不是很懂这句话是什么意思, 我觉得还是从产品, 设计以及宣传的角度去参与工作。 9. Troy Luhman google scholar 做了非常多diffusion 相关的工作, 主要关注高效生成和领域生成, 比较有意思的是文章只有两个作者, 合作者是下面的Eric Luhman, 我觉得可能是一家人。 10. Eric Luhman 推特 主要和Troy Luhman合作, 做diffusion 领域相关的工作。 11. Clarence Ng 学士: 加拿大滑铁卢大学, 计算机与系统设计 在AWS, google Cloud, Orach cloud都做过。 典型的工程师, 主要做云, 分布式系统和性能优化, 还做过对象存储, 可能对Sora的视频读取和处理进行了优化。工程经验极其丰富, 做的项目也很大, 是个优秀的人才。 12. Ricky Wang 学士: UCB, 2013-2016 主要也是工程师的画像, 在Instagram和Meta交替工作了非常多年。 13. Aditya Ramesh 主页 DBLP [summary]https://typeset.io/authors/aditya-ramesh-4xp87jcxw7 是dalle和dalle2的作者, 还层参与GPT的工作, 学术功底深厚, 有14.5k的引用, 在OpenAI应该也算比较元老的任务, 所以是项目的总负责人。从这个视频看, 文字稿, 他还有点呆呆的。 总结总sora的主要作者看, 我们可以从Sora团队学习到的经验是: 超前的认知, 注重scalable 大厂的经历和大厂的资源的支持, 做出出色的工作 极强的工程能力 良好可扩展的训练系统做支撑 好的数学功底和科研经验 参与过多种重大项目的leader的存在 我觉这一切还是值得自己反思的, 特别是大厂的经历和资源方面, 我觉得这是几乎所有人的共同特征。希望这篇博客与诸君共勉, 共同努力和进步。","link":"/blog/Sora-authors/"},{"title":"Transformer-Performance","text":"transformer 性能分析简介随着模型的不断变大, 模型的推理和训练成本在不断的提高, 如何更好的优化模型训练和推理的性能成为非常重要的领域, 10%的性能提升可能带来数十万乃至数百万成本的节省。 主要的性能优化一般来自于在计算逻辑不变的情况下对硬件更好的优化和利用, 或者在少量损失模型计算精度的情况下减少模型推理的计算量和放存量, 从而提高性能。 这篇博客主要学习这篇博客的分析思路, 先从内容的翻译和理解入手, 随后将博客的内容扩展到diffusion transformer以及训练相关的性能分析, 从而给如何优化以更好的理论指导。 约定和基础在本博客中我们对数值的约定和参考的博客有一定的出入, 对于内存占用, 我们只计算元素的个数, 也就是不考虑每个参数的位宽, 只计算参数的个数。默认情况下 矩阵向量乘的计算量:对于矩阵向量乘 , 的计算量为: 。对于 ,矩阵矩阵乘的计算量为 , 其中系数2分别为乘和加。 kv cache解读transformer在推理时分两个阶段, 是处理给定的prompt(是一次简单的forward, token一起喂入模型中) 随后是不断自回归地产生后续的token序列(每次只产生一个token)。这里需要提一点的是, transformer解码时之前所有计算的token对应的latent都和后续的token没有关系(因为attention mask的存在, 这也是为什么模型是autoregressive, 详情请见)。 因为之前的token和后面的token无关系, 所以这部分的值无需重复计算, 但是历史的K和V在每次计算中都需要(只被self attention 需要), 所以需要将每个transformer block的历史KV保存起来, 这部分保存的内容被称之为KV cache, 在使用KV cache的情况下, transformer每次只需要输入一个token用以计算, 无需输入再次前面的全部序列, 计算量是随着token个数线性增长。 对于每个token, 需要保存的KV cache参数量为: 其中(1+1)表示k和v。 对于计算一个新的token的KV cache, 我们需要的计算量为: 同时我们需要的访存量为: 取参数访存量取隐层访存量 总体而言, 是取参数的放存量占大头, token的访存量可忽略。 对于A100GPU而言, fp16的性能为312TFlops, 内存带宽为1.5T/s, 则用于计算一个token的KV访存耗时和计算耗时为: 可见, 用于访存的时间是用于计算的208倍之多, 这说明transformer解码的过程是内存瓶颈, 内存带宽。造成这种瓶颈的主要原因是解码时只计算一个token, 计算量小, 而模型的参数很大。 需要注意的是, KVcache的存在并不是仅仅为了节省计算KV本身所需要的计算量, 而是节省了前面所有token通过模型所需要的计算量。如果没有KVcache, transformer的解码过程会为平方级增长的计算量(第一次forward1个token, 第二次forward2个token, 第三次forward3token….), 这将难以承受。 💡 如果我们增大batch size, 能够获得在每次解码时更多的计算量和相近的访存量(因为主要放存量在模型权重), 而由于计算非常便宜, 我们的收益是大的(额外花1%的时间, 多获得一个token的解码), 这可能会提高每个token的延迟, 但是能够极大增加模型token的吞吐。对此已经有研究(ORCA)进行了优化, 使得模型吞吐上了一个量级, 造福了人类。 容量计算接下来进行简单的容量计算分析, 对于一个52B(52e9 numel)的模型, 如果采用半精度存储, 则大约需要104GB(104e9 Bytes, two bytes for each parameter)的空间, 单卡无法放下, 同时在推理时KVcache 也需要占用空间。 给定4卡的A100 40G卡, 我们可以简单计算可以sample 的token数量。已知模型已经占了104G, 只有16G留给了KVcache。每个token需要的空间为: 所以16G大约可以容下8000token。 模型并行我们一般讨论的模型并行是指张量并行, 也就是将模型纵向切开, 每张卡上都有所有block参数的一部分。模型并行能够使得每张卡只承受一部分的模型参数存储和有一部分的计算量, 这些部分收到卡的数量的影响。模型并行会额外带来的开销是分块计算后同步计算结果的通信开销, 这部分会影响到推理的延迟。 此外, 将模型横向切分, 每张卡包含了若干个完整的block, 这样的方法被称为流水线并行, 由于每个token会依次通过所有block也就会依次通过每张卡一次, 每次只有一张GPU在进行计算, 所以每个token的延迟和单卡基本一致, 但是通过流水线的方法不断喂入token, 总的吞吐可以达到和4张卡一致。流水线并行唯一的好处在于需要的通信量比较小, 这适合卡间带宽比较小的场景。流水线并行需要在每张卡之间传递latent, 而张量并行需要每个block之间进行每张卡之间的通信, 通信量上一个量级。 矩阵向量乘分块并行考虑权重矩阵, GPU数量为N, 输入向量大小。则输出大小应当为。 分块后, 每个GPU的权重矩阵大小为, 输入同时也被切分被向量大小 。每个GPU分别分配到的矩阵向量乘, 得到的输出大小为。每个得到的此时我们可以知道虽然每个GPU得到的结果和输出大小一致, 但是可知真正的结果是每个GPU计算得到的结果做求和得到的, 这就需要做一次All reduce的操作, 是的每个GPU上都是正确的结果之后, 再进行切分, 进行后续的分块并行计算。 attention的并行Attention的并行是通过在attention head的层面进行并行。head的切分维度刚好是在 中的n, 所以之前的计算结束后, 无需额外的通信就可以直接进行attention计算(前提是num head是卡数的n倍)。计算结束后, 甚至可以通过之后再进行通讯合并。同理, KVcache也是在head这个维度上存储在不同的GPU上。 各个模块计算量和访存量分析标记:在diffusion transformer中, num_tokens = s = T * H * W MLP mlp的参数为 in_d , mid_d, out_d 。通常情况而言, in_d = out_d, mid_d = 4 * in_d Flops = 访存量: 这里的访存量包括把结果写到内存中。 Vanilla self attention attention参数为: in_d, mid_d , out_d 。通常情况而言, 三者相等。 Flops = 访存量: Vanilla cross attention 参数为: in_d, context_d, mid_d, out_d, 通常只有context_d 和其他三者不同 Flops = 访存量: Spatial self attention attention参数为: in_d, mid_d , out_d 。通常情况而言, 三者相等。 Flops = 访存量: Spatial cross attention 参数为: in_d, context_d, mid_d, out_d, 通常只有context_d 和其他三者不同 Flops = 访存量: 计算量和vanilla一致 temprol self attention attention参数为: in_d, mid_d , out_d 。通常情况而言, 三者相等。 Flops = 访存量: 小讨论可知MLP的计算量随着序列长度线性增长, 而sefl-Attention的计算量是平方级增长, 在这里我们可以计算一下经典场景下, 当序列长度到达什么水平时, Attention的计算量会成为主要部分。 in_d = mid_d = out_d, MLP mid_d=4in_d 则MLP的计算量公式为: Attention计算量公式为: 在dim = 1536的情况下计算, 画图如下: 在大约3K以后, Attention的计算成为主要部分。 backward分析考虑以下的三层MLP, x为输入, 为三个权重矩阵, 为三个激活, 最后和ground truth计算得到loss。 remark:计算输入的维度大写为权重矩阵小写为中间激活 Operation Computation mul shape FLOP forward Computation FLOP backward mul shape Input ReLU Derivative Hidden1 ReLU Derivative Hidden2 ReLU Loss Update 注意到, 因为input x 无需计算梯度, 所以对于第一层而言, backward节省了约一半的计算量。所以在网络较深的情况下, backward的计算时间约为forward计算时间的两倍。 参考资料 https://kipp.ly/transformer-inference-arithmetic/ https://www.alignmentforum.org/posts/fnjKpBoWJXcSDwhZk/what-s-the-backward-forward-flop-ratio-for-neural-networks","link":"/blog/Transformer-Performance/"},{"title":"sequence parallel","text":"前言随着Sora和kimi的大火, 视频模型和超长序列语言模型的实践不断被人们摆到更重要的位置, 这篇博客主要从需求和基本思路以及实践这几个方面来讲解一下序列并行相关的内容。 需求从产品需求的角度看, 长序列是必然的需求。从语言模型看, 做超长文本检索以及摘要有确定的需求(例如平时看文献)。sora能够生成超长视频, 在使用vanilla attention的情况下, attention的序列长度将为(T, H, W), 也就是时长, 高度, 宽度三者的乘积增长, 序列长度比图像模型上一个量级, 这也一定会成为sora训练和推理的难题。尽管当前的flash attention的显存消耗能够被优化到线性增长的程度, 但是当序列长度足够长, 显存依然可能不够。 就此, 我们的问题被定义为: 如何让transformer支持超长序列的训练和推理 问题的核心: 来源于显存装不下超长序列带来的显存需求, 而非模型太大带来的显存溢出 可能方法: 优化显存或者使用多GPU并行计算分摊显存和计算 基本思路首先我们需要将一句话铭记于心, 默念三遍: transformer模型中, 序列的概念仅在attention这个操作中被需要, 在做MLP等其他操作时, 打乱序列甚至打乱batch都是可以的, 只要在attention操作时将序列顺序恢复即可。 对于超长序列支持问题, 可能最直接的想法就是使用张量并行, 将模型切分, 对应的序列也在(B, L, D)的D维度被切分, 从而可以节省显存。 但是需要注意的是, 我们遇到的问题是来自于序列过长, 而模型并非过大, 特别是diffusion的模型都还非常小, 一张显卡放的下。其次, 使用模型并行中途计算需要的通信同步开销较大, 小模型做张量并行不值得。 再次回想我们铭记于心的话, 我们可以思考, 可否将序列(B, L, D) 在L维度切分, 在多张卡上做MLP等和序列无关的操作(无需通信), 在做attention时将相关的内容从其他GPU获取, 再进行attention操作, 再将计算结果同步到其他GPU上。这样比张量并行的好处在于无需切分模型, 减少通信量, 提高模型的吞吐与性能。 基本实现接下来介绍几个实现了序列并行的库或者算法: 约定 B: batch size L: seq len D: hidden dim A: attention head size Z: number of attention heads N: number of GPU Z x A = D N个GPU上存储了(B, L/N, D)的序列, 给出在分布式情况下计算self attention的算法。 Ring self attention(RSA)这样的方式是通过在query的L维度上切分进行分布式的attention的计算。通信的方式是通过进程之间换装传递K和V的分块然后得到最后的计算结果, 这样的算法不受到Z大小的限制, 对GPU的数量是可扩展的。 第一阶段-环状传递分块的K来得到attention map 第二阶段-环状传递分块的V得到最后的结果 通俗的理解ring attention的机制, 其核心的并行的点在于, attention的计算是可以在query的sequence lens的维度上分块的, 也就是一部分的query和完整的key和value就可以得出此部分qeury对应的计算结果。而由于序列过长, key和value也被打散在不同进程中, 所以需要从其他进程不断传递并计算从而得到完整的key和value以得到最终结果。 不断集齐key和value这样的过程, 最简单的方式就是通过进程顺序点到点的方式完成, 显然这样的效率是不够高的, 在传递过程中如何尽量把各个节点之间的带宽利用好, 同时做好计算和通信的重叠, 而ring的方式就是系统领域典型的算法和方式, 能够做到较好的利用带宽, 在有良好的实线的情况下, 可以做到计算和通信的重叠。 Deepspeed ulyssAttention的另外一种并行方式就是类似于张量并行的按照Attention head进行切分。也就是每个进程拥有完整的序列长度, 但是只有一部分的head个数, 这样同样能够节省显存, 而且这样的方法可以做到不改变Attention的实现, 也就是任何的attention算法都和Deepspeed ulyss兼容。但是缺点是并行的卡的数量不超过头的个数Z。 序列并行要求在MLP层无额外的操作, 所以每个进程中应该有部分的序列, 但是包含完整的attention head, 在进行attention并行时, 又要求每个进程需要完整的序列且是部分的头。所以可以遇见的是在进行attention操作前, 通过通信算子使得每个进程拥有(B, L/N, ZxA) 转化到每个进程拥有(B, L, ZxA/N)的序列内容。attention 操作结束后, 再通过通信算子使得每个进程从拥有(B, L, ZxA/N)的序列内容转化为(B, L/N, ZXA)的序列内容。课件仅需要操作开始前, 结束后需要进行通信, attention的算子是完全独立的, 所以可以采用任意attention算子的实现。 Deepspeed ulyss的实现也非常简洁, 通过源代码就可以看到: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/sequence/layer.pyimport torchfrom typing import Any, Tuplefrom torch import Tensorfrom torch.nn import Moduleimport deepspeed.comm as distdef single_all_to_all(input, scatter_idx, gather_idx, group): seq_world_size = dist.get_world_size(group) inp_shape = list(input.shape) inp_shape[scatter_idx] = inp_shape[scatter_idx] // seq_world_size if scatter_idx &lt; 2: input_t = input.reshape( [seq_world_size, inp_shape[scatter_idx]] + \\ inp_shape[scatter_idx + 1:] ).contiguous() else: # transpose groups of heads with the seq-len parallel dimension, so that we can scatter them! input_t = input.reshape( [-1, seq_world_size, inp_shape[scatter_idx]] + \\ inp_shape[scatter_idx + 1:] ).transpose(0, 1).contiguous() output = torch.empty_like(input_t) dist.all_to_all_single(output, input_t, group=group) # if scattering the seq-dim, transpose the heads back to the original dimension if scatter_idx &lt; 2: output = output.transpose(0, 1).contiguous() return output.reshape( inp_shape[: gather_idx] + \\ [inp_shape[gather_idx] * seq_world_size,] + \\ inp_shape[gather_idx + 1:]).contiguous()class _SeqAllToAll(torch.autograd.Function): @staticmethod def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor, scatter_idx: int, gather_idx: int) -&gt; Tensor: ctx.group = group ctx.scatter_idx = scatter_idx ctx.gather_idx = gather_idx return single_all_to_all(input, scatter_idx, gather_idx, group) @staticmethod def backward(ctx: Any, *grad_output: Tensor) -&gt; Tuple[None, Tensor, None, None]: return (None, _SeqAllToAll.apply(ctx.group, *grad_output, ctx.gather_idx, ctx.scatter_idx), None, None)class DistributedAttention(torch.nn.Module): \"\"\"Initialization. Arguments: local_attention (Module): local attention with q,k,v sequence_process_group (ProcessGroup): sequence parallel process group scatter_idx (int): scatter_idx for all2all comm gather_idx (int): gather_idx for all2all comm \"\"\" def __init__( self, local_attention: Module, sequence_process_group: dist.ProcessGroup, scatter_idx: int = 2, gather_idx: int = 0, ) -&gt; None: super(DistributedAttention, self).__init__() self.local_attn = local_attention self.spg = sequence_process_group self.scatter_idx = scatter_idx self.gather_idx = gather_idx def forward(self, query: Tensor, key: Tensor, value: Tensor, *args: Any) -&gt; Tensor: \"\"\" forward Arguments: query (Tensor): query input to the layer key (Tensor): key input to the layer value (Tensor): value input to the layer args: other args Returns: * output (Tensor): context output \"\"\" # TODO Merge three alltoall calls into one # TODO (Reza): change the api on the megatron-deepspeed side so that we only receive all data (q,k, and v) together! #in shape : e.g., [s/p:h:] query_layer = _SeqAllToAll.apply(self.spg, query, self.scatter_idx, self.gather_idx) key_layer = _SeqAllToAll.apply(self.spg, key, self.scatter_idx, self.gather_idx) value_layer = _SeqAllToAll.apply(self.spg, value, self.scatter_idx, self.gather_idx) #out shape : e.g., [s:h/p:] context_layer = self.local_attn(query_layer, key_layer, value_layer, *args) output = _SeqAllToAll.apply(self.spg, context_layer, self.gather_idx, self.scatter_idx) #out e.g., [s/p::h] return output 核心实现就是_SeqAllToAll这个函数的实现, 给定group, 在forward的时候通过通信将当前进程需要的序列聚积到当前进程, 在backward时, 我们通过后层收到的梯度也应该通过通信分发到正确的进程中, 并从别的进程中取到属于自己进程内容的梯度。以q, k, v的通信算子举例: forward时, 进程拥有(B, L/N, ZxA) 的序列内容, 通信后进程拥有(B, L, ZxA/N) backward时, 进程收到的梯度是(B, L, ZxA/N)内容的梯度, 但是本进程需要正确回传的梯度是(B, L/N, ZxA) 序列的梯度, 所以通过同样的算法得到正确的梯度 对于通信的算子, 最简单的算子就是AlltoAll, 使得每个进程在某个瞬间拥有(B, L, ZxA)的序列, 然后丢弃不需要的部分进行计算, 显然这样的方式会出现不必要的显存分配, 没有做到足够优雅的节省显存问题, 所以deepspeed使用的事AlltoAll_single这样的通信算子, 可以理解为将张量内容在进程间的某两个维度进行转置, 通信过程无需分配很多内容, 进一步提高效率。为了大家对AlltoAll_single有更好的理解, 这里附上pytorch对应算子的文档: torch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False) 12345678910111213&gt;&gt;&gt; input = torch.arange(4) + rank * 4&gt;&gt;&gt; inputtensor([0, 1, 2, 3]) # Rank 0tensor([4, 5, 6, 7]) # Rank 1tensor([8, 9, 10, 11]) # Rank 2tensor([12, 13, 14, 15]) # Rank 3&gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)&gt;&gt;&gt; dist.all_to_all_single(output, input)&gt;&gt;&gt; outputtensor([0, 4, 8, 12]) # Rank 0tensor([1, 5, 9, 13]) # Rank 1tensor([2, 6, 10, 14]) # Rank 2tensor([3, 7, 11, 15]) # Rank 3 总结本文介绍了序列并行解决的问题, 以及两种主流的实现序列并行的方式。其实两种序列并行可以混合使用, 从而达到更好的并行度和性能。同时在语言模型中, 带causal mask的情况下, 简单的序列切分会导致进程间计算负载不均衡, 随后衍生出striped attention。","link":"/blog/sequence-parallel/"},{"title":"加拿大签证记","text":"和学姐合作中了nips，要去加拿大，所以准备申请加拿大签证。 然后因为意大利签证失利，然后加拿大签证就抱着摆烂的心态，随便填，签证type选others，然后资金证明也不管了随便打个流水就上，主打的就是一个爱过不过。因为组里很多人都被check了，自己心态良好。 结果莫名其妙就过了，这可能就是无心插柳柳成荫吧（doge）。 其实身边主要被check的点就是满足了重要高校、博士、计算机等几个标签，就很有可能check，所以建议本科阶段就发文章办签证（doge）。","link":"/life/canada-visa/"},{"title":"第一个生活篇","text":"最近北京天气还可以, 很有春天的意思, 今年北京没有刮黄沙满天飞的沙尘。不过有时候觉得中午下午比较热, 早上和晚上会冷一些, 昼夜温差大。","link":"/life/hello-world/"},{"title":"意大利签证记","text":"这是一个很糟糕的故事。 和学长合作中了eccv24，可以去意大利米兰，八月份开始准备签证，想着北京约不到slot的话就去沈阳。去了沈阳才知道意大利的签证是分领区的，符合领区范围的才可以去沈阳。符合领区的条件是证明你是北京人，可以通过 居住证 户口 学生的在读证明 以上三点来证明。但是彼时我刚入学，学校要三个月后才能开在读证明，然后我并没有迁移集体户口，所以直接gg了。 再想着申请上海领区的slot发现异常拥挤，黄牛也难拿到号，最后不了了之，没去成，错过了一场愉快的意大利欧洲之旅。","link":"/life/italy-visa/"}],"tags":[{"name":"model arch","slug":"model-arch","link":"/tags/model-arch/"},{"name":"test","slug":"test","link":"/tags/test/"},{"name":"career","slug":"career","link":"/tags/career/"},{"name":"mlsys","slug":"mlsys","link":"/tags/mlsys/"}],"categories":[{"name":"blog","slug":"blog","link":"/blog/"},{"name":"life","slug":"life","link":"/life/"}],"pages":[{"title":"Chendong Xiang(项晨东)","text":"BiographyI am a undergraduate student of computer science at Tsinghua University. I am going to join the TSAIL Group in the Department of Computer Science and Technology, Tsinghua University, advised by Prof. Jun Zhu. Currently, my research interest includes topics on deep generative models, including diffusion models, and their applications in computer vision, 3D generation and. I also interested in embodied AI and machine learning system. Publications FeedFace: Efficient Inference-based Face Personalization via Diffusion Models Chendong Xiang, Armando Fortes, Khang Hui Chua, Hang Su, Jun Zhu International Conference on Learning Representations (ICLR2024) [code] CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu (ECCV2024) [code] Identifying and Solving Conditional Image Leakage in Image to-Video Generation Min Zhao*, Hongzhou Zhu*, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, Jun Zhu (NeurIPs2024) [code] / [website] Preprints A Closer Look at Parameter-Efficient Tuning in Diffusion Models Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu [code] Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models Fan Bao, Chendong Xiang*, Gang Yue*, Guande He*, Hongzhou Zhu*, Kaiwen Zheng*, Min Zhao*, Shilong Liu*, Yaole Wang*, Jun Zhu SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Jintao Zhang, Chendong Xiang, Haofeng Huang, Haocheng Xi, Jia Wei, Jun Zhu, Jianfei Chen [code] Competitions International Algorithm Case Competition(IACC), Champion, 2023.12 Computer System Development Capability Competition, OS functional design, First Prize, 2022.8 ACM-China International Parallel Computing Challenge（IPCC）, Third Prize(Fourth place), 2022.10 Chinese Chemistry Olympiad（CChO）, Gold Medal, 2018 Honors &amp; Awards Science and Technology Innovation Excellent Scholarship, Tsinghua University, 2022 Excellent Volunteer Scholarship, Tsinghua University, 2022 Literary and Arts Excellence Scholarship, Tsinghua University, 2021, 2020 National Encouragement Scholarship, 2020","link":"/cv.html"},{"title":"Chendong Xiang","text":"👋 Hi, I’m Chendong Xiang, you could also call me Xiaoyu Xiang，an Fist-year PHD student at THU CST👀 I’m interested in generative model and embodied AI🌱 I’m currently learning pretrain model, such as t2i diffusion models💞️ I’m looking to collaborate on AI projects📫 How to reach me xcd19@mails.tsinghua.edu.cn","link":"/index.html"}]}